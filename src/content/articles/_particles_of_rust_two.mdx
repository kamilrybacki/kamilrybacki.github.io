---
title: "Particles of Rust II: Balls and sticks"
date: "2024-02-12"
description: "Fill our simulation box with particles and simulate their interactions"
tags: ["rust", "molecular dynamics", "simulation", "particles", "potential"]
image:
  thumbnail: "por2"
  hero: "por2"
  alt: "Bond between atoms"
---

## Keep it cool ‚ùÑÔ∏è

Right now, we have our first building blocks of molecular dynamics simulations ready - the definition of a bounding box
that encompasses our system and the definition of a particle. By providing the information about their relative positions
within a repeatable pattern (a unit cell of a crystal lattice) we can construct three-dimensional structures of any
crystalline material. Of course, if we want this structure can also be of any kind - even the completely random one,
since right now we are passing each particle's position manually in the simulation script.

So, the box and balls of our atomistic machine are up and running. Now, it's time to connect the spheres modeling
the atoms with "sticks", or in more scientific terms - with the **potential energy function**. Right now we will not
concern ourselves about implementing their behavior over time i.e. the dynamics of the system but rather focus on
the static picture of our simulation box.

By correct construction of the energetics of the interactions between the
particles, we will ensure that any mishaps we encounter during the integration of the equations of motion are not due to
the incorrect modeling of the potential energy landscape in the simulated lattice. **And believe me**, there is a plethora
of caveats that await us in the upcoming Newtonian puzzles, that are mostly related to the numerical precision of methods
we will choose. But, let's not get ahead of ourselves and focus on the task at hand.

This ignorance of dynamics is not uncommon and even has its own name - **molecular statics** (duh ü§∑‚Äç‚ôÇÔ∏è). It is a
computationally less demanding approach to molecular dynamics simulations, where we are not interested in the
time evolution of the system but rather in the **equilibrium** state of the system. And as nature often has it,
this equilibrium is the situation where the forces acting on the particles are balanced and the system is at rest, so
**their overall energy is at minimum**. Funny enough, physicists identify such states as ones where, hypothetically,
the temperature of the system is absolute zero - as in, there is totally no movement of any particles, they are at 0 Kelvin.
So we're venturing into a very cold world, indeed. ü•∂

## First encounter with the moonspeak üåå

The potential energy function is a mathematical model that describes the interactions between particles in a system.
Speaking 100% technically, it is a function of the relative positions of two particles and it returns the potential
energy of interaction between them, expressed in the same units as any other type of energy (e.g. Joules). There
is a **multitude** of potential energy models, that have been established over the years. Some of them try to
expand upon the information required to correctly calculate interaction energy for a given pair of particles by
introducing additional parameters that are connected to, for example, the closest neighborhood of each particle
or the physical properties of the particles themselves (like their electronic structure). 

Here, for the sake of simplicity, we will stick to the most trivial potential energy model - the **Lennard-Jones potential**.
It is a model that describes the interaction between two neutral atoms or molecules. It is a sum of two terms - the first
one is a repulsive term, that is dominant at short distances and the second one is an attractive term, that is dominant
at long distances. This ensures that if particles start to get too close to each other, they will repel each other
to prevent their overlap and if they are too far away, they will attract each other to form a stable bond. So, in a way,
it is a model that describes the behavior of a pair of particles as if they were connected by a spring. You can
try right now to get a spring and squish it a bit, then release it and observe its behavior. It will, very shortly,
oscillate or do the ["boioioioing" thing], expanding and contracting until it reaches the equilibrium state - the state
where its dimensions come back to normal. I have found a very nice animation (courtesy of [physics-animations.com]), which
You can see below. The behavior of those two bonded particles will be very similar to the explained behavior of the spring.
Of course, we neglect here that there are any other phenomena that may lead to this movement being damped like the friction of
the air or the internal friction of the spring itself (You can imagine internal "layers" of the spring's material rubbing against
each other on the microscopic scale, wild, right? ü§Ø).

<CaptionedImage
  src="/assets/images/por_2_lj_oscillation.gif"
  alt="A model for two bonded particles - two balls and spring"
/>

So we need that dual nature of the potential energy function to model the interactions between our particles and this will
translate into this oscillating motion when we integrate the equations of motion for our particles that will try to maintain
their state around some equilibrium. But You may be a little bit confused right now - I keep talking about potential **energy**
and **force** somewhat interchangeably. There is a simple explanation for that, that can be shown using simple, physical hieroglyphs.

### Unleashing the force üå™Ô∏è

Okay, so maybe You remember from Your high school physics classes that there is something as a **work**, in physical
sense, of course. It is a measure of the energy transfer that occurs when an object is moved over a distance by an
external force at a constant speed. This means, that over some **distance** (let's call it `d`) we apply some **force**
(let's call it `F`) and the object effectively gains some **energy** (let's call it `W`, as work). This relationship
can be briefly expressed as:

$$W = F \cdot d$$

A very graphical representation of this energy transfer would be a bullet that hits a wall. When the collision occurs, 
the bullet starts instantly to act with great force upon a fragment of the wall it hits, transferring its kinetic energy to the wall.
But this energy can also be thought of as a portion of work that the bullet has done on the wall, which forces the wall to adjust its position
by a distance calculated from the equation above: $$d = \frac{W}{F}$$.

However, we can also perform another transformation of this equation, to express the force acting on the object in terms of the work done on it:

$$F = \frac{W}{d}$$

which tells us that if we somehow measure or calculate the amount of work done between two objects separated by a distance `d`, we can
also calculate the force that was acting between them. But why? Well, we can imagine this force as trying to maintain the optimal distance
between those bodies, such that they will not be too close to each other (which would result in a repulsive force) or too far away from each other
(which would result in an attractive force). If they are not in this ideal state, each of them will have to perform some **work** to modify this
distance and its amount will be dependent on how far they are from this hypothetical **equilibrium**.

So, what if they are at this perfect distance from each other? Well, then the force acting between them will be zero, since they will not be trying to
change their relative position. Let's express this for our pair of objects as some perfect (equilibrium) distance $r_0$ for which: $F(r_0) = 0$ i.e. nothing happens.
**Any** deviation from this distance will result in a force acting between them, that will try to bring them back to this equilibrium.
We can write this deviation as a change in distance $\Delta r = r - r_0$ and the force acting between them as $F(\Delta r)$. Going back to our
equation for the force acting between them, we can express it as:

$$F(\Delta r) = -\frac{W(\Delta r)}{\Delta r}$$

where $W(\Delta r)$ is the work that would be needed to compensate for this sudden change and enforce back the equilibrium state.

Wait, why the minus? ü§î Well, remember what is the role of the force $F$. It tries to revert the current state of the system to the
original one, so it will be always antagonistic to the change that has occurred. So, the sign of $F$ must always be opposite to $\Delta r$
and that's why we have this minus sign in the equation above.

At this point, we must do a little switcheroo and say that this work will be some function dependent solely on the distance between particles.
In physics, a function that returns a numerical (scalar) value, which can be used to express the magnitude of some directional quantity
(like force, for example) is called a **potential**. In other words, this potential function will spit out a number that will tell us
how strong the interaction between particles is at a given distance **and** can be directly used to calculate the measure of this
interaction a.k.a force. We will denote this function as $U(\Delta r)$ and call it the **potential energy function**.

The last mental gymnastic we can perform is by saying that if $\Delta r$ can be expressed as a difference between a current
position of the object and some predefined constant (which is the equilibrium distance $r_0$) that will not change over time,
then the real dependence of potential function $U$ is on position $r$ directly. Same with the force acting between particles ($F$).
This may seem like we want to cheat a little bit and just substitute one thing for another, but mathematically speaking
the $r_0$ is a constant and the real argument of the function is $r$ - the equilibrium distance is not a dynamic measure.
Imagine back again a spring - its original, non-deformed length depends only on how it was manufactured and on the material
from which it is made. Similarly, our system also has its equilibrium state which can be derived from its composition
(like, which atoms constitute its makeup and what are their properties). So, our equation for the force acting between objects
can be expressed as:

$$F(r) = -\dfrac{U(r)}{\Delta r}$$

This is true for a significant change in the position, so for $\Delta r >> 1$ (much bigger than one unit of distance).
Now, let's go back to our cases of atoms connected by the aforementioned forces. We can imagine that any movement
measured at their scale will be super **minuscule** or even **infinitesimal** (infinitely small). This means that
our $\Delta r$ will be tending to zero, which allows us to use one of the magical tools of calculus - the derivative.
We can express the force acting between particles as. If we prefix very small amounts of quantities in our equation
with $d$ (like $dr$), we can express the force acting between particles as:

$$F(r) = -\dfrac{dU(r)}{dr}$$

Does that mean that we will have to measure those very small changes in distance? Well, no. The derivative allows us
to take the function that describes the potential between particles ($U(r)$) and then transform it using a set
of differentiation rules into a new function that will describe the force acting between particles ($F(r)$). This
is **huge** for us, because we will be able to calculate those forces **directly** via a mathematical function
implemented as a piece of code.

This sets up a first set of goals for us:

1. Find the mathematical expression for the Lennard-Jones potential energy function.
2. Quickly calculate its derivative to obtain the equation for the force acting between particles.
3. Implement a function that will take the distance between particles as an argument and return the force acting between them.
4. Do the calculation for all pairs of particles in our system and sum up the forces acting on each particle to obtain the net force acting on it.

Let's blitz through the mathematical part and then delve into more programming side of things.

## Slippery slopes of Lennard-Jones üèîÔ∏è

The Lennard-Jones potential is like a baby's first model to implement for the purposes of molecular dynamics simulations.
Finding an expression for it takes [literally one minute] and has a rather simple form with two rational terms:

$$U(r) = 4\epsilon\left[\left(\dfrac{\sigma}{r}\right)^{12}-\left(\dfrac{\sigma}{r}\right)^6\right]$$

To get a better hand on its inner workings, let's investigate it using some good ol' Python and the `matplotlib` library.

import JupyterLiteEmbed from "@components/markdown/JupyterLiteEmbed"

<JupyterLiteEmbed
  size="750px"
  file="por/lj.ipynb"
  kernel="python"
  client:load
/>

After our excursion, we got two major pieces of information. The first one is the expression for the **force of interactions**
between particles derived from Lennard-Jones potential (note the minus again, it was explained in the last Markdown cell):

$$F(r) = - 4 \epsilon \left(\frac{6 \sigma^{6}}{r^{7}} - \frac{12 \sigma^{12}}{r^{13}}\right)$$

that can be visualized using the graph generated at the end of the Jupyter Notebook that was previously used,
which I took the liberty to enhance a little bit to explain its characteristics and relationship with
the equation above:

<CaptionedImage
  src="/assets/images/por_2_lj_whole.png"
  alt="Lennard-Jones potential (blue curve) and force (red curve) with characteristic points and quantities shown"
/>

First of all, an interesting fact is that even though we say that there is an equilibrium in our system at
some predefined distance $r_0$, it is not a state of zero potential energy - this means that even though interactions
between bodies cancel out, they are still **bonded** and require external energy to compensate for this potential well.
For now, I have denoted this energy stored in their shared bond as $U_\mathrm{min}$ (minimal value of potential).
Introducing new designations in physics should always make Your laziness senses tinglin' üï∑Ô∏è, because that means
keeping track of another Latin or Greek squiggle in our equations. We must try to relate it to things we already know,
thus reducing the number of exotic runes we will inscribe onto this digital parchment üìú.

Looking at our graphs and reaching back to the mathematics, we see that this minimum value exists for such $r_\textrm{min}$
that the expression for $F(r)$ reaches zero, so in our moonspeak dialect üåõ this means: $F(r_\textrm{min}) = 0$,
which allows us to find a recipe for this distance **in terms of $\sigma$ and/or $\epsilon$ parameters**:

$$F(r_\textrm{min}) = 0 \Rightarrow - 4 \epsilon \left(\dfrac{6 \sigma^{6}}{r^{7}_\textrm{min}} - \dfrac{12 \sigma^{12}}{r^{13}_\textrm{min}}\right) = 0~~~/ \cdot (-4\varepsilon)$$

$$\dfrac{6 \sigma^{6}}{r^{7}_\textrm{min}} = \dfrac{12 \sigma^{12}}{r^{13}_\textrm{min}}~~~/ \cdot r^{13}_\textrm{min}~~~/ : 12$$

$$r_\textrm{min}^6 = 2\sigma^6 \Rightarrow r_\textrm{min} = 2^{\frac{1}{6}}\sigma$$

As we can see this result goes in line with our observation in the Jupyter Notebook, where playing around with the value
of $\sigma$ parameter, shifted our graphs horizontally i.e. changed the point where it was intersecting the $y$-axis.
Now, leveraging pure mathematics, we can say that this is also the distance at which the potential $U(r)$ has its minimum
because **the location of the minimum of some function is equal to the argument for which its first derivative equals zero**.
So, reaching again for the occult symbols, this means that: $U(r_\textrm{min})=U_\textrm{min}$ (hence the subscript).
Knowing that, let's substitute the $r$'s for $r_\textrm{min}$'s and see what we get:

$$U(r_\textrm{min}) = 4\epsilon\left[\left(\dfrac{\sigma}{r_\textrm{min}}\right)^{12}-\left(\dfrac{\sigma}{r_\textrm{min}}\right)^6\right] = 4\varepsilon\left[\left(\dfrac{\sigma}{2^{\frac{1}{6}}\sigma}\right)^{12}-\left(\dfrac{\sigma}{2^{\frac{1}{6}}\sigma}\right)^6\right] = $$

$$ = 4\epsilon\left[\left(\dfrac{1}{2^{\frac{1}{6}}}\right)^{12}-\left(\dfrac{1}{2^{\frac{1}{6}}}\right)^6\right] = 4\varepsilon \left(\dfrac{1}{4} - \dfrac{1}{2}\right) = -\varepsilon$$

### Limiting our horizons ‚úÇÔ∏è

Now it is obvious why changing the $epsilon$ parameter scaled our graphs vertically - **it is the depth of the potential well itself!**.
Even the minus doubles down on this fact since, as we saw on the plots because the minimum of energy is always below zero.
This means that the combination of those two Lennard-Jones parameters controls the location and depth of the potential minimum
or, in more physical terms, **the strength of interatomic bonding**. In the graph above we can also see, that the potential
tends to zero if we move along its right side, so up the potential well. In theory, the potential will be equal to zero
if the distance $r$ reaches infinity ($r \rightarrow \infty$).

We can also look at this behavior the other way around - if we give a portion of energy equal to $\epsilon$ to our bonded bodies, 
then we will move its potential energy to the point of $U(r) = 0$, which would mean that they are infinitely far from each other. 
**We would break the bond** and that is the role of this potential well in terms of their interaction - it tries to keep the two atoms bonded
to each other, but if a sufficient amount of energy is packed into the system then it will yield and let the particles
free. ü•è You can visualize it as a ball that continuously slides up and down a U-shaped valley - on its own it
doesn't have enough energy to overcome the height of the hills surrounding it and after any disturbance, it will
gravitate towards the bottom (please don't make any comparison between this phenomenon and the occasional manifestation of human folly ü§´).

<CaptionedImage
  src="/assets/images/por_2_potential_well.gif"
  alt="An approximation for oscillations around potential well"
/>

But what about bodies that are far from each other, from the point of view of Lennard-Jones potential of course?
What does "far" even mean in this context? Well, we can see that the potential energy function tends to zero
but it does not reach it at some finite distance. However, after some distance, the value of the potential energy
(and the force acting between particles) will be so small that it will be negligible in comparison to other forces
occurring within the system, even by several orders of magnitude. This means that we can **cut off** the potential
or in other words - **limit its range**.

This is a very common practice in molecular dynamics simulations, where
we want to limit the computational cost of calculating interactions between particles that are very far from each other.
So we would need to introduce a new parameter that will represent the distance at which we will cut off the potential,
normally denoted as $r_\textrm{cut}$. This means that the potential energy function of interactions between pairs of atoms
will be calculated only if their distance is smaller than $r_\textrm{cut}$. Cool, so at each calculation of forces
between atoms we will just calculate the distance between their pairs, then check the aforementioned condition and,
if they meet the criteria, we will plop this distance into the function for the force and calculate it. Yeah, that's
the basic idea, but if we really think about it we just eliminated one-third of the whole problem, because we would
still need to iterate over **all unique pairs of atoms** in our system and calculate the distance between them **and**
check if it is smaller than $r_\textrm{cut}$. This may seem like a nitpick, but if we have a system with a large number
of particles, this operation will be very time-consuming and could be a bottleneck in our simulation.

Maybe we can use our newly introduced parameter to our advantage and limit the number of pairs we need to check? ü§î

### The art of neighborliness üèòÔ∏è

Be honest, do You care (or even know) about the neighbors one street over from Your house? Probably not, because they are
**distant** from Your perspective and **there is no interaction**. The only list of people to keep track of is of those
that actually have some, even small, influence over Your everyday life - they say "hi" to You (which is nice, I highly recommend it üëã) 
when You are taking out the trash since they can recognize You as one of their own. This catalog of people can be set up every couple of years
because people don't move that often, so it is not a big deal to keep it up to date every day.

Simple ideas but they can be applied to our system as well because any movement of particles in our system will be very small and
even if we actually **missed an opportunity** to catch some change in the local neighborhood of the chosen particle,
the error introduced by that will be very small and (in most cases) will correct itself in the next iteration. Thus, 
we can introduce a new concept of [**neighbor list** (or Verlet list)] that will contain only those particles that
are within the distance $r_\textrm{cut}$ from each other. This list will be updated every couple of iterations and
will be used to calculate the forces acting between particles.  Each atom will keep track of IDs of its closest neighbors and 
this will significantly reduce the number of pairs we need to check and will speed up our simulation.

So, since we used Jupyter Notebook to visualize the Lennard-Jones potential and force, we can also use it to
implement the neighbor list and see how it will affect the number of pairs we need to check. Let's do it!

<JupyterLiteEmbed
  size="1000px"
  file="por/neighbor_list.ipynb"
  kernel="python"
  client:load
/>

The near-100% speedup looks juicy, so we will definitely implement the lists in our simulation. But, as always, there is a catch.
The neighbor list will need to be updated every couple of iterations and this will introduce some overhead to our simulation.
This means that we will need to find a balance between the number of iterations after which we will update the neighbor list
i.e. how fast we want our subsequent calculation of forces to be and how much precision we are willing to sacrifice
and still deem our results to be physically meaningful. This is a very common trade-off in computational physics between
speed and precision. ‚öñÔ∏è

With the potential model and the neighbors list in our hands, we are ready to implement the force calculation in our simulation.
It's time to get ...

<img src="/assets/images/por_2_rusty.png" alt="Let's go!" style="margin-left:auto;margin-right:auto;height:200px;"/>

## Transcribing to crabspeak ü¶Ä

To kick things off, we will prepare a new submodule in our molecular dynamics source code, called `statics`, that will hold
all functions related to interatomic interactions and their energetics. We will try to ensure loose coupling between the
potential model interface and the rest of the simulation engine, to allow for easy swapping of potential models in the future.

So, as a little reminder, here is our current project structure:

<FileTree
  root="src/"
  tree={[
    {
      "data/": [
        "mod.rs",
        "input.rs",
        "output.rs"
      ],
    },
    {
      "system/": [
        "atom.rs",
        "box.rs",
        "cell.rs",
        "system.rs",
        "mod.rs",
      ]
    },
    "main.rs",
    "simulation.rs"
  ]}
  annotations={`
    data: module for handling input and output data
    system: module for handling the system of particles
  `}
  links={``}
/>

The new `statics` directory will reside parallel to other submodules and be composed of the following parts:

1. A module called `neighbors.rs` that will hold the implementation of the neighbor list.
2. A subdirectory `models` that will hold all of the implementations of all potential models that we will use in our simulations,
together with something that "glues" them together and redirects our simulation engine to the correct implementation.

Dividing our statics module into smaller parts will allow us to keep our codebase clean and maintainable, **and** is consistent
with our previous implementation of the [`Simulation` structure, which was a composition of smaller parts] that were responsible
for different aspects of the simulation. This will allow us to easily swap potential models and neighbor list implementations
in the future, without the need to change the rest of the overall simulation engine.

So, the interface for "routing" the calculations to the correct potential model will be stored in the `potential_model` field
of this structure, with neighbor list implementation stored in the `neighbors` field. In the future, we will see that
the neighbor list is used not only for the calculation of forces but also for the calculation of the potential energy of the system
or other microscopic properties that we will want to calculate - that is why it is also separated from the potential model
implementation at the code level.

This will result in the following project structure:

<FileTree
  root="src/ "
  tree={[
    {
      "data/": [
        "mod.rs",
        "input.rs",
        "output.rs"
      ],
    },
    {
      "system/": [
        "atom.rs",
        "box.rs",
        "cell.rs",
        "system.rs",
        "mod.rs",
      ]
    },
    {
      "statics/": [
        "neighbors.rs",
        {
          "models/": [
            "mod.rs",
            "lj.rs"
          ]
        },
        "mod.rs"
      ]
    },
    "main.rs",
    "simulation.rs"
  ]}
  annotations={`
    data: module for handling input and output data
    system: module for handling the system of particles
    statics: module for handling the static representation of the system (potential models, neighbor lists etc.)
  `}
  links={``}
/>

Now, we will implement the Lennard-Jones potential model and `enum` that (even syntactically) will act as a
union for all potential types that we will implement in the future.

### Realizing our potential üå†

So, first, we must create a `mod.rs` file inside the `models` subdirectory that will have two purposes -
be an entry point from the point of view of the parent
module and hold the implementation for all of the available potential models.

What does the potential model have to do? Well, it has to calculate the force acting between particles - no matter the
underlying mathematical model. ü§∑‚Äç‚ôÄÔ∏è This means, that each model will possess a **trait**, that allows it to perform this
action and will be required by the simulation engine to be included for each new implementation. From our previous
discussion throughout this article, we know that the force acting between particles is a function of their relative
distance, so we will require each potential model to implement a method that will take this distance as an argument
and return the force acting between particles. We can also throw in a function for calculating pure potential value
since it will be used in the future e.g. for the calculation of the potential energy of the system (and I mean,
the whole thing is called **potential** model so it should be able to calculate the potential, right?).

<CodeSnippet
  language="rust"
  code={`
    pub trait CalculatePotential {
      fn calculate_potential(&self, distance: f64) -> f64;
      fn calculate_force(&self, distance: f64) -> f64;
    }
  `}
  filename="src/statics/models/mod.rs (main trait)"
  annotations={`
    1: This trait has to be public, but the underlying methods can be private
    2-3: Both methods accept distance as a float and spit out a float
  `}
/>

Why is it important to have this trait for potential models? Because we can reference those methods in the rest of our
implementation without knowing the exact type of the potential model that we are using. We've basically abstracted away
these calculations from the simulation engine, which makes our codebase a lil' bit more flexible and maintainable. üë®‚Äçüîß

The same reasoning can be performed for out common potential interface, because from the outside it should be just a black box
that will respond with the force acting between particles and the potential energy of their interaction when prompted
by the distance between them. This gives us a very simple definition of our `enum` and its implementation:

<CodeSnippet
  language="rust"
  code={`
    pub enum PotentialModel {
        LennardJones(lj::LennardJonesModel),
    }

    impl PotentialModel {
        pub fn from(potential_definition: &yaml_rust::Yaml) -> PotentialModel {
            match &potential_definition["model"] {
                yaml_rust::Yaml::String(potential) => match potential.as_str() {
                    "lj" => PotentialModel::LennardJones(lj::LennardJonesModel::initialize(
                        &potential_definition,
                    )),
                    _ => panic!("Potential model not implemented"),
                },
                _ => panic!("Potential model not implemented"),
            }
        }
        pub fn update(&self, atom: &mut Atom, neighbors_list: &NeighborsList) -> () {
            atom.current.potential_energy = 0.0;
            atom.current.force = Vector3::new(0.0, 0.0, 0.0);
            neighbors_list
                .get_neighbors(atom.id as u64)
                .iter_mut()
                .for_each(|neighbor| {
                    let current_pair_potential_energy = match self {
                        PotentialModel::LennardJones(model) => {
                            model.calculate_potential(neighbor.distance)
                        }
                    };
                    atom.current.potential_energy += current_pair_potential_energy;
                    let force = match self {
                        PotentialModel::LennardJones(model) => {
                            model.calculate_force(neighbor.distance)
                        }
                    };
                    atom.current.force += -force * neighbor.distance_vector.normalize();
                });
        }
    }
  `}
  filename="src/statics/models/mod.rs (potential interface)"
  annotations={`
    1: Definition of the enum that will hold all potential models
    2: Here we list all potential models that are available
    6: This method will take the potential parameters from input YAML and return the correct potential model
    9: We will implement this initializer in a second
    17: This method will update the potential energy and force acting on a given atom
    18-19: We zero out the potential energy and force acting on the atom
    21: This getter will be part of the neighbor list implementation shown soon
    22: We iterate over all neighbors of the atom and calculate the potential energy and force acting on it
    24-34: Here we check which model is chosen and then use its methods from the trait
    35: To get the VECTOR of force we have to calculate its value and then multiply it by the normalized distance vector
  `}
/>

You may look at this code and think to Yourself: "Why so many `watch` statements? Aren't the calculations of potential
agnostic to the potential model that we are using?". Well, yes, they are, but we have to remember that we are using
an `enum` to hold all potential models and we have to **pattern match** on it to get the correct implementation of the
potential model. This is a little bit of a hassle, but it is a necessary evil to keep this flexibility up and running.
Also, some potentials may require intermediate calculations that are specific to them (e.g. [MEAM]) and we have to account for that
in our implementation, so a dedicated scope for each model can be very useful. You will see in a second that each
implementation will hold its domain-specific parameters, so any operations on them can also be performed inside those
little protocols.

On line 35, You can see that we do a little trick to get the vector of force acting on the atom. We calculate the force
acting on the atom and then multiply it by the normalized distance vector between the atom and its neighbor. This is
because the distance vector is located in the direction of the force acting on the atom since it is the minimum distance
between them (from its definition) so the direction of the force vector will naturally be the same. To get solely the
**direction** of vector we have to normalize it, so it will have a length of 1. This is a very common operation in
computational physics and is used to get the direction of some vector without its magnitude (because we don't want
to change the value of the force acting on the atom, only set its orientation). You could have already encountered
a very similar concept e.g. during middle-school physics classes, where the teacher was explaining the concept of
the electrostatic force between two charges in space as seen below ([source]):

<CaptionedImage
  src="/assets/images/por_2_force_vector.jpg"
  alt="Relative orientation between distance vector (r) and force vector (F). Their 'lines' are always parallel."
/>

Naturally, the implementation of the Lennard-Jones potential model will be very similar to the one we have seen in the
Jupyter Notebooks, so we will mainly focus on its integration with the rest of the simulation engine. The implementation
of the Lennard-Jones potential model will be located in the `lj.rs` file inside the `models` subdirectory and will
be a simple structure that implements our `CalculatePotential` trait and has the initializer function that will take
the correct input YAML snippet. The snippet in general will look like this:

<CodeSnippet
  language="yaml"
  code={`
    potential:
      model: lj
      parameters:
        epsilon: 0.4080
        sigma: 2.551
      cutoff: 8.5
  `}
  filename="input.yaml (potential definition)"
  annotations={`
    1: The model field will be used to choose the correct potential model
    3-5: The parameters that are specific to the Lennard-Jones potential model
    6: The distance at which we will cut off the potential
  `}
/>

The implementation of the Lennard-Jones potential model will look like this:

<CodeSnippet
  language="rust"
  code={`
    use crate::statics::models::CalculatePotential;
    use yaml_rust::Yaml;

    pub struct LennardJonesModel {
        epsilon: f64,
        sigma: f64,
        cutoff: f64,
    }

    impl LennardJonesModel {
        pub fn initialize(definition: &Yaml) -> LennardJonesModel {
            LennardJonesModel {
                epsilon: definition["parameters"]["epsilon"].as_f64().unwrap(),
                sigma: definition["parameters"]["sigma"].as_f64().unwrap(),
                cutoff: match &definition["cutoff"] {
                    Yaml::Real(cutoff) => match cutoff.parse::<f64>() {
                        Ok(cutoff) => cutoff,
                        Err(_) => panic!("Cutoff must be a real number"),
                    },
                    Yaml::BadValue => definition["parameters"]["sigma"].as_f64().unwrap() * 2.5,
                    _ => panic!("Cutoff must be a real number"),
                },
            }
        }
        fn calculate_potential_at_distance(&self, r: f64) -> f64 {
            let r6 = r.powi(6);
            let r12 = r6.powi(2);
            let potential =
                4.0 * self.epsilon * ((self.sigma.powi(12) / r12) - (self.sigma.powi(6) / r6));
            potential
        }
    }

    impl CalculatePotential for LennardJonesModel {
        fn calculate_potential(&self, distance: f64) -> f64 {
            match distance < self.cutoff {
                true => match distance > 0.0 {
                    true => self.calculate_potential_at_distance(distance),
                    // Lennard-Jones potential is infinite at r = 0
                    false => f64::INFINITY,
                },
                // Lennard-Jones potential is 0 at r > cutoff
                false => 0.0,
            }
        }
        fn calculate_force(&self, distance: f64) -> f64 {
            match distance < self.cutoff {
                true => match distance > 0.0 {
                    true => {
                        let r6 = distance.powi(6);
                        let r12 = distance.powi(12);
                        (24.0 * self.epsilon)
                            * ((2.0 * self.sigma.powi(12) / r12) - (self.sigma.powi(6) / r6))
                    }
                    // Lennard-Jones potential is infinite at r = 0
                    false => f64::INFINITY,
                },
                // Lennard-Jones potential is 0 at r > cutoff
                false => 0.0,
            }
        }
    }
  `}
  filename="src/statics/models/lj.rs"
  annotations={`
    1: We import the trait for defining force and potential calculation
    4-7: We define the structure that will hold the parameters of the Lennard-Jones potential model
    11-24: We implement the initializer for the Lennard-Jones potential model
    16-19: Here we validate cutoff, if it WAS defined in the input YAML
    20: If it wasn't, we calculate it as 2.5 times the sigma parameter (a common practice)
    25-31: This is a helper calculation function, for any distances not equal to zero (Lennard-Jones potential is infinite at r = 0)
    35-45: We implement the method that will calculate the potential energy at a given distance (with a case of r = 0 handled)
    36: We do a last check if the distance is smaller than the cutoff
    46: A similar method for the force acting between particles (only the expression changes)
  `}
/>

Cool, as You can see, the neighbor list is not needed by potential model structures, so we can implement it separately
and then just pass distances recorded between valid particle pairs to the potential model. This will allow us to keep
the potential model implementation clean and focused on its main task - calculating the force or potential energy.
Also, in practice, it is easier to optimize small chunks of an engine, than the sum of its parts and **for sure**
we will need to tweak our simulation thingymajig to make it run faster - that's computational physics for You. üßë‚Äçüî¨üò´
But, thinking from the TDD perspective, we can implement the potential model first and then the neighbor list, so we can
test them in isolation via unit tests and then do integration tests to see if they work together. So, decoupling
is nice. üß©

So, neighbors list, here we go!

### Designing the neighborhood üè°üìê

The neighbor list will be implemented as a separate structure that will hold the list of neighbors for each atom in the
system. Again, it will not deviate much from our crude scaffolding shown via previous Python code. The only difference is
that each entry in the list will be a little, substructure that will hold the ID of the neighbor, distance vector components
and the magnitude of said vector.

<CodeSnippet
  language="rust"
  code={`
    use std::collections::HashMap;
    use nalgebra::Vector3;
    use crate::system::SystemDefinition;
    use rayon::prelude::*;

    #[derive(Debug, Clone)]
    pub struct NeighborsListEntry {
        pub index: u64,
        pub distance_vector: Vector3<f64>,
        pub distance: f64,
    }

    pub struct NeighborsList {
        pub neighbors: HashMap<u64, Vec<NeighborsListEntry>>,
        pub log: bool,
        cutoff: f64,
    }

    impl std::fmt::Display for NeighborsListEntry {
        fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
            write!(
                f,
                "(index: {}, distance_vector: {:?}, distance: {})",
                self.index, self.distance_vector, self.distance
            )
        }
    }
  `}
  filename="src/statics/neighbors.rs"
  annotations={`
    6: We need those traits to print out and copy individual entries of the neighbor list
    7-10: This structure will hold the ID of the neighbor, distance vector and its magnitude
    14:HashMap and O(1), baby üèéÔ∏è
    19-27:We want to debug out the neighbor list, so we implement the Display trait
  `}
/>

So far so good, we defined how the data is kept inside the list, with an additional switch for logging the operations
as a `bool` field in the `NeighborsList` struct. But, You know what comes after the "class" (a semi-heresy in Rust context here,
but practically we are botching together something like that here) data structure definition, right?

<img src="/assets/images/por_2_list_behavior.gif" alt="Tests!" style="margin-left:auto;margin-right:auto;height:200px;"/>

First we will create a method that will initialize the neighbor list using, similarly to the potential model, the input YAML
section called `neighbors`. This will allow us to set the cutoff distance for the neighbor list:

<CodeSnippet
  language="yaml"
  code={`
    neighbors:
      cutoff: 8.5
      log: false
  `}
  filename="input.yaml (neighbor list definition)"
  annotations={`
    2: The distance at which we will cut off the neighbor list
    3: If we want to log the operations of the neighbor list
  `}
/>

You may notice that we are defining the cutoff distance in two places in our simulation input script. This is because
the cutoff distance for the neighbor list and the potential model can be two different things. Some potentials
may require widening the neighborhood of each atom to calculate the potential energy of the system, due to its inherent
definition, for example, to account only for short-range interactions, while the neighbor list will be used to calculate
can be constructed to account for long-range distances for other purposes such as structural analysis of the system's structure.
Thus, I allow here to define both of those distances independently, but in practice, they will be the same in most cases.

So we can implement the initializer for the neighbor list:

<CodeSnippet
  language="rust"
  code={`
    impl NeighborsList {
      pub fn from(neighbors_settings: &yaml_rust::Yaml) -> NeighborsList {
          NeighborsList {
              neighbors: HashMap::new(),
              cutoff: match &neighbors_settings["cutoff"] {
                  yaml_rust::Yaml::Real(cutoff) => match cutoff.parse::<f64>() {
                      Ok(cutoff) => cutoff,
                      Err(_) => panic!("Cutoff must be a real number"),
                  },
                  _ => panic!("Cutoff must be a real number"),
              },
              log: match &neighbors_settings["log"] {
                  yaml_rust::Yaml::Boolean(log) => *log,
                  _ => panic!("Log must be a boolean"),
              },
          }
      }
    }
  `}
  filename="src/statics/neighbors.rs (init)"
  annotations={`
    5-11:A very similar validation as for potential list cutoff
    12-15:We validate the log field as the correct boolean
  `}
/>

Looking at the iteration performed inside `update` methods for the `PotentialModel` structure, we can see that we will
need a method that will return the neighbors of a given atom. This will allow us to iterate over them and calculate
the forces acting between particles. This method will be implemented in the `NeighborsList` structure and will look like this:

<CodeSnippet
  language="rust"
  code={`
    ...
    pub fn get_neighbors(&self, index: u64) -> Vec<NeighborsListEntry> {
        match self.neighbors.get(&(index as u64)) {
            None => {
                println!("No neighbors for index {}", index);
                vec![]
            }
            Some(neighbors) => neighbors.clone(),
        }
    }
    ...
  `}
  filename="src/statics/neighbors.rs (getter)"
  annotations={`
      3:Here we check if the entry for atom with the given index exists in the HashMap
      4-7:If it doesn't, we print out a message and return an empty vector
      8:If it does, we return a copy of the vector of the neighbors
  `}
/>

Why the `clone` though? Shouldn't we be able to access the vector of `NeighborsListEntry` directly? Well, we can't, because
**the ownership of this value is held by the `NeighborsList`** structure and we can't just give it away to the `PotentialModel`
and therefore - a copy is needed. A small sacrifice in terms of memory, but usually the number of neighbors for each atom
is very small, so it won't be a big deal. ü•≥

Now the most juicy part - the method that will update the neighbor list. This method will be called at the beginning of
each multiple of a predefined number of iterations and will update the neighbor list for each atom in the system. Prepare
Yourself for a lot of `for` loops and `if` statements, because we will need to iterate over all pairs of atoms in the
system and check if they are neighbors. This will be a very time-consuming operation, so we will use the `rayon` crate
to parallelize this operation and speed it up. The method will look like this:

<CodeSnippet
  language="rust"
  code={`
      pub fn update(&mut self, system: &mut SystemDefinition) -> () {
          self.neighbors.clear();
          system
              .atoms
              .iter()
              .enumerate()
              .for_each(|(index, _)| self.update_for_atom(index, system));
      }
  `}
  filename="src/statics/neighbors.rs (update)"
  annotations={`
    2:YEET ‚ÜóÔ∏è the old neighbors list
    5:Here we can't use par_iter because system is already borrowed mutably!
    7:We iterate over all atoms in the system and update the neighbors list for each of them
  `}
/>

Okay, okay, seems like we are ready to implement the `update_for_atom` method. It will look like this:

<CodeSnippet
  language="rust"
  code={`
      fn update_for_atom(&mut self, index: usize, system: &SystemDefinition) -> () {
          let new_neighbors = system
              .atoms
              .par_iter()
              .enumerate()
              .filter(|(j, _)| *j != index)
              .map(|(neighbor_index, neighbor)| {
                  let mut distance_vector = Vector3::<f64>::new(
                      neighbor.current.position[0]
                          - system.atoms[index as usize].current.position[0],
                      neighbor.current.position[1]
                          - system.atoms[index as usize].current.position[1],
                      neighbor.current.position[2]
                          - system.atoms[index as usize].current.position[2],
                  );
                  system
                      .simulation_box
                      .vectors
                      .row_iter()
                      .for_each(|basis_vector| {
                          let basis_vector =
                              Vector3::<f64>::new(basis_vector[0], basis_vector[1], basis_vector[2]);
                          let distance_vector_projection =
                              distance_vector.dot(&basis_vector) / basis_vector.norm();
                          let relative_coordinate = distance_vector_projection / basis_vector.norm();
                          let minimum_image_coefficient =
                              (relative_coordinate <= -0.5) as i64 as f64 * -1.0;
                          if minimum_image_coefficient != 0.0 {
                              distance_vector += minimum_image_coefficient * basis_vector;
                          };
                      });
                  NeighborsListEntry {
                      index: neighbor_index as u64,
                      distance_vector: distance_vector,
                      distance: distance_vector.norm(),
                  }
              })
              .filter(|entry| entry.distance < self.cutoff)
              .collect::<Vec<NeighborsListEntry>>();
          self.neighbors
              .insert(index.try_into().unwrap(), new_neighbors);
      }
  `}
  filename="src/statics/neighbors.rs (update p.2)"
  annotations={``}
/>

<CaptionedImage
  src="/assets/images/por_2_im_tired_boss.gif"
  alt="The reality of implementing the neighbor list kicks in"
/>

#### Fear is the mind killer üß†üó°Ô∏è

Okay, okay. Let's break down this method above into smaller parts, because it is a little bit overwhelming at first glance. First, we
iterate over all atoms in the system and for each of them, we iterate over all other atoms in the system and calculate
the distance between them. We use the `par_iter` method from the `rayon` crate to parallelize this operation and speed
it up. We filter out the atom that we are currently iterating over and then calculate the distance between them. After those
initial steps, we can calculate the distance vector between the atoms:

<CodeSnippet
  language="rust"
  code={`
      let new_neighbors = system
          .atoms
          .par_iter()
          .enumerate()
          .filter(|(j, _)| *j != index)
          .map(|(neighbor_index, neighbor)| {
              let mut distance_vector = Vector3::<f64>::new(
                  neighbor.current.position[0]
                      - system.atoms[index as usize].current.position[0],
                  neighbor.current.position[1]
                      - system.atoms[index as usize].current.position[1],
                  neighbor.current.position[2]
                      - system.atoms[index as usize].current.position[2],
              );
  `}
  filename="src/statics/neighbors.rs (distance vector)"
  annotations={`
    2-3:We iterate over all atoms in the system with parallel iterators
    5:We filter out the atom that we are currently updating (its index is passed to method)
    8-13:We calculate RELATIVE distance vector between the atoms
  `}
/>

Next, we must account here for the fact that the atoms are located in a periodic box i.e. the unit cell containing our atoms
is replicated in all three dimensions to form a lattice. This means that there will be interactions between atoms that are
close to the edges of individual unit cells and we have to account for that in our calculations. This is called the
[**minimum image convention**] and is a very common practice in molecular dynamics simulations.

#### The absolute minimum üìâ

It can be visualized
using a simple 2D grid, where the atoms are located in the corners of the cells and the neighborhood cutoff radius
creates a circle around each atom. If we don't account for the periodicity of the system, we will miss some interactions
between pairs that are enclosed by this shape (as seen in the [image below]).

<CaptionedImage
  src="/assets/images/por_2_minimum_image.png"
  alt="Illustration of the minimum image convention"
  height={300}
/>

Does this mean that we have to put some more mathematical squiggles into our way of thinking and then, seemingly
by pure happenstance, manage to implement them using the language of a simple, OOP-loving man? **Hell, no**. There are
two simple ‚úåÔ∏è rules:

1. If the distance vector between atoms is smaller than half of the length of the box in a given dimension,
we don't have to do anything, because the atoms are close enough to each other and the distance vector is already
the minimum distance between them.
2. If the distance vector is larger than half of the length of the box in a given dimension. If yes, then **probably**
there is a copy of the atom in the neighboring cell that is closer to the atom we are currently updating the neighbor list for.
So, just substract or add the length of the box in a given dimension to the distance vector and then check if the new
distance is smaller than cutoff radius.

Just take a look at the image above and the atoms that are marked with $K$ letters.
For the $K$ atom and our black dot, the distance along the horizontal axis is larger than half of the square's side,
so if we shift its horizontal position by this distance, we end up with the $x$ coordinate of the $K'$ atom. Do this
again fo the vertical coordinate and You will find that even though $K$ and the black atom are not neighbors **per se**,
the replica of $K'$ is located within the cutoff radius of the black atom. This is the minimum image convention in a nutshell. ü•ú
Of course, the same reasoning holds for the other atoms, so we have to check all three dimensions and do the shifting
operation whenever we encounter a distance component that is larger than half of the box length in this component dimension.
That is what's happening in the `for_each` block seen within the `update` method. But instead of just simply
reading the vector components in the $(x,y,z)$ coordinate system, we have to project the distance vector onto the basis
vectors of the simulation box. Why? Because we cannot be sure that the box is aligned with the coordinate system e.g.
the box can be rotated in space or the box is not a cube, so the basis vectors will not be aligned with the coordinate
system.

This difference in coordinate systems can be seen in the picture below. The $x$, $y$ and $z$ axes are the global coordinate axes
and the $a$, $b$ and $c$ axes are the basis vectors of the box. The distance vector is projected onto the basis vectors, which
allows us to express its components in terms of how many $a$'s, $b$'s and $c$'s vectors are needed to express in form
of their combination, so this operation looks somewhat like this:

$$\vec{r}_{xyz} = (r_x, r_y, r_z) \Rightarrow \vec{r}_{abc} = \left(\dfrac{\vec{r}\cdot\vec{a}}{\vert\vert\vec{a}\vert\vert},\dfrac{\vec{r}\cdot\vec{b}}{\vert\vert\vec{b}\vert\vert},\dfrac{\vec{r}\cdot\vec{c}}{\vert\vert\vec{c}\vert\vert}\right) = (r_a, r_b, r_c)$$

<CaptionedImage
  src="/assets/images/por_2_triclinic_cell.gif"
  alt="Illustration of position decomposed onto the basis vectors (blue) and the global coordinate system (red)"
  height={400}
/>

As You can see, we have to calculate some dot (scalar) products between the distance vector and the basis vectors and then
divide them by the length of the basis vectors to get the relative coordinates of the distance vector in the basis vector
coordinate system. Then we can check if the relative coordinates are smaller than $-0.5$ or larger than $0.5$ and if yes,
we have to shift the distance vector by the length of the box in a given dimension. Luckily, the `nalgebra` crate has
a method for calculating the dot product and the length of the vector, so we don't have to do this by hand. Remember,
**we don't need custom algebra library**. üö´üìö

<CodeSnippet
  language="rust"
  code={`
      // Our previous distance vector calculation
      ...
      system
          .simulation_box
          .vectors
          .row_iter()
          .for_each(|basis_vector| {
              let basis_vector =
                  Vector3::<f64>::new(basis_vector[0], basis_vector[1], basis_vector[2]);
              let distance_vector_projection =
                  distance_vector.dot(&basis_vector) / basis_vector.norm();
              let relative_coordinate = distance_vector_projection / basis_vector.norm();
              let minimum_image_coefficient =
                  (relative_coordinate <= -0.5) as i64 as f64 +
                  (relative_coordinate >= 0.5) as i64 as f64 * -1.0;
              if minimum_image_coefficient != 0.0 {
                  distance_vector += minimum_image_coefficient * basis_vector;
              };
          });
          NeighborsListEntry {
              index: neighbor_index as u64,
              distance_vector: distance_vector,
              distance: distance_vector.norm(),
          }
      })
      .filter(|entry| entry.distance < self.cutoff)
  `}
  filename="src/statics/neighbors.rs (minimum image)"
  annotations={`
    7:Here we calculate the projection of the distance vector onto the basis vector
    10-12:Here we do the projection of distance onto a chosen basis vector
    13-18:Here, the minimum convention is implemented
    20:Create a temporary entry for the neighbor list
    26:Filter out the neighbors that are further than the cutoff radius
  `}
/>

The `minimum_image_coefficient` is a little bit of a hack or a sneaky one-liner,
but it can be replaced by equivalent `match` statement, like so:

<CodeSnippet
  language="rust"
  code={`
    let minimum_image_coefficient = match relative_coordinate {
        x if x <= -0.5 => 1.0,
        x if x >= 0.5 => 1.0,
        _ => 0.0,
    };
  `}
  filename="minimum_image_coefficient v.2"
  annotations={``}
/>

#### Fingers crossed ü§û

Cool! After collecting all of the neighbors for a given atom, we insert them into the `HashMap` of the `NeighborsList`
structure and we are done with the `update_for_atom` method. Let's check if it constructs the map of our neighbors correctly.
For a system composed of 2 cells across each dimension in the case of previously investigated FCC aluminum,
we can see that the neighbors list is constructed (cutoff was set as $r_\mathrm{cut}=2.87 \AA$ and the lattice
constant was set as $a=4.04 \AA$). The neighbors list for the $30$-th (i.e. random) atom in the system is shown below:

<CodeSnippet
  language="json"
  code={`
    {
      ...
      30: [NeighborsListEntry {
            index: 4,
            distance_vector: [
                [2.0200000000079434, 2.020000000007962, -4.440892098500626e-15]
            ],
            distance: 2.8567113960048993
        }, NeighborsListEntry {
            index: 9,
            distance_vector: [
                [2.0200000000079514, 1.7204015989591426e-12, -2.0200000000017098]
            ],
            distance: 2.8567113960004837
        }, NeighborsListEntry {
            index: 12,
            distance_vector: [
                [2.0200000000073857, -2.0199999999917475, 8.517631044924201e-13]
            ],
            distance: 2.856711395993039
        }, NeighborsListEntry {
            index: 13,
            distance_vector: [
                [2.0200000000102403, 2.957634137601417e-13, 2.019999999992895]
            ],
            distance: 2.856711395995869
        }, NeighborsListEntry {
            index: 19,
            distance_vector: [
                [1.709743457922741e-12, 2.020000000007962, -2.0200000000017098]
            ],
            distance: 2.8567113960004913
        }, NeighborsListEntry {
            index: 20,
            distance_vector: [
                [-2.019999999991758, 2.0200000000073963, 8.517631044924201e-13]
            ],
            distance: 2.856711395993054
        }, NeighborsListEntry {
            index: 23,
            distance_vector: [
                [2.7977620220553945e-13, 2.0200000000102403, 2.019999999992895]
            ],
            distance: 2.856711395995869
        }, NeighborsListEntry {
            index: 25,
            distance_vector: [
                [-2.0199999999917555, 1.4352963262354024e-12, -2.020000000001707]
            ],
            distance: 2.8567113959890293
        }, NeighborsListEntry {
            index: 27,
            distance_vector: [
                [1.4246381851990009e-12, -2.0199999999917404, -2.020000000001707]
            ],
            distance: 2.8567113959890187
        }, NeighborsListEntry {
            index: 28,
            distance_vector: [
                [-2.0199999999909073, -2.0199999999908966, 1.7132961716015416e-12]
            ],
            distance: 2.8567113959807857
        }, NeighborsListEntry {
            index: 29,
            distance_vector: [
                [-2.0199999999926144, 1.0658141036401503e-14, 2.0199999999926046]
            ],
            distance: 2.8567113959832002
        }, NeighborsListEntry {
            index: 31,
            distance_vector: [
                [0.0, -2.019999999992594, 2.0199999999926046]
            ],
            distance: 2.856711395983186
        }]
        ...
    }
  `}
  filename="Neighbours of one atom"
  annotations={``}
/>

As You can see, a magic number of $12$ neighbors was found for an atom in a face-centered cubic aluminum system **AND**
that is exactly the result we would be expecting for this system. The distance vectors are also correct since the 
value for the cutoff chosen was not really random. If You visualize the system, You will see that atoms located at
the centers of each face of the unit cell are the closest neighbors to atoms in the corners of those cubes. Therefore,
the distance between them MUST be equal to half of the diagonal of the cube, which is exactly the value of the cutoff
radius that we have chosen: $r_\mathrm{cut} = \dfrac{a\sqrt{2}}{2} = 2.87 \AA$ (and recall that we were using $a=4.04 \AA$).
To get a more visual explanation of this, You can check the [physics-in-a-nutshell] article, where You can find
a very nice animation of the face-centered cubic lattice and the distances between atoms in it.

This observation is also true for the other atoms in the system, so we can be sure that our neighbor list is working correctly.
We did it! Our statics part of molecular dynamics simulation is ready to be integrated with the dynamics part (soon‚Ñ¢). üéâ

### Summary

In this part, we have implemented the statics part of our molecular dynamics simulation. We have defined the potential
models and the neighbor list, which will be used to calculate the forces acting between particles. Right now we have
a working definition of the system of particles frozen in time, so we can carry on to the more chaotic side of the whole
story. üí£

Next, we will be handling two things: the dynamics part of the simulation and the integration of the statics and dynamics.
This will also mean that we would have to implement the time integration algorithm and somehow **unify** all units of the
quantities used throughout our simulation via some unit system. This will be a little bit of a hassle, but we will manage -
as You will see in the next part of this series. üöÄ

If You have soldiered through this esoteric adventure so far, I am very grateful for Your time and attention. I hope
I've managed to sneak in some snippets of a topic that, even though it is a little bit niche, is very interesting and
definitely still has a little cozy corner in my heart. üß°

["boioioioing" thing]: https://www.youtube.com/watch?v=LPaIS51XGC0
[physics-animations.com]: https://www.physics-animations.com
[literally one minute]: https://www.ucl.ac.uk/~ucfbasc/Theory/lenjon.html
[**neighbor list** (or Verlet list)]: https://en.wikipedia.org/wiki/Verlet_list
[`Simulation` structure, which was a composition of smaller parts]: https://github.com/kamilrybacki/rustomics/blob/0b7294acc05e9408d455ea365a0c2e1925ec568a/src/simulation.rs#L46
[MEAM]: https://www.potfit.net/wiki/doku.php?id=interactions:meam
[source]: https://courses.lumenlearning.com/suny-physics/chapter/18-4-electric-field-concept-of-a-field-revisited/
[**minimum image convention**]: https://en.wikipedia.org/wiki/Periodic_boundary_conditions#(A)_Restrict_particle_coordinates_to_the_simulation_box
[image below]: https://www.researchgate.net/figure/Ilustration-of-the-periodic-boundary-conditions-PBC-method-with-the-minimum-image_fig9_279264054
[physics-in-a-nutshell]: https://www.physics-in-a-nutshell.com/article/11/close-packed-structures-fcc-and-hcp
