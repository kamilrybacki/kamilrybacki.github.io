---
title: "Containerized Integration"
date: "2023-11-30"
description: "How to easily spin up a mock environment for integration testing"
tags: ["gitlab", "ci", "tests", "docker", "containers", "integration"]
image:
  thumbnail: "components"
  hero: "components"
  alt: "Lab in a box"
---

## I'm afraid I can't let you do that, Dev ü§ñ

There is a certain kind of dread that comes with the acknowledgment that You can't always cross the cap
between Your local pythonic incantations and the desired environment, where those enchantments are to be
set to be roaming free, setting and getting those `os.environ`'s. "Being part of the Machine" is not only
a figurative thing that a societal contrarian might say, but - in terms of software development - it is
also a very literal thing. You design cogs of various sizes and shapes, that are to be put in the overarching
conglomerate of multi-lingual, multi-paradigm and multi-purpose mechanisms.

What is actually contrary to popular belief, the net is not about connecting people and/or machines at the upper echelons
of the enterprise-ish hierarchy. It's more often than not a convoluted web of rules and blockades,
that separate this connectivity from one environment to another - be it different teams of developers,
different services (often at their different stages) or in general the intra/internet gatekeeping conundrum.

This is, of course, done with good intent, so that the developers don't accidentally
set the production database to be the target of their unit tests, pushing around
valuable data about somebody's World of Warcraft character on the private server
run by a group of "real chill dudes", where You've recently managed to down
that one annoying boss with Your guild, **and** got the highest roll on [Crystalheart Pulse-Staff] üòå,
to the dismay of Your fellow restoration druids üå≤.

Those are the real casualties that would have been made without that one reasonable sysadmin
at Your software house, who's set up e.g. that VPN so all GitLab CI workers can't reach the K8s cluster,
where the actual services may be running. Additionally, he will also (rightfully) hunt You down
if You try to copy any credentials to those temporary nodes, to "cleverly" bypass those security measures.

I would like to stress here that I am not talking about any continuous deployment (CD) pipelines,
where the code is automatically deployed to the production environment and it mutates its state, in real time.
CD naturally assumes successful CI (continuous integration) and by integration, it means that the code
is already up to the security and coding standards of Your organization. In other words, it is **safe**.
The CI part is what I am really focusing on in this article, so - I am not GitOps hater or something like that üòÖ.

The thing is, some tasks require a bit more than just simple code linting tests and they must
touch upon another important aspect of integration with the existing environment - communication,
more often than not, in the form of authorized API requests
or, in general, calls made through specified protocols.

So we arrive at the "Schr√∂dinger's code problem" i.e. the code that must talk with our services, but really
can't talk with our services at the same time. But what is to be tested here? Language i.e. form.
In other words, we only care if our code can communicate **at all** with the service,
not really if it retrieves a specific portion of the data. The rest can be mocked.
So basically, if You can't play with the real tools - You can at least play with the toys,
and that is what mock test environments are for. An approximation.

In this article, I will try to show You how I spin up those sandboxes for my integration tests,
using GitLab CI Services and Docker, with additional tips for unit testing Python code via containerized
`pytest` nested runners.

**IMPORTANT NOTE**: This article assumes that there is a GitLab CI runner available in Your organization,
that is capable of Docker-in-Docker (DinD) builds. I've included some links to tutorials on how to set up
such a runner, but I will not go into details on how to do that, as it is not the main focus of this article.

## Alive in a box - dead to the outside world üêà‚Äç‚¨õüì¶

To figure out how to create our pseudo-environment, we must first understand what goes on under the hood
when our job is being executed on the GitLab CI runner. In very short terms, the runner
pulls the Docker image specified in the `.gitlab-ci.yml` for a given job and spins up a container,
which has access to the project's files and the GitLab CI variables. The container then executes
the commands specified in the `script` section of the job. The container is then destroyed,
along with all the changes made to the filesystem, unless we explicitly tell the runner to persist
some files between the jobs as either artifacts or cache.

Take a look at the following example:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest 
      script:
        - echo "I am running!"
        - |
          docker \\
            run \\
              alpine:latest \\
              sh \\
                -c \\
                  "echo 'I am running inside the container!'"
  `}
  language="yaml"
  filename=".gitlab-ci.yml"
  annotations={`
    2:We use the docker image to be able to run docker commands inside the container
    4:This command is executed at the uppermost container level
    5:Here we want to communicate to Docker daemon on the host that we want to nest a new container
    12:This command is executed inside the nested container
  `}
/>

A quick side note - if You see me using the `\` operator in the code snippets, it is just a way to
break the line in the `.gitlab-ci.yml` file (or other manifests/shell scripts), so that it is more readable. 
I like to separate the flags and arguments of the commands into separate lines, with indentations added to, for example,
flag values to see which keyword belongs to which flag. It is not necessary, but I find it
more readable than a single line with a bunch of flags and arguments, separated by spaces.

So, coming back to CI, we expect to be greeted by two messages, one from the host and one from the nested container.
However, when we run this job, we will only see the first message executed at the main container level,
but the second one will not be executed at all:

<CaptionedImage
  src="/assets/images/nested_container_no_socket.png"
  alt="Dude, where is my daemon? ü§î"
  height="500"
/>

The main reason is the way in which the parent container attempts to communicate with the Docker daemon
that is located on the host. The original Docker daemon is listening on a Unix socket, which is located at
`/var/run/docker.sock` on the host machine. 

The parent container, however, does not have access to this socket,
because it is not mounted to the container's filesystem. This is what line number 21 in the log printout above
is really about. The nested container tries to look for a way to communicate with its parent's daemon,
so it checks if the Docker socket is mounted into its filesystem. 

If not (which is the case here), it then tries to communicate with an external Docker **service**, 
specified under the `DOCKER_HOST` environment variable. **By default**, this variable (for GitLab runners)
is set to `tcp://docker:2375`, which translates to URL of: `http://docker:2375`.
And this is the reason why the nested container prints out the error message about the failed `TCP` lookup.

We can confirm this behavior by listing the sockets mounted to the nested container under `/var/run` path
and setting the `DOCKER_HOST` envvar to some bogus value like `tcp://docker-service:2375`:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      variables:
        DOCKER_HOST: "tcp://docker-service:2375"
      script: |
        - echo "I am running!"
        - |
          ls \\
            -la \\
            /var/run/
        - |
          docker \\
            run \\
              alpine:latest \\
              sh \\
                -c \\
                  "echo 'I am running inside the container!'"
  `}
  language="yaml"
  filename=".gitlab-ci.yml (checking behaviour)"
  annotations={`
    4:We point to some other Docker service
    7-9:Check for mounted Docker socket in the nested container
  `}
/>

Here we basically expect the overall effect to be the same as previously, but this time we will see
that there is basically nothing mounted under `/var/run` path and the `TCP` lookup will be performed for
the `docker-service` host, which will fail, as expected.

<CaptionedImage
  src="/assets/images/nested_container_behavior.png"
  alt="Bingo! üëç"
  height="500"
/>

And this is what we get, the `/var/run` directory is empty (red box in Pic. 2)
and the `TCP` lookup fails (yellow underline in Pic. 2). So now we have to decide what is really open to us
as a solution to open up the communication between the nested container and the host's Docker daemon.
The first thing that comes to mind is to simply delegate the creation of any new Docker containers 
to an autonomous **Docker service**, that will be set up by the runner and reachable from the nested container.
This can be achieved by using the [`services` keyword in the `.gitlab-ci.yml` file]. 
Those services are basically Docker containers spawned beside the CI container itself, which 
are accessible from the tests container due to the network being shared
between all of the sub-hosts present on the runner.

**BUT** there is a catch. Using Docker-in-Docker (DinD) approach requires an additional configuration of the runner itself,
that can be then used by the CI jobs as a shared runner by specifying the `tags` keyword in the `.gitlab-ci.yml` file.
In most cases, kind of runners are already present as a CI/CD tool for software development teams, 
because, for example, building Docker images is a very common task among them, so we can already capitalize on that.
However, if there is no such runner available, we can always create one ourselves. There is a [plethora] [of] [tutorials] 
on how to do that, so I will **assume that this kind of resource is already present in our organization**.

### To test and to serve üö®

So the game plan now is like so:

1. We will tell GitLab CI to use the runner with the predefined tags set up on it by the administrator.
2. We create a Docker service that will be used by the nested container to communicate with the host's Docker daemon.
We will name it `my-docker-service` and we will use the `docker:dind` image for that purpose.
3. We will set the `DOCKER_HOST` environment variable to `tcp://my-docker-service:2375` in the CI container.
4. We will run the command in the nested container and hope for the best. ü§û

One caveat is that if we want to use the 2375 port, we need to disable TLS verification, which can be done
by setting the `DOCKER_TLS_CERTDIR` environment variable to an empty string. This is because the runner
is not configured to use TLS by default. We can also set the `FF_NETWORK_PER_BUILD` environment variable
to `true`, to enable the network sharing between the containers, which will be useful in a second when we will
configure our mock test environment in a sec. Any `docker` command within the CI container will also need to use
an additional `--network=host` flag to use the CI container network that is shared with the DinD service.

All of these steps can be performed by modifying the `.gitlab-ci.yml` file like so:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      tags:
        - "docker"
        - "dind"
      services:
        - name: docker:dind
          alias: my-docker-service
      variables:
        DOCKER_HOST: "tcp://my-docker-service:2375"
        DOCKER_TLS_CERTDIR: ""
        FF_NETWORK_PER_BUILD: "true"
      script: |
        - echo "I am running!"
        - |
          docker \\
            run \\
              --network=host \\
              alpine:latest \\
              sh \\
                -c \\
                  "echo 'I am running inside the container!'"
  `}
  language="yaml"
  filename=".gitlab-ci.yml (working)"
  annotations={`
    2:We use the docker image to be able to run docker commands inside the CI container
    3-5:We tell GitLab CI to use the runner with the predefined tags set up on it by the administrator
    6-8:We create a Docker service that will be used by the nested container to communicate with the host's Docker daemon
    10:We set the DOCKER_HOST environment variable to 'tcp://my-docker-service:2375' in the CI container
    11:We set the DOCKER_TLS_CERTDIR environment variable to an empty string, to disable TLS verification
    12:We set the FF_NETWORK_PER_BUILD environment variable to true, to enable the network sharing between the containers
    13:BTW, I've removed the ls command since it's not really needed here
    17:We use the --network=host flag to access the DinD container via the host's bridge network
  `}
/>

<CaptionedImage
  src="/assets/images/nested_container_dind.png"
  alt="Whale communication channel established üê≥"
  height="700"
/>

So what we have achieved here is that we have successfully created a workflow for creating environments
made up of multiple containers that can share a network and communicate with each other via the host's bridge network.
What is also useful is that services that are spawned via the `services` keyword are automatically resolvable
by their names, so we don't have to worry about the IP addresses of those containers. We can show it with
a quick `ping-pong` test for a Redis database hosted both as a service and as a nested container.
That nested container will then try to send a `PING` signal via `redis-cli` to the Redis service
named `some-random-cache`. This can be done with the following `.gitlab-ci.yml` file:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      tags:
        - "docker"
        - "dind"
      services:
        - name: docker:dind
          alias: my-docker-service
        - name: redis:latest
          alias: some-random-cache
      variables:
        DOCKER_HOST: "tcp://my-docker-service:2375"
        DOCKER_TLS_CERTDIR: ""
        FF_NETWORK_PER_BUILD: "true"
      script: |
        - echo "I am running!"
        - |
          docker \\
            run \\
              --network=host \\
              redis:latest \\
              sh \\
                -c \\
                  "redis-cli -h some-random-cache -p 6379 PING"
  `}
  language="yaml"
  filename=".gitlab-ci.yml (Redis test)"
  annotations={`
    9:We create the target Redis service
    23:We use the -h flag to specify the hostname and -p flag to specify the host and port of the Redis service
  `}
/>

<CaptionedImage
  src="/assets/images/nested_container_ping_pong.png"
  alt="The ping has been ponged üèì"
  height="700"
/>

### Snake-in-a-box üêçüì¶ (but not that hypercube one)

Cool! So now we can spawn whatever we want inside our CI runner and make it communicate with each other,
but what about the actual integration tests? Well, we can use the same approach, but just changing the
type of image used by the nested container to the one containing our tests. Let's say we have a Python project
with some integration tests that require a Redis database to be present. The files can be structured like so:

<FileTree
  root="project/"
  tree={[
    ".env",
    "connector.py",
    "test_connector.py",
    "requirements.txt"
  ]}
  annotations={`
    .env:A way to introduce env variables to the nested container (for now).
    connector.py:Code with primitive adapter to Redis database.
    test_connector.py:Integration tests for the connector.py module.
    requirements.txt:Python dependencies for the connector.py module.
  `}
/>

The `connector.py` module will use the `redis` library to connect to the database 
and the `test_connector.py` module will contain tests for our new class. 
The `requirements.txt` file will contain the `redis` library as a dependency
to ensure that the runner container will have all of the required modules installed. So what does
the `connector.py` module has to offer? A janky üçë wrapper around the basic `redis.StricRedis` methods
that will allow us to connect to the database and set/get some values from it:

<CodeSnippet
  code={`
    import dataclasses
    import logging
    import typing

    import redis

    @dataclasses.dataclass
    class RedisConnector:
        host: str = "localhost"
        port: int = 6379
        db: int = 0
        _session: redis.StrictRedis = dataclasses.field(init=False, default_factory=redis.StrictRedis)

        def __post_init__(
            self,
        ) -> redis.StrictRedis:
            try:
                self._session = redis.StrictRedis(
                    host=self.host,
                    port=self.port,
                    db=self.db
                )
                logging.info("Redis connection established")
                return self._session
            except Exception as connection_failed:
                logging.error("Redis connection failed")
                logging.error(connection_failed)
                raise connection_failed

        def __del__(self) -> None:
            self.disconnect()

        def disconnect(self) -> None:
            if self._session:
                self._session.close()
                logging.info("Redis connection closed")

        def __getitem__(self, key) -> typing.Any:
            return self._session.get(key)

        def __setitem__(self, key: str, value: typing.Any) -> None:
            self._session.set(key, value)
  `}
  language="python"
  filename="connector.py"
  annotations={`
    7:We will use dataclasses to simplify the initialization of the class
    12:This field will store the already established connection to the Redis database
    14-28:Here we try to connect to the Redis database and log the result
    30:Here we define the destructor for the class, which will close the connection
    33:Here we define the method for closing the connection
    38-42:Here we define the setter and getter for the Redis database
  `}
/>

This may not be the apex of the Pythonic design, but it will do for our purposes. The `test_connector.py` module
is just a basic implementation of the `ping-pong` test, that utilizes the `.ping()` method of the `redis.StrictRedis`
class:

<CodeSnippet
  code={`
    import os

    import connector


    def test_redis_connection():
        target_host = os.environ.get("REDIS_HOST")
        target_port = os.environ.get("REDIS_PORT")
        target_db = os.environ.get("REDIS_DB")
        redis_instance = connector.RedisConnector(
            host=target_host,
            port=target_port,
            db=target_db
        )
        assert redis_instance._session.ping()
  `}
  language="python"
  filename="test_connector.py"
  annotations={`
    7-9:We retrieve the environment variables from the nested container
    15:We reach to the underlying redis.StricRedis class and use its .ping() method
  `}
/>

Here we can see the purpose of `.env` file - itwill contain the two environment variables 
that are to be used by the connector class to, well, ... connect to the Redis database: 
`REDIS_HOST`, `REDIS_PORT` and `REDIS_DB`. So we want to do the following things:

1. Set up a temporary Redis database.
2. Mount the repository with out code to a `pytest` runner container.
3. Install the required dependencies to carry out the unit tests.
4. Run `pytest` and (as previously) - hope for the best. ü§û

The main goal of these tests will be to check if our interface is compatible with the one
offered by the `redis` library. So we will not be testing the actual data retrieval, but rather
the ability to retrieve the data at all. A simple case, that we will, of course, complicate in a bit.
So, the `.gitlab-ci.yml`:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      tags:
        - "docker"
        - "dind"
      services:
        - name: docker:dind
          alias: my-docker-service
        - name: redis:latest
          alias: some-random-cache
      variables:
        DOCKER_HOST: "tcp://my-docker-service:2375"
        DOCKER_TLS_CERTDIR: ""
        FF_NETWORK_PER_BUILD: "true"
      script: |
        - echo "I am running!"
        - |
          docker \\
            run \\
              --network=host \\
              --volume $PWD:/app \\
              --workdir /app \\
              --env-file .env \\
              python:3.10 \\
              sh -c "pip install -r requirements.txt pytest && pytest"
  `}
  language="yaml"
  filename=".gitlab-ci.yml (testing connector)"
  annotations={`
    20:We mount the WHOLE repo to some path in the container
    21:We set the working directory to the mounted repo
    22:We set the environment variables from the mounted .env file
    24:We install the required dependencies and run the tests
  `}
/>

The value for the `REDIS_HOST` environment variable will be `some-random-cache` and the value for the `REDIS_PORT`
environment variable will be `6379`. The `REDIS_DB` environment variable will be set to `0` by default.
As it can be seen in the log snippet below, the tests have passed (I've omitted the initial setup logs,
since it they are almost identical to previous ones - only this time the `python:3.10` image is pulled ü§∑):

<CaptionedImage
  src="/assets/images/nested_container_pytest_pong.png"
  alt="The ping has been ponged again, programmatically üêçüèì"
  height="700"
/>

### Matryoshka of problems ü™Ü

So we have successfully created a mock test environment for our integration tests.
It is self-contained and somewhat "invisible" (or dead üíÄ) to the outside architecture of the organization,
but we can see it is alive and kicking from the inside üò∏. However, some things about it may (and in practice - often will) 
create some practical issues. üòø

The first one is the fact that we have stored our environment variables in a file that is mounted to the container. 
This is not a good practice, because we are basically storing our secrets in plain text and totally negating the existence of any
sensitive data protection measures that we may have in place e.g. Vault or some other secrets management tool.
Also, GitLab can use environment variables defined in CI/CD settings of the repo, so we can use that
to our advantage.

The second one is that if we want to spawn more than one container via `docker` command, we will have to
create long `script` sections in our `.gitlab-ci.yml` file, which will be hard to maintain and read.
Moreover, if we would like to pass the aforementioned environment variables to the nested containers,
we would have to do it manually for each of them, using the `--env` flag. This is not a good practice either,
because we are basically repeating ourselves and we are not really using the full potential of the containerization.
Also - container dependencies. If we want to spawn a container that depends on another container, we would have to
create a script that will wait for the first container to be ready and then spawn the second one, manually.
And how do we define that the first container is ready? Well, we would have to create a healthcheck script.
And mount it to the container, under a predefined path for Docker daemon to use.

The third issue that may pop up from time to time - network management. Sometimes, we want to be able to
use raw IP addresses of the containers, instead of their DNS-resolved names. In some cases, it may even speed up the
tests execution because the DNS lookups are not being performed constantly.

And the fourth one - observability. We want to be able to see what is going on inside the containers,
especially the ones defined as services, since we will be sending various requests to them and we want to be sure
that they are being processed correctly.

So, we have three issues connected to the configuration of **compositions** made up from **Docker** containers.
I think You know where I'm going with this. ü§î

## Compose Yourself, before You test Yourself üêãüõ†Ô∏è

So yeah, let's pack up our happy family of containers into a single YAML, that can be nicely stored in our repo
and reviewed by our fellow developers. We can do that by using the `docker-compose` tool, which is a part of the
Docker ecosystem. It allows us to define a set of containers that will be spawned together and may share a network.

I will try to include all of the aforementioned aspects into this new setup, but bear in mind that for this
simple case of a single container, it may be a bit of an overkill üôà. But it will easily scale to a more complex
and appropriate problem shortly. Without further ado, let's take a look at the `docker-compose.yml` file:

<CodeSnippet
  code={`
    version: '3'

    services:
      redis:
        container_name: redis
        image: redis:6.2.6-alpine
        ports:
          - 6379:6379
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        healthcheck:
          test: ["CMD", "redis-cli", "ping", '-h', 'localhost', '-p', '6379']
          interval: 5s
          timeout: 5s
          retries: 5

      pytest-runner:
        container_name: pytest-runner
        image: python:3.10
        depends_on:
          - redis
        networks:
          test-network:
            ipv4_address: 10.5.0.3
        volumes:
          - \$\{CI_PROJECT_DIR\}:/app
        working_dir: /app
        environment:
          - REDIS_HOST
          - REDIS_PORT
          - REDIS_DB
        command:
          - sh
          - -c
          - |
            pip \\
              install \\
                -r requirements.txt \\
                pytest
            pytest

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml"
  annotations={`
    9-11:Redis service networking configuration
    12-16:Redis healthcheck
    23-25:Pytest runner networking configuration
    24:Mounting the code
    28:Setting the working directory to cloned repo
    29-32:Environment variables pulled from parent CI container
    34-41:Multiline command syntax I tend to use
    48-50:Setting manually the subnet and gateway for the test network
  `}
/>

Since it is a lot to take in at once due to some caveats present, I will go through the most
juicy parts ü•ùüçãüçé of this file.

### Decomposing the composition ‚úÇÔ∏è

The general goal of this file is to create a network called `test-network` and spawn two containers
inside it - one with a Redis database and one with our `pytest` runner. So, I will divide this section
into two aspects: container general setup and container interconnectivity (i.e. networking).

#### Container general setup üë©‚Äçüè≠üì¶

The first thing that we can see is that we have two services defined under the `services` keyword:

* the Redis database, which is defined as a service with the `redis` container name,
* the `pytest` runner, which is defined as a service with the `pytest-runner` container name.

**NOTE**: I am explicitly setting the container names (via `container_name` field), because I want to be able to 
refer to them by invariant, hardcoded aliases, instead of the [automatically generated ones], that may contain names 
of directories where the manifests are located (if You decide to copy them and place in some folder). Better safe than sorry.

For the first one, since we're focused here on setting up **JUST** the Redis database,
without any additional services in its container, the `alpine` version of Redis Docker image is used.

For the `pytest` runner, we use the `python:3.10` image, which is the same as the previously used tag
in our last version of `.gitlab-ci.yml` file. The logical thing here is to sping up that container
**only** if the Redis service has not only started, but is also ready to accept connections i.e. healthy.
This can be achieved by using the [`depends_on` Docker Compose keyword], which will make the `pytest` runner container
wait for the Redis container to be ready.

To ensure that the `redis` container is ready, we will
manually define a [healthcheck] for it, which will be performed by the Docker daemon. This healthcheck
will be performed every 5 seconds, 5 times, with a timeout of 5 seconds. The gist of it is the
`redis-cli ping` command, that You have seen at the beginning of this article.

<CodeSnippet
  code={`
    healthcheck:
      test: ["CMD", "redis-cli", "ping", '-h', 'localhost', '-p', '6379']
      interval: 5s
      timeout: 5s
      retries: 5
  `}
  language="yaml"
  filename="docker-compose.yml (healthcheck)"
  annotations={`
    2:We define the command that will be executed by the Docker daemon
    3:We define the interval between the healthchecks
    4:We define the timeout for the healthcheck
    5:We define the number of retries for the healthcheck
  `}
/>

We also need to configure the `pytest` to contain necessary environment variables, that will be used
by the `connector.py` module to connect to the Redis database. This can be done by using the `environment`
keyword, which will set the environment variables in the container. Here, we can just list the names
of the variables, without their values, because they will be pulled from the parent CI container
(the one that is actually spawned by the GitLab runner). However, nothing stops us from setting
the values here, if we want to override the values from the parent container, using the
`[VARIABLE_NAME]: [VARIABLE_VALUE]` syntax.

And of course, since we are testing our Python module,
we need to mount the repository with our code to the container via `volumes` keyword and set the
working directory to the mounted repo via `working_dir` keyword. We can utilize the `CI_PROJECT_DIR`
variable, which is set by the GitLab runner and points to the root directory of the cloned repo.
The last step is optional, but I prefer to do it because it makes it more natural to issue commands in the container
as if we were in the repo root directory on our local machine.

When it comes to the `pytest-runner` commands, we can see that we are using the multiline syntax
to install the dependencies and run the tests. This is because we want to avoid the use of
the `&&` operator, which is used to chain target commands in a single line or some other line-breaking
maneuvers (due to the underlying `sh` command), which are not really readable. So as You can see,
I just pack the actual directives into a single string and use the `|` operator to tell the shell
to treat it as a multiline command:

<CodeSnippet
  code={`
    command:
      - sh
      - -c
      - |
        pip \\
          install \\
            -r requirements.txt \\
            pytest
        pytest
  `}
  language="yaml"
  filename="docker-compose.yml (multiline command)"
  annotations={`
    4:The | operator tells the shell to treat the following lines as a multiline command
  `}
/>

And that's it for the general setup of the containers. Now let's take a look at the networking.

#### Container interconnectivity üì¶üîóüì¶

First, we will start from the bottom of our `docker-compose.yml` file, where we define the `test-network`.
Here, just to be üíØ% sure, I explicitly set its name to `test-network`. It's not really necessary,
since it will be automatically named after the directory in which the `docker-compose.yml` file is located,
if it suits Your case.

<CodeSnippet
  code={`
      networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml (network)"
  annotations={`
    3:We define the name of the network
    4:We do the manual IP address management (IPAM)
    7:Creating the subnet (group of IP addresses) for the network
    8:Setting the gateway for the network
  `}
/>

Then, we move onto the IP address management (IPAM) part. Here, we define the subnet for the network
where out containers will be located, by explicitly assigning them adresses from the range
from `10.5.0.0` to `10.5.255.254` (where 65534 possible hosts is ofc an overshot, but it's just an example).
To get this kind of IP address range, we must define the subnet as `10.5.0.0/16`, where the `/16` part
is the CIDR notation for the subnet mask. The `gateway` keyword is used to define the IP address
of the gateway for the network, which is the address of the host machine on which the containers
will be spawned i.e. the parent CI container. The rule of thumb here is that **the gateway address
must be the first address in the subnet range**, so in our case, it would be `10.5.0.1`.

As for the network configuration of the services, we must simply specify for each service the
network to which it belongs and the IP address that it will be assigned to. This can be done
by using the `networks` keyword, which will contain the name of the network and the `ipv4_address`
field with the desired IP address:

<CodeSnippet
  code={`
    services:
      redis:
        ...
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        ...

      pytest-runner:
        ...
        networks:
          test-network:
            ipv4_address: 10.5.0.3
        ...
  `}
  language="yaml"
  filename="docker-compose.yml (services networking snippets)"
  annotations={`
    4-6:Redis service networking configuration
    11-13:Pytest runner networking configuration
  `}
/>

So now in our configuration of unit tests, we can be sure that whenever we make any calls like
API calls or database queries, we will be able to use the IP addresses of the containers instead
of their DNS-resolved names.

Finally, after running our modified mock stack setup, we should see that the Docker daemon
successfully spins up a new network with two containers inside it, performs the tests defined
in the `test_connector.py` module and then destroys the network along with the containers.
Let's see if that is the case üôà.

<CaptionedImage
  src="/assets/images/compose_first_stack.png"
  alt="The composed pinger has received its service-oriented pong üêãüèì"
  height="400"
/>

We see not only that the mock environment has been successfully created and executed its workload,
**but** we have now access to nicely separated logs of the services that are spawned by the `docker-compose` tool,
due to their lines being prepended with the container's name. These logs can be even scrapped by some other
tool (if we're talking about a setup oriented towards CD) for further analysis. The possibilities are there‚ùï

But, we have reached our goal of testing integration of our connector with the Redis database,
but the thing is that we have used the `redis` library that, by the design, already accomplishes
that. So now it is time to move to some raw and dirty HTTP requests and barebones JSON responses.
We're talking about REST API here. 4Ô∏è‚É£0Ô∏è‚É£4Ô∏è‚É£‚ò†Ô∏è

## Covert DevOps üïµÔ∏èüì¶

We will go wild on this one, but it actually references one of the cases I've encountered in my work.
The main goal here is to provide a middle-man application, **an agent** üïµÔ∏è, that requests metrics
from one database and then pushes it into some other remote endpoint. This is kind of similar to
[how Prometheus works in remote-write mode] - it scrapes the metrics from the targets
and passes them forward to some other place. We will implement a very crude version of that
as a miniature Python module (a script, really) that will be imported into some other
source code.

For demonstration purposes, the agent library will be a simple Python module that will be
sending HTTP requests using `requests` library. To produce some metrics, we will use
the awesome [`fake-metrics-generator`] project by Grafana team, which already has a Docker image available
to quickly spin up containers spitting out random metrics. This service will be scraped by Prometheus
to which we will send PromQL queries via the agent for the metrics that we want to retrieve.
The target endpoint for this data will be a simple [MongoDB] instance with a database named 
`metrics`.

This setup may look out of touch with reality, but imagine substituting the `fake-metrics-generator`
with real application that produces metrics or Grafana instance collecting cluster-wide data or 
MongoDB with some other database or application that analyzes the metrics.
Or just "don't-imagine" projects like [prometheus-kafka-adapter],
these things need implementation all over the place due to the different data flows present in the organizations.

This also gives us a chance to do some HTTP calls to the `fake-metrics-generator` container
and communicate with MongoDB via `pymongo` library. So, two distinct ways of communication -
via REST API and a programmatic interface, which is cool for our tutorial here.

Test will be performed by the `pytest` runner, which will use the module to transfer the aforementioned
mock data from Prometheus to MongoDB.

### Pre-mission briefing üìã

We have learned how to glue together containers into a single composition with Docker Compose,
so it is now a matter of defining three aspects of our test stack:

1. Dependency between containers - which container needs another service running to start performing its job
2. Healthchecks - how to define that a container is ready to accept requests or, in general, is healthy
3. Static and dynamic configuration - what can be set in stone via environment variables and what needs templating

These things will help us write a correct Compose YAML and sometimes further organize file structure of our project.

#### No. 1 and 2: Together we start, divided we fail üè≥Ô∏è

Due to the intertwinement of the aforementioned aspects, I will cover them together in this section.

This part should be rather simple, but requires some knowledge about the services that we want to use.
At the beginning, let's focus on the input data i.e. mock metrics to be pushed further to the database.
Our main source of information mined via the agent app will be a Prometheus instance. This service
will need to constantly scrape metrics from our `fake-metrics-generator` service, so it will be
dependent on it. 

This means that before we start the Prometheus service, we need to make sure that the `fake-metrics-generator`
service is up and running. This can be achieved by using the `depends_on` keyword, which will make
the Prometheus service wait for the `fake-metrics-generator` service to be ready.

<CodeSnippet
  code={`
    version: '3'

    services:
      mock-metrics-generator:
        container_name: mock-metrics-generator
        image: node:16-alpine
        command: ['node', '/app/main.node.js']
        ports:
          - 5000:5000
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        
      prometheus:
        container_name: prometheus
        image: prom/prometheus:latest
        depends_on:
          mock-metrics-generator:
            condition: service_healthy
        ports:
          - 9090:9090
        networks:
          test-network:
            ipv4_address: 10.5.0.3

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml (depends_on)"
  annotations={`
    6-7:Generator is a NodeJS app that is built here from source during CI
    9:This port config comes from the project GitHub README
    17-19:We say that this service should wait until the generator is healthy
    21:Default Prometheus port mapping
  `}
/>

Sweet, now the Prometheus will wait until the `fake-metrics-generator` service is healthy,
so all should be good. Well... ü§∑‚Äç‚ôÇÔ∏è. What does it mean that our generator is healthy?
Normally, popular services like Redis or MongoDB have some kind of healthcheck mechanism
built-in, so we can just use that. But what about our `fake-metrics-generator` service?
To be honest, its just a random app that publishes some API endpoint and spits out data
like there is no tomorrow. In this case, **WE** have to define what it means for this service
to be healthy. What we know about this application is that, after booting up, it will
expose the '/metrics' endpoint, so we can continuously send `GET` HTTP requests via `curl` to it
and check if it responds with a `200` status code. To do that, we can use the `--fail` flag,
which will make `curl` return a non-zero exit code if the response status code is not `200`.
This procedure can be defined for `mock-metrics-generator` by using the `healthcheck` keyword:

<CodeSnippet
  code={`
    version: '3'

    services:
      mock-metrics-generator:
        container_name: mock-metrics-generator
        image: node:16-alpine
        command: ['node', '/app/main.node.js']
        ports:
          - 5000:5000
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        healthcheck:
          test: curl --fail http://10.5.0.2:5000/metrics
          interval: 5s
          timeout: 3s
          retries: 5
        
      prometheus:
        container_name: prometheus
        image: prom/prometheus:latest
        depends_on:
          mock-metrics-generator:
            condition: service_healthy
        ports:
          - 9090:9090
        networks:
          test-network:
            ipv4_address: 10.5.0.3

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml (depends_on remastered)"
  annotations={`
    14-17:Just hit that endpoint
  `}
/>

Another healthcheck that we need to define is the one for the Prometheus service. This one
is a bit more tricky, because we need to make sure that the Prometheus service is ready to accept
PromQL queries i.e. have its API open to the outside world. This can be achieved by using the
`wget` tool (for variety sake here), but this time we will use it to hit the root endpoint under `9090` port:

<CodeSnippet
  code={`
    version: '3'

    services:
      mock-metrics-generator:
        container_name: mock-metrics-generator
        image: node:16-alpine
        command: ['node', '/app/main.node.js']
        ports:
          - 5000:5000
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        healthcheck:
          test: curl --fail http://10.5.0.2:5000/metrics
          interval: 5s
          timeout: 3s
          retries: 5
        
      prometheus:
        container_name: prometheus
        image: prom/prometheus:latest
        depends_on:
          mock-metrics-generator:
            condition: service_healthy
        ports:
          - 9090:9090
        networks:
          test-network:
            ipv4_address: 10.5.0.3
        healthcheck:
          test: ["CMD", "wget", "http://localhost:9090"]
          interval: 10s
          timeout: 15s
          retries: 10
          start_period: 40s

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml (Prom healthcheck)"
  annotations={`
    31-35:Just hit that endpoint, Part 2
  `}
/>

On the other side of our data flow pipeline is the MongoDB database. This service will be
responsible for storing the metrics that we will be sending to it via the agent app.
So, we need to make sure that the database is up and running before we start the agent.
Again, we will use `depends_on` keyword and (just to be 100% sure) define a very simple
healthcheck for it:

<CodeSnippet
  code={`
    version: '3'

    services:
      mock-metrics-generator:
        container_name: mock-metrics-generator
        image: node:16-alpine
        command: ['node', '/app/main.node.js']
        ports:
          - 5000:5000
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        healthcheck:
          test: curl --fail http://10.5.0.2:5000/metrics
          interval: 5s
          timeout: 3s
          retries: 5
        
      prometheus:
        container_name: prometheus
        image: prom/prometheus:latest
        depends_on:
          mock-metrics-generator:
            condition: service_healthy
        ports:
          - 9090:9090
        networks:
          test-network:
            ipv4_address: 10.5.0.3
        healthcheck:
          test: ["CMD", "wget", "http://localhost:9090"]
          interval: 10s
          timeout: 15s
          retries: 10
          start_period: 40s

      mongo:
        container_name: mongo
        image: mongo:5.0.3
        ports:
          - 21017:27017
        networks:
          test-network:
            ipv4_address: 10.5.0.4
        healthcheck:
          test: echo 'db.runCommand("ping").ok' | mongo --quiet
          interval: 5s
          timeout: 3s
          retries: 5

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml (Mongo No. 5, bby)"
  annotations={`
    35:Again, the default for MongoDB instances
    40:We can use the mongo client command to check if the database is up and running
  `}
/>

If You want to check out how You can set more complicated healthchecks for MongoDB, check out
the official documentation on the [`mongo` commands] (there are like a billion of them üòµ).

Now, the *creme de la creme* ü§å of this section - tests runner with our module installed
and ready to throw some bogus metrics around. Those tests will require our whole stack to be
up and running, so the `depends_on` keyword will be used again, but with a little more robust
set of conditions:

<CodeSnippet
  code={`
    version: '3'

    services:
      mock-metrics-generator:
        container_name: mock-metrics-generator
        image: node:16-alpine
        command: ['node', '/app/main.node.js']
        ports:
          - 5000:5000
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        healthcheck:
          test: curl --fail http://10.5.0.2:5000/metrics
          interval: 5s
          timeout: 3s
          retries: 5
        
      prometheus:
        container_name: prometheus
        image: prom/prometheus:latest
        depends_on:
          mock-metrics-generator:
            condition: service_healthy
        ports:
          - 9090:9090
        networks:
          test-network:
            ipv4_address: 10.5.0.3
        healthcheck:
          test: ["CMD", "wget", "http://localhost:9090"]
          interval: 10s
          timeout: 15s
          retries: 10
          start_period: 40s

      mongo:
        container_name: mongo
        image: mongo:5.0.3
        ports:
          - 21017:27017
        networks:
          test-network:
            ipv4_address: 10.5.0.4
        healthcheck:
          test: echo 'db.runCommand("ping").ok' | mongo --quiet
          interval: 5s
          timeout: 3s
          retries: 5

    pytest-runner:
      container_name: pytest-runner
      image: python:3.10-alpine
      depends_on:
        mongo:
          condition: service_healthy
        prometheus:
          condition: service_healthy
      networks:
        test-network:
          ipv4_address: 10.5.0.5
      volumes:
        - \${CI_PROJECT_DIR}:/app
      working_dir: /app
      command:
        - sh
        - -c
        - |
          pip \\
            install \\
              pytest
          pytest \\
            test_agent.py

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml (team assembled)"
  annotations={`
    48-52:We wait for both MongoDB and Prometheus to be healthy
    59-67:We install the dependencies and run the tests
  `}
/>

Right now, we have our stack ready to be tested, but we have to remember that we have to
configure Prometheus to scrape the mock data-generating service. Also, as set in out requirements
for the test environment, the MongoDB instance must have a database named `metrics` created, before
we will attempt our tests.

#### No. 3: Knobs and dials üîß

In terms of configuration, we have to solve the following problems in our environment setup:

1. Add a new [scraping target] to Prometheus configuration via `prometheus.yml` file
2. Create a database named `metrics` in MongoDB

These tasks will be classified by me here as the "dynamic" part of the configuration, because
they require some kind of templating to be done at the time of the CI job execution. Next ones
are connected with hardcoded values that can be set via environment variables or, in other words,
typed by us in the `docker-compose.yml` or `.gitlab-ci.yml` files:

3. Move the IP addresses and ports of the services to one place in GitLab CI pipeline as environment variables
4. Set the IP addresses and ports of the services in the `docker-compose.yml` file via environment variables

Configuring Prometheus will be a two-step process, because we have to add a new scraping target
to the `prometheus.yml` file from template using CI commands and then mount it to the container
via `volumes` keyword. The `prometheus.yml` file will be mounted to the container under the path
`/etc/prometheus/prometheus.yml`. First, the YAML part:

<CodeSnippet
  code={`
    scrape_configs:
      - job_name: 'mock_metrics'
        metrics_path: /metrics
        scrape_interval: 5s
        static_configs:
  `}
  language="yaml"
  filename="prometheus.yml (scrape_configs)"
  annotations={`
    2:This is exposed by the fake-metrics-generator
  `}
/>

Now, if You are in any way familiar with Prometheus, You know that this is not enough to make it
due to one line missing from this file. And that is exactly the line that we will be adding
via CI commands. The magic ingredient is the `- targets: ['10.5.0.3:5000']` with the IP address
and port of generator service. This can be done by simply `echo`'ing the line to the file,
being careful to use the `>>` operator, which will append the line to the file, instead of
overwriting it. Plus, we need to account for the indentation of the line:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      tags:
        - "docker"
        - "dind"
      services:
        - name: docker:dind
          alias: my-docker-service
      variables:
        DOCKER_HOST: "tcp://my-docker-service:2375"
        DOCKER_TLS_CERTDIR: ""
        FF_NETWORK_PER_BUILD: "true"
      script:
        ...
        - |
          echo \\
            "    - targets: [10.5.0.3:5000]" \\
            >> prometheus.yml
        ...
  `}
  language="yaml"
  filename=".gitlab-ci.yml (finishing prometheus.yml)"
  annotations={`
    16:We echo the line with targets definition
    18:We append the line to the file
  `}
/>

Now, we have to mount the `prometheus.yml` file to the container. This can be done by using
the `volumes` keyword, which will mount the file from the host machine to the container
under the path `/etc/prometheus/prometheus.yml`:

<CodeSnippet
  code={`
    ...
    prometheus:
      ...
      volumes:
        - \${CI_PROJECT_DIR}/prometheus.yml:/etc/prometheus/prometheus.yml
      ...
  `}
  language="yaml"
  filename="docker-compose.yml (prometheus.yml mount)"
  annotations={`
    1:I've omitted the rest of the compose for brevity
  `}
/>

Next, we have to create the `metrics` database in MongoDB. The `mongo` image used by us has an easy way
to run initialization scripts on the container boot. We just have to mount a JavaScript (or shell) script
to the container under the `/docker-entrypoint-initdb.d/` directory and it will be executed by the `mongo` client
automagically ü™Ñ. The script is rather simple because it adds a user for the `metrics` database,
ensuring its existence at the time of the agent module tests:

<CodeSnippet
  code={`
    db.createUser({
      user: "metrics",
      pwd: "metrics",
      roles: [
        {
          role: "readWrite",
          db: "metrics"
        }
      ]
    })
  `}
  language="javascript"
  filename="mongo-init.js"
  annotations={`
    2-3:Some toy credentials
    4-9:Predefined MongoDB role assignment
  `}
/>

To be even more raunchy, I will just `cat` the contents of this file to the `mongo-init.js` file from the
`.gitlab-ci.yml` file - I will use this fact in a second.

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      tags:
        - "docker"
        - "dind"
      services:
        - name: docker:dind
          alias: my-docker-service
      variables:
        DOCKER_HOST: "tcp://my-docker-service:2375"
        DOCKER_TLS_CERTDIR: ""
        FF_NETWORK_PER_BUILD: "true"
      script:
        ...
        - |
          cat <<EOT >> mongo-init.js
          db.createUser({
            user: "metrics",
            pwd: "metrics",
            roles: [
              {
                role: "readWrite",
                db: "metrics"
              }
            ]
          })
          EOT
        ...
  `}
  language="yaml"
  filename=".gitlab-ci.yml (mongo-init.js)"
  annotations={`
    16:We echo the contents of the file
    18:We append the contents to the file
  `}
/>

Then we plop (mount) that bad boi ü¶π into the `docker-entrypoint-initdb.d` directory and we're good to go.
Moreover, we will set the credentials for the MongoDB database via environment variables, since
we're already using tricks made available by this Docker image.

<CodeSnippet
  code={`
    ...
    mongo:
      ...
      environment:
        MONGO_INITDB_ROOT_USERNAME: root
        MONGO_INITDB_ROOT_PASSWORD: root
        MONGO_INITDB_DATABASE: metrics
      volumes:
        - \${CI_PROJECT_DIR}/mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js
      ...
  `}
  language="yaml"
  filename="docker-compose.yml (mongo-init.js mount)"
  annotations={``}
/>

Cool, we've got our metrics scraping ready and our database is up and running. If we check out our
full `docker-compose.yml` file, we can see that we have a lot of hardcoded values there. I've marked
the more important lines with single-line comments. We can move those values to the environment variables
defined in the `.gitlab-ci.yml` file, so that we can easily change them in one place. This is possible because
Docker Compose, as shown previously, **can use the environment variables of its host*. So, we will be able to
configure the whole CI setup from the `.gitlab-ci.yml` file, which is nice.

<CodeSnippet
  code={`
    version: '3'

    services:
      mock-metrics-generator:
        container_name: mock-metrics-generator
        image: node:16-alpine
        command: ['node', '/app/main.node.js']
        ports:
          - 5000:5000
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        healthcheck:
          test: curl --fail http://10.5.0.2:5000/metrics
          interval: 5s
          timeout: 3s
          retries: 5
        
      prometheus:
        container_name: prometheus
        image: prom/prometheus:latest
        depends_on:
          mock-metrics-generator:
            condition: service_healthy
        volumes:
          - \${CI_PROJECT_DIR}/prometheus.yml:/etc/prometheus/prometheus.yml
        ports:
          - 9090:9090
        networks:
          test-network:
            ipv4_address: 10.5.0.3
        healthcheck:
          test: ["CMD", "wget", "http://localhost:9090"]
          interval: 10s
          timeout: 15s
          retries: 10
          start_period: 40s

      mongo:
        container_name: mongo
        image: mongo:5.0.3
        environment:
          MONGO_INITDB_ROOT_USERNAME: root
          MONGO_INITDB_ROOT_PASSWORD: root
          MONGO_INITDB_DATABASE: metrics
        volumes:
          - \${CI_PROJECT_DIR}/mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js
        ports:
          - 21017:27017
        networks:
          test-network:
            ipv4_address: 10.5.0.4
        healthcheck:
          test: echo 'db.runCommand("ping").ok' | mongo --quiet
          interval: 5s
          timeout: 3s
          retries: 5

    pytest-runner:
      container_name: pytest-runner
      image: python:3.10-alpine
      depends_on:
        mongo:
          condition: service_healthy
        prometheus:
          condition: service_healthy
      networks:
        test-network:
          ipv4_address: 10.5.0.5
      volumes:
        - \${CI_PROJECT_DIR}:/app
      working_dir: /app
      command:
        - sh
        - -c
        - |
          pip \\
            install \\
              pytest
          pytest \\
            test_agent.py

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  annotations={`
    9:Generator port -> GENERATOR_PORT
    12:Generator address -> GENERATOR_IP_ADDRESS
    14:Here we can substitute for the GENERATOR_* variables
    28:Prometheus port -> PROMETHEUS_PORT
    31:Prometheus address -> PROMETHEUS_IP_ADDRESS
    33:Here we can substitute the port
    43:MongoDB root username -> MONGO_ROOT_USERNAME
    44:MongoDB root password -> MONGO_ROOT_PASSWORD
    45:MongoDB database name -> MONGO_TEST_DATABASE
    49:MongoDB port -> MONGO_PORT
    52:MongoDB address -> MONGO_IP_ADDRESS
    69:Here we can omit the IP address - we 100% won't be using it ü§∑
  `}
/>

Now, we can create the desired variables to the `.gitlab-ci.yml`, as global variables so that
they are nicely separated from the rest of the file as a sort of configuration section:

<CodeSnippet
  code={`
    variables:
      MOCK_GENERATOR_HOST: "10.5.0.3"
      MOCK_GENERATOR_PORT: "5000"
      MONGODB_HOST: "10.5.0.2"
      MONGODB_PORT: "27017"
      PROMETHEUS_HOST: "10.5.0.4"
      PROMETHEUS_PORT: "9090"
      PYTEST_HOST: "10.5.0.5"
      MONGODB_ROOT_USERNAME: "root"
      MONGODB_ROOT_PASSWORD: "r00t"
      MONGODB_TEST_DATABASE: "metrics"

    test-job:
      image: docker:latest
      ...
  `}
  language="yaml"
  filename=".gitlab-ci.yml (global variables)"
  annotations={`
    10:They will never know ü§´
    15:The rest is as it was
  `}
/>

Returning to our `docker-compose.yml` file, we can now substitute the hardcoded values with the
environment variables (and actually pass the envvars to the `pytest-runner` container):

<CodeSnippet
  code={`
    version: '3'

    services:
      mock-metrics-generator:
        container_name: mock-metrics-generator
        image: node:latest
        volumes:
          - \${CI_PROJECT_DIR}/fake-metrics-generator/dist:/app
          - \${CI_PROJECT_DIR}/mock.json:/app/config/data.json
        ports:
          - \${MOCK_GENERATOR_PORT}:5000
        command: \['node', '/app/main.node.js'\]
        networks:
          test-network:
            ipv4_address: \${MOCK_GENERATOR_HOST}
        healthcheck:
          test: curl --fail http://\${MOCK_GENERATOR_HOST}:\${MOCK_GENERATOR_PORT}/metrics
          interval: 5s
          timeout: 3s
          retries: 5
        
      prometheus:
        container_name: prometheus
        image: prom/prometheus:latest
        depends_on:
          mock-metrics-generator:
            condition: service_healthy
        volumes:
          - \${CI_PROJECT_DIR}/prometheus.yml:/etc/prometheus/prometheus.yml
        ports:
          - \${PROMETHEUS_PORT}:9090
        networks:
          test-network:
            ipv4_address: \${PROMETHEUS_HOST}
        healthcheck:
          test: \["CMD", "wget", "http://localhost:9090"\]
          interval: 10s
          timeout: 15s
          retries: 10
          start_period: 40s

      mongo:
        container_name: mongo
        image: mongo:5.0.3
        environment:
          - MONGO_INITDB_ROOT_USERNAME=\${MONGODB_ROOT_USERNAME}
          - MONGO_INITDB_ROOT_PASSWORD=\${MONGODB_ROOT_PASSWORD}
          - MONGO_INITDB_DATABASE=\${MONGODB_TEST_DATABASE}
        volumes:
          - \${CI_PROJECT_DIR}/metrics.db-init.js:/docker-entrypoint-initdb.d/mongo-init.js
        ports:
          - \${MONGODB_PORT}:27017
        networks:
          test-network:
            ipv4_address: \${MONGODB_HOST}
        healthcheck:
          test: echo 'db.runCommand("ping").ok' | mongo --quiet
          interval: 5s
          timeout: 3s
          retries: 5

      pytest-runner:
        container_name: pytest-runner
        image: python:3.10-alpine
        depends_on:
          mongo:
            condition: service_healthy
          prometheus:
            condition: service_healthy
        environment:
          - MONGODB_HOST
          - MONGODB_PORT
          - MONGODB_USERNAME
          - MONGODB_PASSWORD
          - MOCK_GENERATOR_HOST
          - MOCK_GENERATOR_PORT
        volumes:
          - \${CI_PROJECT_DIR}:/app
        working_dir: /app
        command:
          - sh
          - -c
          - |
            pip \\
              install \\
                pytest
            pytest \\
              test_agent.py

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml (FINAL)"
  annotations={`
    71-76:These variables will be useful during the tests
  `}
/>

[Crystalheart Pulse-Staff]: https://www.wowhead.com/tbc/item=28782/crystalheart-pulse-staff
[`services` keyword in the `.gitlab-ci.yml` file]: https://docs.gitlab.com/ee/ci/services/
[plethora]: https://medium.com/@oachuy/gitlab-runner-with-docker-dind-3e0e1862662f
[of]: https://docs.gitlab.com/ee/ci/docker/using_docker_build.html
[tutorials]: https://docs.gitlab.com/ee/ci/docker/using_docker_build.html
[`depends_on` Docker Compose keyword]: https://docs.docker.com/compose/compose-file/05-services/#depends_on
[automatically generated ones]: https://github.com/docker/compose/issues/6316
[healthcheck]: https://docs.docker.com/engine/reference/builder/#healthcheck
[how Prometheus works in remote-write mode]: https://prometheus.io/docs/concepts/remote_write_spec/
[`fake-metrics-generator`]: https://github.com/grafana/fake-metrics-generator
[MongoDB]: https://www.mongodb.com/
[prometheus-kafka-adapter]: https://github.com/Telefonica/prometheus-kafka-adapter
[`mongo` commands]: https://docs.mongodb.com/manual/reference/command/ping/
[scraping target]: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config
