---
title: "Containerized Integration"
date: "2023-11-30"
description: "How to easily spin up a mock environment for integration testing"
tags: ["gitlab", "ci", "tests", "docker", "containers", "integration"]
image:
  thumbnail: "components"
  hero: "components"
  alt: "Lab in a box"
---

## I'm afraid I can't let you do that, Dev ü§ñ

There is a certain kind of dread that comes with the acknowledgment that You can't always cross the cap
between Your local pythonic incantations and the desired environment, where those enchantments are to be
set to be roaming free, setting and getting those `os.environ`s. "Being part of the Machine" is not only
a figurative thing that a societal contrarian might say, but - in terms of software development - it is
also a very literal thing. You design cogs of various sizes and shapes, that are to be put in the overarching
conglomerate of multi-lingual, multi-paradigm and multi-purpose mechanisms.

What is actually contrary to popular belief, the net is not about connecting people and/or machines at the upper echelons
of the enterprise-ish hierarchy. It's more often than not a convoluted web of rules and blockades,
that separate this connectivity from one environment to another - be it different teams of developers,
different services (often at their different stages) or in general the intra/internet gatekeeping conundrum.

This is, of course, done with good intent, so that the developers don't accidentally
set the production database to be the target of their unit tests, pushing around
valuable data about somebody's World of Warcraft character on the private server
run by a group of "real chill dudes", where You've recently managed to down
that one annoying boss with Your guild, **and** got the highest roll on [Crystalheart Pulse-Staff] üòå,
to the dismay of Your fellow restoration druids üå≤.

Those are the real casualties that would have been made without that one reasonable sysadmin
at Your software house, who's set up e.g. that VPN so all GitLab CI workers can't reach the K8s cluster,
where the actual services may be running. Additionally, he will also (rightfully) hunt You down
if You try to copy any credentials to those temporary nodes, to "cleverly" bypass those security measures.

I would like to stress here that I am not talking about any continuous deployment (CD) pipelines,
where the code is automatically deployed to the production environment and it mutates its state, in real time.
CD naturally assumes successful CI (continuous integration) and by integration, it means that the code
is already up to the security and coding standards of Your organization. In other words, it is **safe**.
The CI part is what I am really focusing on in this article, so - I am not GitOps hater or something like that üòÖ.

The thing is, some tasks require a bit more than just simple code linting tests and they must
touch upon another important aspect of integration with the existing environment - communication,
more often than not, in the form of authorized API requests
or, in general, calls made through specified protocols.

So we arrive at the "Schr√∂dinger's code problem" i.e. the code that must talk with our services, but really
can't talk with our services at the same time. But what is to be tested here? Language i.e. form.
In other words, we only care if our code can communicate **at all** with the service,
not really if it retrieves a specific portion of the data. The rest can be mocked.
So basically, if You can't play with the real tools - You can at least play with the toys,
and that is what mock test environments are for. An approximation.

In this article, I will try to show You how I spin up those sandboxes for my integration tests,
using GitLab CI Services and Docker, with additional tips for unit testing Python code via containerized
`pytest` nested runners.

**IMPORTANT NOTE**: This article assumes that there is a GitLab CI runner available in Your organization,
that is capable of Docker-in-Docker (DinD) builds. I've included some links to tutorials on how to set up
such a runner, but I will not go into details on how to do that, as it is not the main focus of this article.
That is why I am including the 99% of code snippets here in this article and no repo to go with it,
because cloning it and running will simply not do. Maybe in the future, I will make an article about
setting such workers on Your home/private cluster, but that's a topic for another time üòõ (SOON‚Ñ¢).

## Alive in a box - dead to the outside world üêà‚Äç‚¨õüì¶

To figure out how to create our pseudo-environment, we must first understand what goes on under the hood
when our job is being executed on the GitLab CI runner. In very short terms, the runner
pulls the Docker image specified in the `.gitlab-ci.yml` for a given job and spins up a container,
which has access to the project's files and the GitLab CI variables. The container then executes
the commands specified in the `script` section of the job. The container is then destroyed,
along with all the changes made to the filesystem, unless we explicitly tell the runner to persist
some files between the jobs as either artifacts or cache.

Take a look at the following example:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest 
      script:
        - echo "I am running!"
        - |
          docker \\
            run \\
              alpine:latest \\
              sh \\
                -c \\
                  "echo 'I am running inside the container!'"
  `}
  language="yaml"
  filename=".gitlab-ci.yml"
  annotations={`
    2:We use the docker image to be able to run docker commands inside the container
    4:This command is executed at the uppermost container level
    5:Here we want to communicate to Docker daemon on the host that we want to nest a new container
    12:This command is executed inside the nested container
  `}
/>

A quick side note - if You see me using the `\` operator in the code snippets, it is just a way to
break the line in the `.gitlab-ci.yml` file (or other manifests/shell scripts), so that it is more readable. 
I like to separate the flags and arguments of the commands into separate lines, with indentations added to, for example,
flag values to see which keyword belongs to which flag. It is not necessary, but I find it
more readable than a single line with a bunch of flags and arguments, separated by spaces.

So, coming back to CI, we expect to be greeted by two messages, one from the host and one from the nested container.
However, when we run this job, we will only see the first message executed at the main container level,
but the second one will not be executed at all:

<CaptionedImage
  src="/assets/images/nested_container_no_socket.png"
  alt="Dude, where is my daemon? ü§î"
  height="500"
/>

The main reason is the way in which the parent container attempts to communicate with the Docker daemon
that is located on the host. The original Docker daemon is listening on a Unix socket, which is located at
`/var/run/docker.sock` on the host machine. 

The parent container, however, does not have access to this socket,
because it is not mounted to the container's filesystem. This is what line number 21 in the log printout above
is really about. The nested container tries to look for a way to communicate with its parent's daemon,
so it checks if the Docker socket is mounted into its filesystem. 

If not (which is the case here), it then tries to communicate with an external Docker **service**, 
specified under the `DOCKER_HOST` environment variable. **By default**, this variable (for GitLab runners)
is set to `tcp://docker:2375`, which translates to URL of: `http://docker:2375`.
And this is the reason why the nested container prints out the error message about the failed `TCP` lookup.

We can confirm this behavior by listing the sockets mounted to the nested container under `/var/run` path
and setting the `DOCKER_HOST` envvar to some bogus value like `tcp://docker-service:2375`:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      variables:
        DOCKER_HOST: "tcp://docker-service:2375"
      script: |
        - echo "I am running!"
        - |
          ls \\
            -la \\
            /var/run/
        - |
          docker \\
            run \\
              alpine:latest \\
              sh \\
                -c \\
                  "echo 'I am running inside the container!'"
  `}
  language="yaml"
  filename=".gitlab-ci.yml (checking behaviour)"
  annotations={`
    4:We point to some other Docker service
    7-9:Check for mounted Docker socket in the nested container
  `}
/>

Here we basically expect the overall effect to be the same as previously, but this time we will see
that there is basically nothing mounted under `/var/run` path and the `TCP` lookup will be performed for
the `docker-service` host, which will fail, as expected.

<CaptionedImage
  src="/assets/images/nested_container_behavior.png"
  alt="Bingo! üëç"
  height="500"
/>

And this is what we get, the `/var/run` directory is empty (red box in Pic. 2)
and the `TCP` lookup fails (yellow underline in Pic. 2). So now we have to decide what is really open to us
as a solution to open up the communication between the nested container and the host's Docker daemon.
The first thing that comes to mind is to simply delegate the creation of any new Docker containers 
to an autonomous **Docker service**, that will be set up by the runner and reachable from the nested container.
This can be achieved by using the [`services` keyword in the `.gitlab-ci.yml` file]. 
Those services are basically Docker containers spawned beside the CI container itself, which 
are accessible from the tests container due to the network being shared
between all of the sub-hosts present on the runner.

**BUT** there is a catch. Using Docker-in-Docker (DinD) approach requires an additional configuration of the runner itself,
that can be then used by the CI jobs as a shared runner by specifying the `tags` keyword in the `.gitlab-ci.yml` file.
In most cases, kind of runners are already present as a CI/CD tool for software development teams, 
because, for example, building Docker images is a very common task among them, so we can already capitalize on that.
However, if there is no such runner available, we can always create one ourselves. There is a [plethora] [of] [tutorials] 
on how to do that, so I will **assume that this kind of resource is already present in our organization**.

### Whaleception üê≥üê≥

So the game plan now is like so:

1. We will tell GitLab CI to use the runner with the predefined tags set up on it by the administrator.
2. We create a Docker service that will be used by the nested container to communicate with the host's Docker daemon.
We will name it `my-docker-service` and we will use the `docker:dind` image for that purpose.
3. We will set the `DOCKER_HOST` environment variable to `tcp://my-docker-service:2375` in the CI container.
4. We will run the command in the nested container and hope for the best. ü§û

One caveat is that if we want to use the 2375 port, we need to disable TLS verification, which can be done
by setting the `DOCKER_TLS_CERTDIR` environment variable to an empty string. This is because the runner
is not configured to use TLS by default. We can also set the `FF_NETWORK_PER_BUILD` environment variable
to `true`, to enable the network sharing between the containers, which will be useful in a second when we will
configure our mock test environment in a sec. Any `docker` command within the CI container will also need to use
an additional `--network=host` flag to use the CI container network that is shared with the DinD service.

All of these steps can be performed by modifying the `.gitlab-ci.yml` file like so:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      tags:
        - "docker"
        - "dind"
      services:
        - name: docker:dind
          alias: my-docker-service
      variables:
        DOCKER_HOST: "tcp://my-docker-service:2375"
        DOCKER_TLS_CERTDIR: ""
        FF_NETWORK_PER_BUILD: "true"
      script: |
        - echo "I am running!"
        - |
          docker \\
            run \\
              --network=host \\
              alpine:latest \\
              sh \\
                -c \\
                  "echo 'I am running inside the container!'"
  `}
  language="yaml"
  filename=".gitlab-ci.yml (working)"
  annotations={`
    2:We use the docker image to be able to run docker commands inside the CI container
    3-5:We tell GitLab CI to use the runner with the predefined tags set up on it by the administrator
    6-8:We create a Docker service that will be used by the nested container to communicate with the host's Docker daemon
    10:We set the DOCKER_HOST environment variable to 'tcp://my-docker-service:2375' in the CI container
    11:We set the DOCKER_TLS_CERTDIR environment variable to an empty string, to disable TLS verification
    12:We set the FF_NETWORK_PER_BUILD environment variable to true, to enable the network sharing between the containers
    13:BTW, I've removed the ls command since it's not really needed here
    17:We use the --network=host flag to access the DinD container via the host's bridge network
  `}
/>

<CaptionedImage
  src="/assets/images/nested_container_dind.png"
  alt="Whale communication channel established üê≥"
  height="700"
/>

So what we have achieved here is that we have successfully created a workflow for creating environments
made up of multiple containers that can share a network and communicate with each other via the host's bridge network.
What is also useful is that services that are spawned via the `services` keyword are automatically resolvable
by their names, so we don't have to worry about the IP addresses of those containers. We can show it with
a quick `ping-pong` test for a Redis database hosted both as a service and as a nested container.
That nested container will then try to send a `PING` signal via `redis-cli` to the Redis service
named `some-random-cache`. This can be done with the following `.gitlab-ci.yml` file:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      tags:
        - "docker"
        - "dind"
      services:
        - name: docker:dind
          alias: my-docker-service
        - name: redis:latest
          alias: some-random-cache
      variables:
        DOCKER_HOST: "tcp://my-docker-service:2375"
        DOCKER_TLS_CERTDIR: ""
        FF_NETWORK_PER_BUILD: "true"
      script: |
        - echo "I am running!"
        - |
          docker \\
            run \\
              --network=host \\
              redis:latest \\
              sh \\
                -c \\
                  "redis-cli -h some-random-cache -p 6379 PING"
  `}
  language="yaml"
  filename=".gitlab-ci.yml (Redis test)"
  annotations={`
    9:We create the target Redis service
    23:We use the -h flag to specify the hostname and -p flag to specify the host and port of the Redis service
  `}
/>

<CaptionedImage
  src="/assets/images/nested_container_ping_pong.png"
  alt="The ping has been ponged üèì"
  height="700"
/>

### Snake-in-a-box üêçüì¶ (but not that hypercube one)

Cool! So now we can spawn whatever we want inside our CI runner and make it communicate with each other,
but what about the actual integration tests? Well, we can use the same approach, but just changing the
type of image used by the nested container to the one containing our tests. Let's say we have a Python project
with some integration tests that require a Redis database to be present. The files can be structured like so:

<FileTree
  root="project/"
  tree={[
    ".env",
    "connector.py",
    "test_connector.py",
    "requirements.txt"
  ]}
  annotations={`
    .env:A way to introduce env variables to the nested container (for now).
    connector.py:Code with primitive adapter to Redis database.
    test_connector.py:Integration tests for the connector.py module.
    requirements.txt:Python dependencies for the connector.py module.
  `}
/>

The `connector.py` module will use the `redis` library to connect to the database 
and the `test_connector.py` module will contain tests for our new class. 
The `requirements.txt` file will contain the `redis` library as a dependency
to ensure that the runner container will have all of the required modules installed. So what does
the `connector.py` module has to offer? A janky üçë wrapper around the basic `redis.StricRedis` methods
that will allow us to connect to the database and set/get some values from it:

<CodeSnippet
  code={`
    import dataclasses
    import logging
    import typing

    import redis

    @dataclasses.dataclass
    class RedisConnector:
        host: str = "localhost"
        port: int = 6379
        db: int = 0
        _session: redis.StrictRedis = dataclasses.field(init=False, default_factory=redis.StrictRedis)

        def __post_init__(
            self,
        ) -> redis.StrictRedis:
            try:
                self._session = redis.StrictRedis(
                    host=self.host,
                    port=self.port,
                    db=self.db
                )
                logging.info("Redis connection established")
                return self._session
            except Exception as connection_failed:
                logging.error("Redis connection failed")
                logging.error(connection_failed)
                raise connection_failed

        def __del__(self) -> None:
            self.disconnect()

        def disconnect(self) -> None:
            if self._session:
                self._session.close()
                logging.info("Redis connection closed")

        def __getitem__(self, key) -> typing.Any:
            return self._session.get(key)

        def __setitem__(self, key: str, value: typing.Any) -> None:
            self._session.set(key, value)
  `}
  language="python"
  filename="connector.py"
  annotations={`
    7:We will use dataclasses to simplify the initialization of the class
    12:This field will store the already established connection to the Redis database
    14-28:Here we try to connect to the Redis database and log the result
    30:Here we define the destructor for the class, which will close the connection
    33:Here we define the method for closing the connection
    38-42:Here we define the setter and getter for the Redis database
  `}
/>

This may not be the apex of the Pythonic design, but it will do for our purposes. The `test_connector.py` module
is just a basic implementation of the `ping-pong` test, that utilizes the `.ping()` method of the `redis.StrictRedis`
class:

<CodeSnippet
  code={`
    import os

    import connector


    def test_redis_connection():
        target_host = os.environ.get("REDIS_HOST")
        target_port = os.environ.get("REDIS_PORT")
        target_db = os.environ.get("REDIS_DB")
        redis_instance = connector.RedisConnector(
            host=target_host,
            port=target_port,
            db=target_db
        )
        assert redis_instance._session.ping()
  `}
  language="python"
  filename="test_connector.py"
  annotations={`
    7-9:We retrieve the environment variables from the nested container
    15:We reach to the underlying redis.StricRedis class and use its .ping() method
  `}
/>

Here we can see the purpose of `.env` file - itwill contain the two environment variables 
that are to be used by the connector class to, well, ... connect to the Redis database: 
`REDIS_HOST`, `REDIS_PORT` and `REDIS_DB`. So we want to do the following things:

1. Set up a temporary Redis database.
2. Mount the repository with out code to a `pytest` runner container.
3. Install the required dependencies to carry out the unit tests.
4. Run `pytest` and (as previously) - hope for the best. ü§û

The main goal of these tests will be to check if our interface is compatible with the one
offered by the `redis` library. So we will not be testing the actual data retrieval, but rather
the ability to retrieve the data at all. A simple case, that we will, of course, complicate in a bit.
So, the `.gitlab-ci.yml`:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      tags:
        - "docker"
        - "dind"
      services:
        - name: docker:dind
          alias: my-docker-service
        - name: redis:latest
          alias: some-random-cache
      variables:
        DOCKER_HOST: "tcp://my-docker-service:2375"
        DOCKER_TLS_CERTDIR: ""
        FF_NETWORK_PER_BUILD: "true"
      script: |
        - echo "I am running!"
        - |
          docker \\
            run \\
              --network=host \\
              --volume $PWD:/app \\
              --workdir /app \\
              --env-file .env \\
              python:3.10 \\
              sh -c "pip install -r requirements.txt pytest && pytest"
  `}
  language="yaml"
  filename=".gitlab-ci.yml (testing connector)"
  annotations={`
    20:We mount the WHOLE repo to some path in the container
    21:We set the working directory to the mounted repo
    22:We set the environment variables from the mounted .env file
    24:We install the required dependencies and run the tests
  `}
/>

The value for the `REDIS_HOST` environment variable will be `some-random-cache` and the value for the `REDIS_PORT`
environment variable will be `6379`. The `REDIS_DB` environment variable will be set to `0` by default.
As it can be seen in the log snippet below, the tests have passed (I've omitted the initial setup logs,
since it they are almost identical to previous ones - only this time the `python:3.10` image is pulled ü§∑):

<CaptionedImage
  src="/assets/images/nested_container_pytest_pong.png"
  alt="The ping has been ponged again, programmatically üêçüèì"
  height="700"
/>

### Matryoshka of problems ü™Ü

So we have successfully created a mock test environment for our integration tests.
It is self-contained and somewhat "invisible" (or dead üíÄ) to the outside architecture of the organization,
but we can see it is alive and kicking from the inside üò∏. However, some things about it may (and in practice - often will) 
create some practical issues. üòø

The first one is the fact that we have stored our environment variables in a file that is mounted to the container. 
This is not a good practice, because we are basically storing our secrets in plain text and totally negating the existence of any
sensitive data protection measures that we may have in place e.g. Vault or some other secrets management tool.
Also, GitLab can use environment variables defined in CI/CD settings of the repo, so we can use that
to our advantage.

The second one is that if we want to spawn more than one container via `docker` command, we will have to
create long `script` sections in our `.gitlab-ci.yml` file, which will be hard to maintain and read.
Moreover, if we would like to pass the aforementioned environment variables to the nested containers,
we would have to do it manually for each of them, using the `--env` flag. This is not a good practice either,
because we are basically repeating ourselves and we are not really using the full potential of the containerization.
Also - container dependencies. If we want to spawn a container that depends on another container, we would have to
create a script that will wait for the first container to be ready and then spawn the second one, manually.
And how do we define that the first container is ready? Well, we would have to create a healthcheck script.
And mount it to the container, under a predefined path for Docker daemon to use.

The third issue that may pop up from time to time - network management. Sometimes, we want to be able to
use raw IP addresses of the containers, instead of their DNS-resolved names. In some cases, it may even speed up the
tests execution because the DNS lookups are not being performed constantly.

And the fourth one - observability. We want to be able to see what is going on inside the containers,
especially the ones defined as services, since we will be sending various requests to them and we want to be sure
that they are being processed correctly.

So, we have three issues connected to the configuration of **compositions** made up from **Docker** containers.
I think You know where I'm going with this. ü§î

## Compose Yourself üìø

So yeah, let's pack up our happy family of containers into a single YAML, that can be nicely stored in our repo
and reviewed by our fellow developers. We can do that by using the `docker-compose` tool, which is a part of the
Docker ecosystem. It allows us to define a set of containers that will be spawned together and may share a network.

I will try to include all of the aforementioned aspects into this new setup, but bear in mind that for this
simple case of a single container, it may be a bit of an overkill üôà. But it will easily scale to a more complex
and appropriate problem shortly. Without further ado, let's take a look at the `docker-compose.yml` file:

<CodeSnippet
  code={`
    version: '3'

    services:
      redis:
        container_name: redis
        image: redis:6.2.6-alpine
        ports:
          - 6379:6379
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        healthcheck:
          test: ["CMD", "redis-cli", "ping", '-h', 'localhost', '-p', '6379']
          interval: 5s
          timeout: 5s
          retries: 5

      pytest-runner:
        container_name: pytest-runner
        image: python:3.10
        depends_on:
          - redis
        networks:
          test-network:
            ipv4_address: 10.5.0.3
        volumes:
          - \$\{CI_PROJECT_DIR\}:/app
        working_dir: /app
        environment:
          - REDIS_HOST
          - REDIS_PORT
          - REDIS_DB
        command:
          - sh
          - -c
          - |
            pip \\
              install \\
                -r requirements.txt \\
                pytest
            pytest

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml"
  annotations={`
    9-11:Redis service networking configuration
    12-16:Redis healthcheck
    23-25:Pytest runner networking configuration
    24:Mounting the code
    28:Setting the working directory to cloned repo
    29-32:Environment variables pulled from parent CI container
    34-41:Multiline command syntax I tend to use
    48-50:Setting manually the subnet and gateway for the test network
  `}
/>

Since it is a lot to take in at once due to some caveats present, I will go through the most
juicy parts ü•ùüçãüçé of this file.

### Decomposing the composition ‚úÇÔ∏è

The general goal of this file is to create a network called `test-network` and spawn two containers
inside it - one with a Redis database and one with our `pytest` runner. So, I will divide this section
into two aspects: container general setup and container interconnectivity (i.e. networking).

#### Container general setup üë©‚Äçüè≠üì¶

The first thing that we can see is that we have two services defined under the `services` keyword:

* the Redis database, which is defined as a service with the `redis` container name,
* the `pytest` runner, which is defined as a service with the `pytest-runner` container name.

**NOTE**: I am explicitly setting the container names (via `container_name` field), because I want to be able to 
refer to them by invariant, hardcoded aliases, instead of the [automatically generated ones], that may contain names 
of directories where the manifests are located (if You decide to copy them and place in some folder). Better safe than sorry.

For the first one, since we're focused here on setting up **JUST** the Redis database,
without any additional services in its container, the `alpine` version of Redis Docker image is used.

For the `pytest` runner, we use the `python:3.10` image, which is the same as the previously used tag
in our last version of `.gitlab-ci.yml` file. The logical thing here is to sping up that container
**only** if the Redis service has not only started, but is also ready to accept connections i.e. healthy.
This can be achieved by using the [`depends_on` Docker Compose keyword], which will make the `pytest` runner container
wait for the Redis container to be ready.

To ensure that the `redis` container is ready, we will
manually define a [healthcheck] for it, which will be performed by the Docker daemon. This healthcheck
will be performed every 5 seconds, 5 times, with a timeout of 5 seconds. The gist of it is the
`redis-cli ping` command, that You have seen at the beginning of this article.

<CodeSnippet
  code={`
    healthcheck:
      test: ["CMD", "redis-cli", "ping", '-h', 'localhost', '-p', '6379']
      interval: 5s
      timeout: 5s
      retries: 5
  `}
  language="yaml"
  filename="docker-compose.yml (healthcheck)"
  annotations={`
    2:We define the command that will be executed by the Docker daemon
    3:We define the interval between the healthchecks
    4:We define the timeout for the healthcheck
    5:We define the number of retries for the healthcheck
  `}
/>

We also need to configure the `pytest` to contain necessary environment variables, that will be used
by the `connector.py` module to connect to the Redis database. This can be done by using the `environment`
keyword, which will set the environment variables in the container. Here, we can just list the names
of the variables, without their values, because they will be pulled from the parent CI container
(the one that is actually spawned by the GitLab runner). However, nothing stops us from setting
the values here, if we want to override the values from the parent container, using the
`[VARIABLE_NAME]: [VARIABLE_VALUE]` syntax.

And of course, since we are testing our Python module,
we need to mount the repository with our code to the container via `volumes` keyword and set the
working directory to the mounted repo via `working_dir` keyword. We can utilize the `CI_PROJECT_DIR`
variable, which is set by the GitLab runner and points to the root directory of the cloned repo.
The last step is optional, but I prefer to do it because it makes it more natural to issue commands in the container
as if we were in the repo root directory on our local machine.

When it comes to the `pytest-runner` commands, we can see that we are using the multiline syntax
to install the dependencies and run the tests. This is because we want to avoid the use of
the `&&` operator, which is used to chain target commands in a single line or some other line-breaking
maneuvers (due to the underlying `sh` command), which are not really readable. So as You can see,
I just pack the actual directives into a single string and use the `|` operator to tell the shell
to treat it as a multiline command:

<CodeSnippet
  code={`
    command:
      - sh
      - -c
      - |
        pip \\
          install \\
            -r requirements.txt \\
            pytest
        pytest
  `}
  language="yaml"
  filename="docker-compose.yml (multiline command)"
  annotations={`
    4:The | operator tells the shell to treat the following lines as a multiline command
  `}
/>

And that's it for the general setup of the containers. Now let's take a look at the networking.

#### Container interconnectivity üì¶üîóüì¶

First, we will start from the bottom of our `docker-compose.yml` file, where we define the `test-network`.
Here, just to be üíØ% sure, I explicitly set its name to `test-network`. It's not really necessary,
since it will be automatically named after the directory in which the `docker-compose.yml` file is located,
if it suits Your case.

<CodeSnippet
  code={`
      networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml (network)"
  annotations={`
    3:We define the name of the network
    4:We do the manual IP address management (IPAM)
    7:Creating the subnet (group of IP addresses) for the network
    8:Setting the gateway for the network
  `}
/>

Then, we move onto the IP address management (IPAM) part. Here, we define the subnet for the network
where out containers will be located, by explicitly assigning them adresses from the range
from `10.5.0.0` to `10.5.255.254` (where 65534 possible hosts is ofc an overshot, but it's just an example).
To get this kind of IP address range, we must define the subnet as `10.5.0.0/16`, where the `/16` part
is the CIDR notation for the subnet mask. The `gateway` keyword is used to define the IP address
of the gateway for the network, which is the address of the host machine on which the containers
will be spawned i.e. the parent CI container. The rule of thumb here is that **the gateway address
must be the first address in the subnet range**, so in our case, it would be `10.5.0.1`.

As for the network configuration of the services, we must simply specify for each service the
network to which it belongs and the IP address that it will be assigned to. This can be done
by using the `networks` keyword, which will contain the name of the network and the `ipv4_address`
field with the desired IP address:

<CodeSnippet
  code={`
    services:
      redis:
        ...
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        ...

      pytest-runner:
        ...
        networks:
          test-network:
            ipv4_address: 10.5.0.3
        ...
  `}
  language="yaml"
  filename="docker-compose.yml (services networking snippets)"
  annotations={`
    4-6:Redis service networking configuration
    11-13:Pytest runner networking configuration
  `}
/>

So now in our configuration of unit tests, we can be sure that whenever we make any calls like
API calls or database queries, we will be able to use the IP addresses of the containers instead
of their DNS-resolved names.

Finally, after running our modified mock stack setup, we should see that the Docker daemon
successfully spins up a new network with two containers inside it, performs the tests defined
in the `test_connector.py` module and then destroys the network along with the containers.
Let's see if that is the case üôà.

<CaptionedImage
  src="/assets/images/compose_first_stack.png"
  alt="The composed pinger has received its service-oriented pong üêãüèì"
  height="400"
/>

We see not only that the mock environment has been successfully created and executed its workload,
**but** we have now access to nicely separated logs of the services that are spawned by the `docker-compose` tool,
due to their lines being prepended with the container's name. These logs can be even scrapped by some other
tool (if we're talking about a setup oriented towards CD) for further analysis. The possibilities are there‚ùï

But, we have reached our goal of testing integration of our connector with the Redis database,
but the thing is that we have used the `redis` library that, by the design, already accomplishes
that. So now it is time to move to some raw and dirty HTTP requests and barebones JSON responses.
We're talking about REST API here. 4Ô∏è‚É£0Ô∏è‚É£4Ô∏è‚É£‚ò†Ô∏è

## Covert DevOps üïµÔ∏èüì¶

We will go wild on this one, but it actually references one of the cases I've encountered in my work.
The main goal here is to provide a middle-man application, **an agent** üïµÔ∏è, that requests metrics
from one database and then pushes it into some other remote endpoint. This is kind of similar to
[how Prometheus works in remote-write mode] - it scrapes the metrics from the targets
and passes them forward to some other place. We will implement a very crude version of that
as a miniature Python module (a script, really) that will be imported into some other
source code.

For demonstration purposes, the agent library will be a simple Python module that will be
sending HTTP requests using `requests` library. To produce some metrics, we will use
the awesome [`fake-metrics-generator`] project by Grafana team, which already has a Docker image available
to quickly spin up containers spitting out random metrics. This service will be scraped by Prometheus
to which we will send PromQL queries via the agent for the metrics that we want to retrieve.
The target endpoint for this data will be a simple [MongoDB] instance with a database named 
`metrics`.

This setup may look out of touch with reality, but imagine substituting the `fake-metrics-generator`
with real application that produces metrics or Grafana instance collecting cluster-wide data or 
MongoDB with some other database or application that analyzes the metrics.
Or just "don't-imagine" projects like [prometheus-kafka-adapter],
these things need implementation all over the place due to the different data flows present in the organizations.

This also gives us a chance to do some HTTP calls to the `fake-metrics-generator` container
and communicate with MongoDB via `pymongo` library. So, two distinct ways of communication -
via REST API and a programmatic interface, which is cool for our tutorial here.

Test will be performed by the `pytest` runner, which will use the module to transfer the aforementioned
mock data from Prometheus to MongoDB.

### Pre-mission briefing üìã

We have learned how to glue together containers into a single composition with Docker Compose,
so it is now a matter of defining three aspects of our test stack:

1. Dependency between containers - which container needs another service running to start performing its job
2. Healthchecks - how to define that a container is ready to accept requests or, in general, is healthy
3. Static and dynamic configuration - what can be set in stone via environment variables and what needs templating

These things will help us write a correct Compose YAML and sometimes further organize file structure of our project.

#### No. 1 and 2: Together we start, divided we fail üè≥Ô∏è

Due to the intertwinement of the aforementioned aspects, I will cover them together in this section.

This part should be rather simple, but requires some knowledge about the services that we want to use.
At the beginning, let's focus on the input data i.e. mock metrics to be pushed further to the database.
Our main source of information mined via the agent app will be a Prometheus instance. This service
will need to constantly scrape metrics from our `fake-metrics-generator` service, so it will be
dependent on it. 

This means that before we start the Prometheus service, we need to make sure that the `fake-metrics-generator`
service is up and running. This can be achieved by using the `depends_on` keyword, which will make
the Prometheus service wait for the `fake-metrics-generator` service to be ready.

<CodeSnippet
  code={`
    version: '3'

    services:
      mock-metrics-generator:
        container_name: mock-metrics-generator
        image: node:16-alpine
        command: ['node', '/app/main.node.js']
        ports:
          - 5000:5000
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        
      prometheus:
        container_name: prometheus
        image: prom/prometheus:latest
        depends_on:
          mock-metrics-generator:
            condition: service_healthy
        ports:
          - 9090:9090
        networks:
          test-network:
            ipv4_address: 10.5.0.3

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml (depends_on)"
  annotations={`
    6-7:Generator is a NodeJS app that is built here from source during CI
    9:This port config comes from the project GitHub README
    17-19:We say that this service should wait until the generator is healthy
    21:Default Prometheus port mapping
  `}
/>

Sweet, now the Prometheus will wait until the `fake-metrics-generator` service is healthy,
so all should be good. Well... ü§∑‚Äç‚ôÇÔ∏è. What does it mean that our generator is healthy?
Normally, popular services like Redis or MongoDB have some kind of healthcheck mechanism
built-in, so we can just use that. But what about our `fake-metrics-generator` service?
To be honest, its just a random app that publishes some API endpoint and spits out data
like there is no tomorrow. In this case, **WE** have to define what it means for this service
to be healthy. What we know about this application is that, after booting up, it will
expose the '/metrics' endpoint, so we can continuously send `GET` HTTP requests via `curl` to it
and check if it responds with a `200` status code. To do that, we can use the `--fail` flag,
which will make `curl` return a non-zero exit code if the response status code is not `200`.
This procedure can be defined for `mock-metrics-generator` by using the `healthcheck` keyword:

<CodeSnippet
  code={`
    version: '3'

    services:
      mock-metrics-generator:
        container_name: mock-metrics-generator
        image: node:16-alpine
        command: ['node', '/app/main.node.js']
        ports:
          - 5000:5000
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        healthcheck:
          test: curl --fail http://10.5.0.2:5000/metrics
          interval: 5s
          timeout: 3s
          retries: 5
        
      prometheus:
        container_name: prometheus
        image: prom/prometheus:latest
        depends_on:
          mock-metrics-generator:
            condition: service_healthy
        ports:
          - 9090:9090
        networks:
          test-network:
            ipv4_address: 10.5.0.3

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml (depends_on remastered)"
  annotations={`
    14-17:Just hit that endpoint üëä
  `}
/>

Another healthcheck that we need to define is the one for the Prometheus service. This one
is a bit more tricky, because we need to make sure that the Prometheus service is ready to accept
PromQL queries i.e. have its API open to the outside world. This can be achieved by using the
`wget` tool (for variety sake here), but this time we will use it to hit the root endpoint under `9090` port:

<CodeSnippet
  code={`
    version: '3'

    services:
      mock-metrics-generator:
        container_name: mock-metrics-generator
        image: node:16-alpine
        command: ['node', '/app/main.node.js']
        ports:
          - 5000:5000
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        healthcheck:
          test: curl --fail http://10.5.0.2:5000/metrics
          interval: 5s
          timeout: 3s
          retries: 5
        
      prometheus:
        container_name: prometheus
        image: prom/prometheus:latest
        depends_on:
          mock-metrics-generator:
            condition: service_healthy
        ports:
          - 9090:9090
        networks:
          test-network:
            ipv4_address: 10.5.0.3
        healthcheck:
          test: ["CMD", "wget", "http://localhost:9090"]
          interval: 10s
          timeout: 15s
          retries: 10
          start_period: 40s

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml (Prom healthcheck)"
  annotations={`
    31-35:Just hit that endpoint, Part 2
  `}
/>

On the other side of our data flow pipeline is the MongoDB database. This service will be
responsible for storing the metrics that we will be sending to it via the agent app.
So, we need to make sure that the database is up and running before we start the agent.
Again, we will use `depends_on` keyword and (just to be 100% sure) define a very simple
healthcheck for it:

<CodeSnippet
  code={`
    version: '3'

    services:
      mock-metrics-generator:
        container_name: mock-metrics-generator
        image: node:16-alpine
        command: ['node', '/app/main.node.js']
        ports:
          - 5000:5000
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        healthcheck:
          test: curl --fail http://10.5.0.2:5000/metrics
          interval: 5s
          timeout: 3s
          retries: 5
        
      prometheus:
        container_name: prometheus
        image: prom/prometheus:latest
        depends_on:
          mock-metrics-generator:
            condition: service_healthy
        ports:
          - 9090:9090
        networks:
          test-network:
            ipv4_address: 10.5.0.3
        healthcheck:
          test: ["CMD", "wget", "http://localhost:9090"]
          interval: 10s
          timeout: 15s
          retries: 10
          start_period: 40s

      mongo:
        container_name: mongo
        image: mongo:5.0.3
        ports:
          - 21017:27017
        networks:
          test-network:
            ipv4_address: 10.5.0.4
        healthcheck:
          test: echo 'db.runCommand("ping").ok' | mongo --quiet
          interval: 5s
          timeout: 3s
          retries: 5

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml (Mongo No. 5, bby)"
  annotations={`
    35:Again, the default for MongoDB instances
    40:We can use the mongo client command to check if the database is up and running
  `}
/>

If You want to check out how You can set more complicated healthchecks for MongoDB, check out
the official documentation on the [`mongo` commands] (there are like a billion of them üòµ).

Now, the *creme de la creme* ü§å of this section - tests runner with our module installed
and ready to throw some bogus metrics around. Those tests will require our whole stack to be
up and running, so the `depends_on` keyword will be used again, but with a little more robust
set of conditions:

<CodeSnippet
  code={`
    version: '3'

    services:
      mock-metrics-generator:
        container_name: mock-metrics-generator
        image: node:16-alpine
        command: ['node', '/app/main.node.js']
        ports:
          - 5000:5000
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        healthcheck:
          test: curl --fail http://10.5.0.2:5000/metrics
          interval: 5s
          timeout: 3s
          retries: 5
        
      prometheus:
        container_name: prometheus
        image: prom/prometheus:latest
        depends_on:
          mock-metrics-generator:
            condition: service_healthy
        ports:
          - 9090:9090
        networks:
          test-network:
            ipv4_address: 10.5.0.3
        healthcheck:
          test: ["CMD", "wget", "http://localhost:9090"]
          interval: 10s
          timeout: 15s
          retries: 10
          start_period: 40s

      mongo:
        container_name: mongo
        image: mongo:5.0.3
        ports:
          - 21017:27017
        networks:
          test-network:
            ipv4_address: 10.5.0.4
        healthcheck:
          test: echo 'db.runCommand("ping").ok' | mongo --quiet
          interval: 5s
          timeout: 3s
          retries: 5

    pytest-runner:
      container_name: pytest-runner
      image: python:3.10-alpine
      depends_on:
        mongo:
          condition: service_healthy
        prometheus:
          condition: service_healthy
      networks:
        test-network:
          ipv4_address: 10.5.0.5
      volumes:
        - \${CI_PROJECT_DIR}:/app
      working_dir: /app
      command:
        - sh
        - -c
        - |
          pip \\
            install \\
              pytest
          pytest \\
            test_agent.py

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml (team assembled)"
  annotations={`
    48-52:We wait for both MongoDB and Prometheus to be healthy
    59-67:We install the dependencies and run the tests
  `}
/>

Right now, we have our stack ready to be tested, but we have to remember that we have to
configure Prometheus to scrape the mock data-generating service. Also, as set in out requirements
for the test environment, the MongoDB instance must have a database named `metrics` created, before
we will attempt our tests.

#### No. 3: Knobs and dials üîß

In terms of configuration, we have to solve the following problems in our environment setup:

1. Add a new [scraping target] to Prometheus configuration via `prometheus.yml` file
2. Create a database named `metrics` in MongoDB

These tasks will be classified by me here as the "dynamic" part of the configuration, because
they require some kind of templating to be done at the time of the CI job execution. Next ones
are connected with hardcoded values that can be set via environment variables or, in other words,
typed by us in the `docker-compose.yml` or `.gitlab-ci.yml` files:

3. Move the IP addresses and ports of the services to one place in GitLab CI pipeline as environment variables
4. Set the IP addresses and ports of the services in the `docker-compose.yml` file via environment variables

Configuring Prometheus will be a two-step process, because we have to add a new scraping target
to the `prometheus.yml` file from template using CI commands and then mount it to the container
via `volumes` keyword. The `prometheus.yml` file will be mounted to the container under the path
`/etc/prometheus/prometheus.yml`. First, the YAML part:

<CodeSnippet
  code={`
    scrape_configs:
      - job_name: 'mock_metrics'
        metrics_path: /metrics
        scrape_interval: 5s
        static_configs:
  `}
  language="yaml"
  filename="prometheus.yml (scrape_configs)"
  annotations={`
    2:This is exposed by the fake-metrics-generator
  `}
/>

Now, if You are in any way familiar with Prometheus, You know that this is not enough to make it
due to one line missing from this file. And that is exactly the line that we will be adding
via CI commands. The magic ingredient is the `- targets: ['10.5.0.3:5000']` with the IP address
and port of generator service. This can be done by simply `echo`'ing the line to the file,
being careful to use the `>>` operator, which will append the line to the file, instead of
overwriting it. Plus, we need to account for the indentation of the line:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      tags:
        - "docker"
        - "dind"
      services:
        - name: docker:dind
          alias: my-docker-service
      variables:
        DOCKER_HOST: "tcp://my-docker-service:2375"
        DOCKER_TLS_CERTDIR: ""
        FF_NETWORK_PER_BUILD: "true"
      script:
        ...
        - |
          echo \\
            "    - targets: [10.5.0.3:5000]" \\
            >> prometheus.yml
        ...
  `}
  language="yaml"
  filename=".gitlab-ci.yml (finishing prometheus.yml)"
  annotations={`
    16:We echo the line with targets definition
    18:We append the line to the file
  `}
/>

Now, we have to mount the `prometheus.yml` file to the container. This can be done by using
the `volumes` keyword, which will mount the file from the host machine to the container
under the path `/etc/prometheus/prometheus.yml`:

<CodeSnippet
  code={`
    ...
    prometheus:
      ...
      volumes:
        - \${CI_PROJECT_DIR}/prometheus.yml:/etc/prometheus/prometheus.yml
      ...
  `}
  language="yaml"
  filename="docker-compose.yml (prometheus.yml mount)"
  annotations={`
    1:I've omitted the rest of the compose for brevity
  `}
/>

Next, we have to create the `metrics` database in MongoDB. The `mongo` image used by us has an easy way
to run initialization scripts on the container boot. We just have to mount a JavaScript (or shell) script
to the container under the `/docker-entrypoint-initdb.d/` directory and it will be executed by the `mongo` client
automagically ü™Ñ. The script is rather simple because it adds a user for the `metrics` database,
ensuring its existence at the time of the agent module tests:

<CodeSnippet
  code={`
    db.createUser({
      user: "metrics",
      pwd: "metrics",
      roles: [
        {
          role: "readWrite",
          db: "metrics"
        }
      ]
    })
  `}
  language="javascript"
  filename="mongo-init.js"
  annotations={`
    2-3:Some toy credentials
    4-9:Predefined MongoDB role assignment
  `}
/>

To be even more raunchy, I will just `cat` the contents of this file to the `mongo-init.js` file from the
`.gitlab-ci.yml` file - I will use this fact in a second.

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      tags:
        - "docker"
        - "dind"
      services:
        - name: docker:dind
          alias: my-docker-service
      variables:
        DOCKER_HOST: "tcp://my-docker-service:2375"
        DOCKER_TLS_CERTDIR: ""
        FF_NETWORK_PER_BUILD: "true"
      script:
        ...
        - |
          cat <<EOT >> mongo-init.js
          db.createUser({
            user: "metrics",
            pwd: "metrics",
            roles: [
              {
                role: "readWrite",
                db: "metrics"
              }
            ]
          })
          EOT
        ...
  `}
  language="yaml"
  filename=".gitlab-ci.yml (mongo-init.js)"
  annotations={`
    16:We echo the contents of the file
    18:We append the contents to the file
  `}
/>

Then we plop (mount) that bad boi ü¶π into the `docker-entrypoint-initdb.d` directory and we're good to go.
Moreover, we will set the credentials for the MongoDB database via environment variables, since
we're already using tricks made available by this Docker image.

<CodeSnippet
  code={`
    ...
    mongo:
      ...
      environment:
        MONGO_INITDB_ROOT_USERNAME: root
        MONGO_INITDB_ROOT_PASSWORD: root
        MONGO_INITDB_DATABASE: metrics
      volumes:
        - \${CI_PROJECT_DIR}/mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js
      ...
  `}
  language="yaml"
  filename="docker-compose.yml (mongo-init.js mount)"
  annotations={``}
/>

Cool, we've got our metrics scraping ready and our database is up and running. If we check out our
full `docker-compose.yml` file, we can see that we have a lot of hardcoded values there. I've marked
the more important lines with single-line comments. We can move those values to the environment variables
defined in the `.gitlab-ci.yml` file, so that we can easily change them in one place. This is possible because
Docker Compose, as shown previously, **can use the environment variables of its host*. So, we will be able to
configure the whole CI setup from the `.gitlab-ci.yml` file, which is nice.

<CodeSnippet
  code={`
    version: '3'

    services:
      mock-metrics-generator:
        container_name: mock-metrics-generator
        image: node:16-alpine
        command: ['node', '/app/main.node.js']
        ports:
          - 5000:5000
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        healthcheck:
          test: curl --fail http://10.5.0.2:5000/metrics
          interval: 5s
          timeout: 3s
          retries: 5
        
      prometheus:
        container_name: prometheus
        image: prom/prometheus:latest
        depends_on:
          mock-metrics-generator:
            condition: service_healthy
        volumes:
          - \${CI_PROJECT_DIR}/prometheus.yml:/etc/prometheus/prometheus.yml
        ports:
          - 9090:9090
        networks:
          test-network:
            ipv4_address: 10.5.0.3
        healthcheck:
          test: ["CMD", "wget", "http://localhost:9090"]
          interval: 10s
          timeout: 15s
          retries: 10
          start_period: 40s

      mongo:
        container_name: mongo
        image: mongo:5.0.3
        environment:
          MONGO_INITDB_ROOT_USERNAME: root
          MONGO_INITDB_ROOT_PASSWORD: root
          MONGO_INITDB_DATABASE: metrics
        volumes:
          - \${CI_PROJECT_DIR}/mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js
        ports:
          - 21017:27017
        networks:
          test-network:
            ipv4_address: 10.5.0.4
        healthcheck:
          test: echo 'db.runCommand("ping").ok' | mongo --quiet
          interval: 5s
          timeout: 3s
          retries: 5

    pytest-runner:
      container_name: pytest-runner
      image: python:3.10-alpine
      depends_on:
        mongo:
          condition: service_healthy
        prometheus:
          condition: service_healthy
      networks:
        test-network:
          ipv4_address: 10.5.0.5
      volumes:
        - \${CI_PROJECT_DIR}:/app
      working_dir: /app
      command:
        - sh
        - -c
        - |
          pip \\
            install \\
              pytest
          pytest \\
            test_agent.py

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  annotations={`
    9:Generator port -> GENERATOR_PORT
    12:Generator address -> GENERATOR_IP_ADDRESS
    14:Here we can substitute for the GENERATOR_* variables
    28:Prometheus port -> PROMETHEUS_PORT
    31:Prometheus address -> PROMETHEUS_IP_ADDRESS
    33:Here we can substitute the port
    43:MongoDB root username -> MONGO_ROOT_USERNAME
    44:MongoDB root password -> MONGO_ROOT_PASSWORD
    45:MongoDB database name -> MONGO_TEST_DATABASE
    49:MongoDB port -> MONGO_PORT
    52:MongoDB address -> MONGO_IP_ADDRESS
    69:Here we can omit the IP address - we 100% won't be using it ü§∑
  `}
/>

Now, we can create the desired variables to the `.gitlab-ci.yml`, as global variables so that
they are nicely separated from the rest of the file as a sort of configuration section:

<CodeSnippet
  code={`
    variables:
      MOCK_GENERATOR_HOST: "10.5.0.3"
      MOCK_GENERATOR_PORT: "5000"
      MONGODB_HOST: "10.5.0.2"
      MONGODB_PORT: "27017"
      PROMETHEUS_HOST: "10.5.0.4"
      PROMETHEUS_PORT: "9090"
      PYTEST_HOST: "10.5.0.5"
      MONGODB_ROOT_USERNAME: "root"
      MONGODB_ROOT_PASSWORD: "r00t"
      MONGODB_TEST_DATABASE: "metrics"

    test-job:
      image: docker:latest
      ...
  `}
  language="yaml"
  filename=".gitlab-ci.yml (global variables)"
  annotations={`
    10:They will never know ü§´
    15:The rest is as it was
  `}
/>

Returning to our `docker-compose.yml` file, we can now substitute the hardcoded values with the
environment variables (and actually pass the envvars to the `pytest-runner` container):

<CodeSnippet
  code={`
    version: '3'

    services:
      mock-metrics-generator:
        container_name: mock-metrics-generator
        image: node:latest
        volumes:
          - \${CI_PROJECT_DIR}/fake-metrics-generator/dist:/app
          - \${CI_PROJECT_DIR}/mock.json:/app/config/data.json
        ports:
          - \${MOCK_GENERATOR_PORT}:5000
        command: \['node', '/app/main.node.js'\]
        networks:
          test-network:
            ipv4_address: \${MOCK_GENERATOR_HOST}
        healthcheck:
          test: curl --fail http://\${MOCK_GENERATOR_HOST}:\${MOCK_GENERATOR_PORT}/metrics
          interval: 5s
          timeout: 3s
          retries: 5
        
      prometheus:
        container_name: prometheus
        image: prom/prometheus:latest
        depends_on:
          mock-metrics-generator:
            condition: service_healthy
        volumes:
          - \${CI_PROJECT_DIR}/prometheus.yml:/etc/prometheus/prometheus.yml
        ports:
          - \${PROMETHEUS_PORT}:9090
        networks:
          test-network:
            ipv4_address: \${PROMETHEUS_HOST}
        healthcheck:
          test: \["CMD", "wget", "http://localhost:9090"\]
          interval: 10s
          timeout: 15s
          retries: 10
          start_period: 40s

      mongo:
        container_name: mongo
        image: mongo:5.0.3
        environment:
          - MONGO_INITDB_ROOT_USERNAME=\${MONGODB_ROOT_USERNAME}
          - MONGO_INITDB_ROOT_PASSWORD=\${MONGODB_ROOT_PASSWORD}
          - MONGO_INITDB_DATABASE=\${MONGODB_TEST_DATABASE}
        volumes:
          - \${CI_PROJECT_DIR}/metrics.db-init.js:/docker-entrypoint-initdb.d/mongo-init.js
        ports:
          - \${MONGODB_PORT}:27017
        networks:
          test-network:
            ipv4_address: \${MONGODB_HOST}
        healthcheck:
          test: echo 'db.runCommand("ping").ok' | mongo --quiet
          interval: 5s
          timeout: 3s
          retries: 5

      pytest-runner:
        container_name: pytest-runner
        image: python:3.10-alpine
        depends_on:
          mongo:
            condition: service_healthy
          prometheus:
            condition: service_healthy
        environment:
          - MONGODB_HOST
          - MONGODB_PORT
          - MONGODB_USERNAME
          - MONGODB_PASSWORD
          - MOCK_GENERATOR_HOST
          - MOCK_GENERATOR_PORT
        networks:
          - test-network
        volumes:
          - \${CI_PROJECT_DIR}:/app
        working_dir: /app
        command:
          - sh
          - -c
          - |
            pip \\
              install \\
                pytest
            pytest \\
              test_agent.py

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml (FINAL)"
  annotations={`
    71-76:These variables will be useful during the tests
    78:We still need on the test network!
  `}
/>

If we run this behemoth in our CI, we should see a couple of logs that reassure us that
we're on the right track and our test environment is ready to be dissected, poked,
prodded and analyzed with some botched-together Python code. In the first (1) section,
we can see that all components have started. The second part (2) is a shout-out from `mongo`
service that our bogus database user has been created. The third part (3) is a log from
Prometheus which has successfully parsed out templated `prometheus.yml` file and started
its rule manager. And finally, the last part (4) is a log from the `pytest-runner` service
which has successfully installed the `pytest` library and ran the tests. At this moment those
tests are some two `assert True` placeholder functions, hence the two passes from the `test_agent.py`
suite (those two little dots at the end of the log).

<CaptionedImage
  src="/assets/images/compose_second_stack.png"
  alt="It's alive üßü‚Äç‚ôÇÔ∏è"
  height="700"
/>

Again, note the fact that we can actually **see and analyze** the logs of the services,
which starts to become **really** helpful in the context of managing this test environment.
The only part left now is to write a very minimal agent for pushing metrics from Prometheus to
MongoDB.

## A pythonic sidequest üêçüó∫Ô∏è

`request` and `mongoengine`. Those will be the two building blocks of our agent. The first one
will be used to send HTTP requests to the Prometheus service and the second one will be used
to communicate with the MongoDB database i.e. create documents in the `metrics` collection.
It is not important how we will organize the data, because it is not the focus of this article.
We only case that the trash from point A is transferred to point B. Our agent must be able to
perform these three basic tasks:

1. Get configuration about MongoDB and Prometheus instances from environment variables
2. Send HTTP request to Prometheus to retrieve metrics for our fake job
3. Send the metrics to MongoDB as simple documents.

For connection with MongoDB we can use the [`mongoengine` library], which is a nice wrapper
around the `pymongo` library. It allows us to define the structure of the documents that we
want to store in the database and then use the `save()` method to push them to the database.
Communication with Prometheus will be realized strictly via [HTTP GET requests, that will
contain a PromQL query as a parameter]. The response will be a plain text with the metrics,
but we won't delve into parsing it into JSON entries.

So a basic scaffolding of our agent will look like this:

<CodeSnippet
  code={`
    import dataclasses

    import mongoengine
    import pymongo
    import requests


    @dataclasses.dataclass()
    class MetricsPushingAgent:
        config: dict = dataclasses.field(default_factory=dict)
        _mongo: pymongo.MongoClient = dataclasses.field(init=False, default_factory=pymongo.MongoClient)
        _prometheus: requests.Session = dataclasses.field(init=False, default_factory=requests.Session)

        def __post_init__(self) -> None:
            ...

        def get(self) -> str:
            ...

        def push(self, metrics: str) -> None:
            ...
  `}
  language="python"
  filename="agent.py (boilerplate)"
  annotations={`
    4:This import is for typing purposes really
    8:I have an undying love for dataclasses, sorry ü§∑
    10:We will store the configuration in a dictionary with some known schema
    11-12:We will use these two objects to communicate with the databases
    14-15:We will initialize them in the __post_init__ method
    17:We will use this method to get the metrics from Prometheus
    20:We will use this method to push the metrics to MongoDB
  `}
/>

Let's move on onto the most basic task - configuration dictionary of our agent,
since we can explicitly define the schema of the configuration dictionary via simple
`typing.TypedDict` object:

<CodeSnippet
  code={`
    import dataclasses
    import typing

    import mongoengine
    import pymongo
    import requests


    class AgentConfig(typing.TypedDict):
        mongo_host: str
        mongo_port: int
        mongo_user: str
        mongo_db: str
        mongo_password: str
        prometheus_host: str
        prometheus_port: int
        prometheus_job: str


    @dataclasses.dataclass()
    class MetricsPushingAgent:
        config: AgentConfig = dataclasses.field(default_factory=dict)  # type: ignore
        ...
  `}
  language="python"
  filename="agent.py (config)"
  annotations={`
    17:This field corresponds to the job name set in the prometheus.yml file
  `}
/>

This results in a small QoL improvement, because now we can use the `config` field
as a dictionary with the known schema. This means that our IDE will be able to
suggest the keys of the dictionary. Also, this enforces the schema of the configuration
dictionary, so we can be sure that we won't be passing some random garbage to the agent.
After this small change, we can move to initialize the `pymongo` and `requests` objects.

### Turning over new leaves üçÉüçÅ

Checking out the documentation of the [`mongoengine` library for connecting to MongoDB instances],
we can see that we can use `mongoengine.connect` function to connect to the database, that will
return an authorized `mongo` session, which will be stored in the `_mongo` field of our agent.

<CodeSnippet
  code={`
    import dataclasses
    import typing

    import mongoengine
    import pymongo
    import requests


    class AgentConfig(typing.TypedDict):
        mongo_host: str
        mongo_port: int
        mongo_user: str
        mongo_db: str
        mongo_password: str
        prometheus_host: str
        prometheus_port: int
        prometheus_job: str


    @dataclasses.dataclass()
    class MetricsPushingAgent:
        config: AgentConfig = dataclasses.field(default_factory=dict)  # type: ignore
        _mongo: pymongo.MongoClient = dataclasses.field(init=False, default_factory=pymongo.MongoClient)
        _prometheus: requests.Session = dataclasses.field(init=False, default_factory=requests.Session)
        _prometheus_full_url: str = dataclasses.field(init=False, default='')
        _logger: logging.Logger = dataclasses.field(init=False, default_factory=logging.getLogger)

        def __post_init__(self) -> None:
          logging.basicConfig(level=logging.INFO)
          self._logger.setLevel(logging.INFO)
          self._logger.addHandler(logging.StreamHandler())

          self._mongo = mongoengine.connect(
              host=self.config['mongo_host'],
              port=self.config['mongo_port'],
              username=self.config['mongo_user'],
              password=self.config['mongo_password'],
              db=self.config['mongo_db'],
              alias='default'
          )
          self._logger.info(
              'Connected to MongoDB instance: %s:%s',
              self.config['mongo_host'], self.config['mongo_port']
          )

          self._prometheus = requests.Session()
          self._prometheus_full_url = f'http://{self.config["prometheus_host"]}:{self.config["prometheus_port"]}'
          self._logger.info(
              'Connected to Prometheus instance: %s',
              self._prometheus_full_url
          )

        self._logger.info('Agent ready!')
      ...
  `}
  language="python"
  filename="agent.py (initialization)"
  annotations={`
    26:Loggers are always useful ü™µü™ì
    29-31:This ensures that out log entries show up in pytest summaries
    34-35:These variables point to our MongoDB instance from Docker Compose
    36-37:Authorization data set in CI environment variables
    38:This sets the default database for the session to our metrics one
    39:We set the alias to 'default' so that we can use it in the data models
    47:This variable is for readability purposes
  `}
/>

When it comes to the `mongoengine` library we have to remember that it is an ODM (Object Document Mapper),
which means that we **must** define the structure of the documents that we want to store in the database.
This is done via classes that inherit from the [`mongoengine.Document`] class. So, we will define a simple
data model for our metrics, nested in the `MetricsPushingAgent` class and put at the top of its code to
somewhat group it visually with the rest of the agent fields.

To define the structure of the document, we will use the `mongoengine.fields` classes, which are
the equivalent of the `pymongo` fields. We will use the `timestamp` of `mongoengine.fields.DateTimeField` type to store
the timestamp of the metrics, the `metrics` and `job` fields of `mongoengine.fields.StringField` kind to store the job name 
and the metrics themselves. We will also construct the `meta` dictionary, which will be used to specify
the database and collection that we want to store our documents in. The `prometheus` value for our collection name
will signify that these metrics come from Prometheus instance. You can imagine creating different collections
for different sources of metrics, like `grafana`, `datadog`, `newrelic`, etc. via this simple syntax.

<CodeSnippet
  code={`
    @dataclasses.dataclass()
    class MetricsPushingAgent:
        config: AgentConfig = dataclasses.field(default_factory=dict)  # type: ignore
        _mongo: pymongo.MongoClient = dataclasses.field(init=False, default_factory=pymongo.MongoClient)
        _prometheus: requests.Session = dataclasses.field(init=False, default_factory=requests.Session)
        _prometheus_full_url: str = dataclasses.field(init=False, default='')
        _logger: logging.Logger = dataclasses.field(init=False, default_factory=logging.getLogger)

        class MetricsMongoEntry(mongoengine.Document):
            timestamp = mongoengine.fields.DateTimeField()
            job = mongoengine.fields.StringField()
            metrics = mongoengine.fields.StringField()

            meta: dict = {
                'db_alias': 'default',
                'collection': 'prometheus'
            }
  `}
  language="python"
  filename="agent.py (data model)"
  annotations={`
    9:Setting the right inheritance
    10-12:Defining the fields of the document using mongoengine.fields classes
    14-17:This metadata is needed to specify the target database and collection
  `}
/>

This class possesses two attributes that are of the upmost importance for us, that come from underlying
`mongoengine.Document` class. The first one is the `save()` method, which will be used to push the
documents to the database i.e. register a POST transaction to MongoDB. The second one is the `objects`
attribute, which is a `mongoengine.queryset.QuerySet` object, which will be used to query the database
for the documents, which will come in handy when we will be writing the tests for our agent.

Since we are on the topic of MongoDB, we will define the `push` method of our agent, which will be used
to push the metrics to the database. This method will be very simple, because we will just create a new
document, populate it with the data and then save it to the database. We will also log the fact that
we have pushed the metrics to the database. For readiblity, the actual creation of `mongoengine` entry
will be done in a separate method called `create_mongo_document`.

<CodeSnippet
  code={`
    @dataclasses.dataclass()
    class MetricsPushingAgent:
        config: AgentConfig = dataclasses.field(default_factory=dict)  # type: ignore
        _mongo: pymongo.MongoClient = dataclasses.field(init=False, default_factory=pymongo.MongoClient)
        _prometheus: requests.Session = dataclasses.field(init=False, default_factory=requests.Session)
        _prometheus_full_url: str = dataclasses.field(init=False, default='')
        _logger: logging.Logger = dataclasses.field(init=False, default_factory=logging.getLogger)

        class MetricsMongoEntry(mongoengine.Document):
            timestamp = mongoengine.fields.DateTimeField()
            job = mongoengine.fields.StringField()
            metrics = mongoengine.fields.StringField()

            meta: dict = {
                'db_alias': 'default',
                'collection': 'prometheus'
            }
        
        def __post_init__(self) -> None:
          ...

        def push(self, metrics: str) -> None:
          self._logger.info(
              'Pushing metrics to MongoDB instance: %s',
              self.config['mongo_host'] + ':' + str(self.config['mongo_port'])
          )
          try:
              self.create_mongo_document(metrics)
              self._logger.info('Metrics pushed')
          except Exception:  # pylint: disable=broad-except
              self._logger.exception('Could not push metrics to MongoDB instance')

        def create_mongo_document(self, metrics: str) -> None:
            entry = self.MetricsMongoEntry(
                timestamp=datetime.datetime.now(),
                job=self.config['prometheus_job'],
                metrics=metrics
            )
            entry.save()
  `}
  language="python"
  filename="agent.py (push)"
  annotations={`
    20:Same as previously
    22:Metrics will be fetched from Prometheus shortly
    30:A basic 'try-except' block to catch any errors (we silence pylint here to not annoy us with warnings)
    34-38:We populate a new entry with current time, mock job name and serialized metrics from Prometheus
    39:Posting the new document
  `}
/>

Mongo support is now done for our agent and the only thing left is fetching data from Prometheus
with a very simple `get` method. This method will be used to send a GET request to the Prometheus
service, which will return a plain text with the metrics. The endpoint we are aiming for is
[`/api/v1/query`], which is used to query the Prometheus database with PromQL queries.
What is rather weird is that this endpoint accepts `query` parameter, which is string containing the
desired query, resulting in a funny-looking `/query?query=...` call. But whatever, `requests` library
will take care of that for us. But what will be the query? Since the overall theme of this section is
"whatever, just give me something ü§∑", we will use the following query: `{job=job_name}`. This
will return all the metrics from the job, which is the one defined in the `config`.
This is a very simple query, but it will be enough for our purposes:

<CodeSnippet
  code={`
    @dataclasses.dataclass()
    class MetricsPushingAgent:
        config: AgentConfig = dataclasses.field(default_factory=dict)  # type: ignore
        _mongo: pymongo.MongoClient = dataclasses.field(init=False, default_factory=pymongo.MongoClient)
        _prometheus: requests.Session = dataclasses.field(init=False, default_factory=requests.Session)
        _prometheus_full_url: str = dataclasses.field(init=False, default='')
        _logger: logging.Logger = dataclasses.field(init=False, default_factory=logging.getLogger)

        class MetricsMongoEntry(mongoengine.Document):
            timestamp = mongoengine.fields.DateTimeField()
            job = mongoengine.fields.StringField()
            metrics = mongoengine.fields.StringField()

            meta: dict = {
                'db_alias': 'default',
                'collection': 'prometheus'
            }

        def __post_init__(self) -> None:
            ...

        def get(self) -> str:
            self._logger.info(
                'Getting metrics from Prometheus instance at %s',
                self._prometheus_full_url
            )
            job_name = self.config['prometheus_job']
            response = self._prometheus.get(
                url=self._prometheus_full_url + '/api/v1/query',
                params={
                    'query': f'{{job="{job_name}"}}'
                },
                timeout=5
            )
            if response.status_code != 200:
                raise RuntimeError(
                    'Could not get metrics from Prometheus instance:',
                    response.text
                )
            return response.text

        def push(self, metrics: str) -> None:
            ...

        def create_mongo_document(self, metrics: str) -> None:
            ...
  `}
  language="python"
  filename="agent.py (get)"
  annotations={`
    27:Quickly snag the job name from our config
    29:This is the endpoint URL
    31:The query parameter
    40:We return a raw text response to store inside a string field in the Mongo document
  `}
/>

And that's it! Our agent is ready to be tested. Below the whole code for this small module is presented,
so that You can see how it all fits together:

<CodeSnippet
  code={`
    import dataclasses
    import datetime
    import logging
    import typing

    import mongoengine
    import mongoengine.fields
    import pymongo
    import requests


    class AgentConfig(typing.TypedDict):
        mongo_host: str
        mongo_port: int
        mongo_user: str
        mongo_db: str
        mongo_password: str
        prometheus_host: str
        prometheus_port: int
        prometheus_job: str


    @dataclasses.dataclass()
    class MetricsPushingAgent:
        config: AgentConfig = dataclasses.field(default_factory=dict)  # type: ignore
        _mongo: pymongo.MongoClient = dataclasses.field(init=False, default_factory=pymongo.MongoClient)
        _prometheus: requests.Session = dataclasses.field(init=False, default_factory=requests.Session)
        _prometheus_full_url: str = dataclasses.field(init=False, default='')
        _logger: logging.Logger = dataclasses.field(init=False, default_factory=logging.getLogger)

        class MetricsMongoEntry(mongoengine.Document):
            timestamp = mongoengine.fields.DateTimeField()
            job = mongoengine.fields.StringField()
            metrics = mongoengine.fields.StringField()

            meta: dict = {
                'db_alias': 'default',
                'collection': 'prometheus'
            }

        def __post_init__(self) -> None:
            logging.basicConfig(level=logging.INFO)
            self._logger.setLevel(logging.INFO)
            self._logger.addHandler(logging.StreamHandler())

            self._mongo = mongoengine.connect(
                host=self.config['mongo_host'],
                port=self.config['mongo_port'],
                username=self.config['mongo_user'],
                password=self.config['mongo_password'],
                db=self.config['mongo_db'],
                alias='default'
            )
            self._logger.info(
                'Connected to MongoDB instance: %s:%s',
                self.config['mongo_host'], self.config['mongo_port']
            )

            self._prometheus = requests.Session()
            self._prometheus_full_url = f'http://{self.config["prometheus_host"]}:{self.config["prometheus_port"]}'
            self._logger.info(
                'Connected to Prometheus instance: %s',
                self._prometheus_full_url
            )

            self._logger.info('Agent ready!')

        def get(self) -> str:
            self._logger.info(
                'Getting metrics from Prometheus instance at %s',
                self._prometheus_full_url
            )
            job_name = self.config['prometheus_job']
            response = self._prometheus.get(
                url=self._prometheus_full_url + '/api/v1/query',
                params={
                    'query': f'{{job="{job_name}"}}'
                },
                timeout=5
            )
            if response.status_code != 200:
                raise RuntimeError(
                    'Could not get metrics from Prometheus instance:',
                    response.text
                )
            return response.text

        def push(self, metrics: str) -> None:
            self._logger.info(
                'Pushing metrics to MongoDB instance: %s',
                self.config['mongo_host'] + ':' + str(self.config['mongo_port'])
            )
            try:
                self.create_mongo_document(metrics)
                self._logger.info('Metrics pushed')
            except Exception:  # pylint: disable=broad-except
                self._logger.exception('Could not push metrics to MongoDB instance')

        def create_mongo_document(self, metrics: str) -> None:
            entry = self.MetricsMongoEntry(
                timestamp=datetime.datetime.now(),
                job=self.config['prometheus_job'],
                metrics=metrics
            )
            entry.save()
  `}
  language="python"
  filename="agent.py (full)"
  annotations={``}
/>

## The final stretch üèÅ

Now, we can finally write the tests for our agent. We will use the `pytest` library for this task and
its plugin called `pytest-order` to strictly enforce the order of the tests üëÆ‚Äç‚ôÄÔ∏è. This is needed because
we want to save some time if, for example, the MongoDB instance is not up and running. Then, performing
any tests on the agent will be pointless, because it will fail anyway and it wouldn't be our code's fault.

First, we will specify any requirements in a `requirements.txt` file do all necessary dependencies
of the agent module are present during the tests (plus the aforementioned plugin). Note that the
form of this file is consistent with the approach of "kind-of done is slightly better than perfect", used for the
demonstration purposes of this article. In a real-world scenario, these packages should be tagged with
appropriate versions for which the package has been tested and is known to work. But whatever.

<CodeSnippet
  code={`
    mongoengine
    requests
    pytest-order
  `}
  language="text"
  filename="requirements.txt"
  annotations={``}
/>

Then, we will start out tests definition (in the `test_agent.py` file) with a simple fixture
that will be used to initialize the agent with the configuration from the environment variables.

<CodeSnippet
  code={`
    # pylint: disable=protected-access, redefined-outer-name
    import os
    import requests

    import pytest

    import agent


    @pytest.fixture(scope="module")
    def initialized_agent() -> agent.MetricsPushingAgent:
        with open('prometheus.yml', 'r', encoding='utf-8') as prometheus_config_file:
            prometheus_config = prometheus_config_file.read()
            mock_job_name = prometheus_config.split('job_name: ')[1].split('\\n')[0]
            return agent.MetricsPushingAgent({
                'mongo_host': os.environ['MONGODB_HOST'],
                'mongo_port': int(os.environ['MONGODB_PORT']),
                'mongo_user': os.environ['MONGODB_ROOT_USERNAME'],
                'mongo_db': os.environ['MONGODB_TEST_DATABASE'],
                'mongo_password': os.environ['MONGODB_ROOT_PASSWORD'],
                'prometheus_host': os.environ['PROMETHEUS_HOST'],
                'prometheus_port': int(os.environ['PROMETHEUS_PORT']),
                'prometheus_job': mock_job_name.replace("'", '')
            })
  `}
  language="python"
  filename="test_agent.py (fixture)"
  annotations={`
    1:We WILL break those two rules by using fixtures and nested fields of the agent
    7:Here he is! üëÄ
    10:Will work without the scope as well, but this is a good practice
    11:Sweet typing using the import from the agent module (IDE will remember that)
    13-14:We read the job name from the prometheus.yml file (knowing its structure)
    16-22:Those variables correspond to those set in CI/Docker Compose
    23:We pass the job name to the agent config
  `}
/>

With this approach, we initialize the agent **ONCE** ‚òùÔ∏è and just pass it to the tests that need it.
As for out first test we want to check if Prometheus instance is even up and ready to accept out test GETs.
We can do that by listing the active targets of the Prometheus instance and checking if the scrape job
for out `mock-metrics-generator` is present. This is done via the `/api/v1/targets` endpoint, which
returns a JSON with the active targets. We will use the `requests` library to send a raw GET request to
this endpoint and then check if the response code is 200 and if the job name is present in the response text.
Note the `@pytest.mark.order(1)` decorator, which will ensure that this test will be run first, before
any other tests. For other complicated dependencies between tests, check out the [plugin documentation]!

<CodeSnippet
  code={`
    @pytest.mark.order(1)
    def test_if_prometheus_job_is_active(initialized_agent: agent.MetricsPushingAgent) -> None:
        response = requests.get(
            url=f'{initialized_agent._prometheus_full_url}/api/v1/targets',
            timeout=5
        )
        assert response.status_code == 200
        assert initialized_agent.config['prometheus_job'] in response.text
  `}
  language="python"
  filename="test_agent.py (prometheus up)"
  annotations={`
    1:Mark as the first test 1Ô∏è‚É£
    3:We send a GET request to the Prometheus instance /targets endpoint
    7:We assert that the response code is 200
    8:We assert that the job name is present in the response text among some active jobs 
  `}
/>

If this test is successful, we can move on to the next one, which will check if our agent can send a GET
request for the metrics to the Prometheus instance. We will use the `get` method of the agent to do that
and check if the response contains a hardcoded prefix for each metric of `fake__` (which comes from the
`fake-metrics-generator` default setup):

<CodeSnippet
  code={`
    @pytest.mark.order(2)
    def test_prometheus_call(initialized_agent: agent.MetricsPushingAgent) -> None:
        test_metrics = initialized_agent.get()
        assert test_metrics is not None
        assert 'fake__' in test_metrics
  `}
  language="python"
  filename="test_agent.py (Prometheus get via agent)"
  annotations={`
    1:Mark as the second test 2Ô∏è‚É£
    2:We can just put the initialized adapter fixture as an argument to pass it here üòÉ
    5:That is the check for the metrics prefix
  `}
/>

After our agent passes those first two tests, we can move to checking MongoDB readiness to accept new data,
by manually creating some random entry in the `metrics` database and then checking if it is present
via `objects` attribute of the `MetricsMongoEntry` class. We will use the `create_mongo_document` method
to glue together that "sub-mock" entry like so:

<CodeSnippet
  code={`
    @pytest.mark.order(3)
    def test_mongo_call(initialized_agent: agent.MetricsPushingAgent) -> None:
        test_metrics = 'happines 100%'
        initialized_agent.create_mongo_document(test_metrics)
        job_name = initialized_agent.config['prometheus_job']
        fetched_entry = initialized_agent.MetricsMongoEntry.objects(  # type: ignore
            job=job_name
        ).first()
        assert fetched_entry is not None
        assert fetched_entry.job == job_name
        assert fetched_entry.metrics == test_metrics
  `}
  language="python"
  filename="test_agent.py (MongoDB get via agent)"
  annotations={`
    1:Mark as the third test 3Ô∏è‚É£
    3:The random metric content, we don't care about it, just a string value
    4.We create a new document in the database, by passing the string "metric"
    6-8:We fetch the document from the database, by querying for the job name
    10:The job name is set automagically by the create_mongo_document method ü™Ñ
    11:We check if the metrics are the same as the ones we have created like 0.5s ago
  `}
/>

Those three tests ensure that before moving to our final test, all components are up and running,
the data flow **should** work and the agent is ready to be tested. The final test will be subsequent combinations
of `get` and `push` agent methods, which are called here "an event loop". This loop will contain 10 of such steps,
where we will fetch the metrics from Prometheus and then push them to MongoDB. After each push, the resulting
document will be fetched from the database and checked if it is present and if the metrics are the same as
the ones we have pushed. This will be done in a loop, so that we can check if the agent is able to perform
this task multiple times in a row. Note the `@pytest.mark.order(4)` decorator, which will ensure that this
test will be run last, after all other tests.

<CodeSnippet
  code={`
    @pytest.mark.order(4)
    def test_basic_event_loop(initialized_agent: agent.MetricsPushingAgent) -> None:
        target_database = initialized_agent.config['mongo_db']
        number_of_documents_to_create = 10

        for _ in range(number_of_documents_to_create):
            initialized_agent.push(
                metrics=initialized_agent.get()
            )
            metrics_from_mongo = initialized_agent._mongo[target_database].prometheus.find_one(
                {
                    'job': initialized_agent.config['prometheus_job']
                }
            )

            assert metrics_from_mongo is not None
            initialized_agent._logger.info(
                'Metrics found in MongoDB: %s',
                metrics_from_mongo['metrics']
            )

            assert metrics_from_mongo['metrics']
            assert metrics_from_mongo['timestamp']

        documents_count = initialized_agent._mongo[target_database].prometheus.count_documents(
            {
                'job': initialized_agent.config['prometheus_job']
            }
        )
        assert documents_count == number_of_documents_to_create
        initialized_agent._logger.info('Event loop test finished successfully')
  `}
  language="python"
  filename="test_agent.py (event loop)"
  annotations={`
    1:Mark as the fourth test 4Ô∏è‚É£
    4:We will perform 10 iterations of the event loop
    7-9:We push what we fetch
    10-14:We fetch the document from the database
    16:We check if the document is present
    25-30:We check if the number of documents in the database is equal to the number of iterations
  `}
/>

And this concludes our tests for the agent. Running them on out containerized stack within the CI
should result in four little dots in the `pytest` summary, which means that all tests have passed,
the environment **works** and the agent is ready to be used in the "production" environment.
So, fingers crossed ü§û!

<CaptionedImage
  src="/assets/images/compose_final_stack.png"
  alt="Holy s#*%! All tests passed üéâ"
  height="700"
/>

An astounding success. We can even see (if You zoom in a lil bit), that MongoDB accepted new entries,
as seen in the yellow-boxed portion of the logs. The data flows, the environment works and we didn't
need any access to external Prometheus and MongoDB instances to check if our module is simply compliant
with protocols set up by those tools or at least third-party libraries offering such functionality.

<img src="/assets/images/independent.png" alt="Independent" height="500" />

## Containerization of independence üì¶üá∫üá∏üéÜ

I know it has been a long journey, so let's recap what we have managed to learn and achieve:

- how GitLab sets up its containers and what are services (so, Docker stuff)
- using Docker Compose to create a containerized environment for our tests inside GitLab CI ü¶ä
- how to make the configuration and networking a little more readable and adjustable via environment variables
- a fun tool to generate mock metrics (thanks Grafana Labs team! üôè)
- how to setup Prometheus and MongoDB instances in Docker Compose (that's a bonus! üí≤)
- a quick look at interfacing with those services from Python (via libraries and raw HTTP requests)

You can similarly organize your tests but with different tools and services. The important
thing is that you can do it **independently** from the rest of the world, gluing everything together with
Docker Compose manifests stored in some CI-dedicated directory. This way, you can be sure that
the environment will be the same for everyone, no matter if they are using Windows, Linux or macOS.
It is all in nicely defined, idempotent and reproducible Docker images.

I hope that You will see the value and possible applications of this approach. I know that
it may not be perfect, but it also opens up a way to solve other problems, like mounting secret data
to custom Docker images. Docker Compose gives us the flexibility to shape our little experimental world
inside other miniature worlds, which I think is a very awesome concept. Just think about it - what are
people if not such worlds on our beautiful planet Earth? üåç

See, I've even managed to sneak in some philosophic ramble at the end. I hope that You have enjoyed
this article and that You have learned something new. If You have any questions, suggestions or
criticism, please contact me via social media - I suspect that this Containerized Integration‚Ñ¢ thought
experiment can be further improved and I would love to hear Your ideas on how to do that. Peace ‚úåÔ∏è

<img src="/assets/images/peace-out.gif" alt="Bye" style="height:300px" />

[Crystalheart Pulse-Staff]: https://www.wowhead.com/tbc/item=28782/crystalheart-pulse-staff
[`services` keyword in the `.gitlab-ci.yml` file]: https://docs.gitlab.com/ee/ci/services/
[plethora]: https://medium.com/@oachuy/gitlab-runner-with-docker-dind-3e0e1862662f
[of]: https://docs.gitlab.com/ee/ci/docker/using_docker_build.html
[tutorials]: https://docs.gitlab.com/ee/ci/docker/using_docker_build.html
[`depends_on` Docker Compose keyword]: https://docs.docker.com/compose/compose-file/05-services/#depends_on
[automatically generated ones]: https://github.com/docker/compose/issues/6316
[healthcheck]: https://docs.docker.com/engine/reference/builder/#healthcheck
[how Prometheus works in remote-write mode]: https://prometheus.io/docs/concepts/remote_write_spec/
[`fake-metrics-generator`]: https://github.com/grafana/fake-metrics-generator
[MongoDB]: https://www.mongodb.com/
[prometheus-kafka-adapter]: https://github.com/Telefonica/prometheus-kafka-adapter
[`mongo` commands]: https://docs.mongodb.com/manual/reference/command/ping/
[scraping target]: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config
[`mongoengine` library]: https://mongoengine-odm.readthedocs.io/
[HTTP GET requests, that will contain a PromQL query as a parameter]: https://prometheus.io/docs/prometheus/latest/querying/api/#expression-queries
[`mongoengine` library for connecting to MongoDB instances]: https://mongoengine-odm.readthedocs.io/guide/connecting.html
[`mongoengine.Document`]: https://docs.mongoengine.org/guide/defining-documents.html
[`/api/v1/query`]: https://prometheus.io/docs/prometheus/latest/querying/api/#expression-queries
[plugin documentation]: https://pytest-order.readthedocs.io/en/stable/
