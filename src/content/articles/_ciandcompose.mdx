---
title: "Containerized Integration"
date: "2023-11-30"
description: "How to easily spin up a mock environment for integration testing"
tags: ["gitlab", "ci", "tests", "docker", "containers", "integration"]
image:
  thumbnail: "components"
  hero: "components"
  alt: "Lab in a box"
---

## I'm afraid I can't let you do that, Dev ü§ñ

There is a certain kind of dread that comes with the acknowledgment that You can't always cross the cap
between Your local pythonic incantations and the desired environment, where those enchantments are to be
set to be roaming free, setting and getting those `os.environ`'s. "Being part of the Machine" is not only
a figurative thing that a societal contrarian might say, but - in terms of software development - it is
also a very literal thing. You design cogs of various sizes and shapes, that are to be put in the overarching
conglomerate of multi-lingual, multi-paradigm and multi-purpose mechanisms.

What is actually contrary to popular belief, the net is not about connecting people and/or machines at the upper echelons
of the enterprise-ish hierarchy. It's more often than not a convoluted web of rules and blockades,
that separate this connectivity from one environment to another - be it different teams of developers,
different services (often at their different stages) or in general the intra/internet gatekeeping conundrum.

This is, of course, done with good intent, so that the developers don't accidentally
set the production database to be the target of their unit tests, pushing around
valuable data about somebody's World of Warcraft character on the private server
run by a group of "real chill dudes", where You've recently managed to down
that one annoying boss with Your guild, **and** got the highest roll on Crystalheart Pulse-Staff üòå,
to the dismay of Your fellow restoration druids üå≤.

Those are the real casualties that would have been made without that one reasonable sysadmin
at Your software house, who's set up e.g. that VPN so all GitLab CI workers can't reach the K8s cluster,
where the actual services may be running. Additionally, he will also (rightfully) hunt You down
if You try to copy any credentials to those temporary nodes, to "cleverly" bypass those security measures.

I would like to stress here that I am not talking about any continuous deployment (CD) pipelines,
where the code is automatically deployed to the production environment and it mutates its state, in real time.
CD naturally assumes successful CI (continuous integration) and by integration, it means that the code
is already up to the security and coding standards of Your organization. In other words, it is **safe**.
The CI part is what I am really focusing on in this article, so - I am not GitOps hater or something like that üòÖ.

The thing is, some tasks require a bit more than just simple code linting tests and they must
touch upon another important aspect of integration with the existing environment - communication,
more often than not, in the form of authorized API requests
or, in general, calls made through specified protocols.

So we arrive at the "Schr√∂dinger's code problem" i.e. the code that must talk with our services, but really
can't talk with our services at the same time. But what is to be tested here? Language i.e. form.
In other words, we only care if our code can communicate **at all** with the service,
not really if it retrieves a specific portion of the data. The rest can be mocked.
So basically, if You can't play with the real tools - You can at least play with the toys,
and that is what mock test environments are for. An approximation.

In this article, I will try to show You how I spin up those sandboxes for my integration tests,
using GitLab CI Services and Docker, with additional tips for unit testing Python code via containerized
`pytest` nested runners.

**IMPORTANT NOTE**: This article assumes that there is a GitLab CI runner available in Your organization,
that is capable of Docker-in-Docker (DinD) builds. I've included some links to tutorials on how to set up
such a runner, but I will not go into details on how to do that, as it is not the main focus of this article.

## Alive in a box - dead to the outside world üêà‚Äç‚¨õüì¶

To figure out how to create our pseudo-environment, we must first understand what goes on under the hood
when our job is being executed on the GitLab CI runner. In very short terms, the runner
pulls the Docker image specified in the `.gitlab-ci.yml` for a given job and spins up a container,
which has access to the project's files and the GitLab CI variables. The container then executes
the commands specified in the `script` section of the job. The container is then destroyed,
along with all the changes made to the filesystem, unless we explicitly tell the runner to persist
some files between the jobs as either artifacts or cache.

Take a look at the following example:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest 
      script: |
        echo "I am running!"
        docker \\
          run \\
            alpine:latest \\
            sh \\
              -c \\
                "echo 'I am running inside the container!'"
  `}
  language="yaml"
  filename=".gitlab-ci.yml"
  annotations={`
    2:We use the docker image to be able to run docker commands inside the container
    4:This command is executed at the uppermost container level
    5:Here we want to communicate to Docker daemon on the host that we want to nest a new container
    12:This command is executed inside the nested container
  `}
/>

So, we expect to be greeted by two messages, one from the host and one from the nested container.
However, when we run this job, we will only see the first message executed at the main container level,
but the second one will not be executed at all:

<CaptionedImage
  src="/assets/images/nested_container_no_socket.png"
  alt="Dude, where is my daemon? ü§î"
  height="500"
/>

The main reason is the way in which the parent container attempts to communicate with the Docker daemon
that is located on the host. The original Docker daemon is listening on a Unix socket, which is located at
`/var/run/docker.sock` on the host machine. 

The parent container, however, does not have access to this socket,
because it is not mounted to the container's filesystem. This is what line number 21 in the log printout above
is really about. The nested container tries to look for a way to communicate with its parent's daemon,
so it checks if the Docker socket is mounted into its filesystem. 

If not (which is the case here), it then tries to communicate with an external Docker **service**, 
specified under the `DOCKER_HOST` environment variable. **By default**, this variable (for GitLab runners)
is set to `tcp://docker:2375`, which translates to URL of: `http://docker:2375`.
And this is the reason why the nested container prints out the error message about the failed `TCP` lookup.

We can confirm this behavior by listing the sockets mounted to the nested container under `/var/run` path
and setting the `DOCKER_HOST` envvar to some bogus value like `tcp://docker-service:2375`:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      variables:
        DOCKER_HOST: "tcp://docker-service:2375"
      script: |
        echo "I am running!"
        ls \\
          -la \\
          /var/run/
        docker \\
          run \\
            alpine:latest \\
            sh \\
              -c \\
                "echo 'I am running inside the container!'"
  `}
  language="yaml"
  filename=".gitlab-ci.yml (checking behaviour)"
  annotations={`
    4:We point to some other Docker service
    7-9:Check for mounted Docker socket in the nested container
  `}
/>

Here we basically expect the overall effect to be the same as previously, but this time we will see
that there is basically nothing mounted under `/var/run` path and the `TCP` lookup will be performed for
the `docker-service` host, which will fail, as expected.

<CaptionedImage
  src="/assets/images/nested_container_behavior.png"
  alt="Bingo! üëç"
  height="500"
/>

And this is what we get, the `/var/run` directory is empty (red box in Pic. 2)
and the `TCP` lookup fails (yellow underline in Pic. 2). So now we have to decide what is really open to us
as a solution to open up the communication between the nested container and the host's Docker daemon.
The first thing that comes to mind is to simply delegate the creation of any new Docker containers 
to an autonomous **Docker service**, that will be set up by the runner and reachable from the nested container.
This can be achieved by using the [`services` keyword in the `.gitlab-ci.yml` file]. 
Those services are basically Docker containers spawned beside the CI container itself, which 
are accessible from the tests container due to the network being shared
between all of the sub-hosts present on the runner.

**BUT** there is a catch. Using Docker-in-Docker (DinD) approach requires an additional configuration of the runner itself,
that can be then used by the CI jobs as a shared runner by specifying the `tags` keyword in the `.gitlab-ci.yml` file.
In most cases, kind of runners are already present as a CI/CD tool for software development teams, 
because, for example, building Docker images is a very common task among them, so we can already capitalize on that.
However, if there is no such runner available, we can always create one ourselves. There is a [plethora] [of] [tutorials] 
on how to do that, so I will **assume that this kind of resource is already present in our organization**.

So the game plan now is like so:

1. We will tell GitLab CI to use the runner with the predefined tags set up on it by the administrator.
2. We create a Docker service that will be used by the nested container to communicate with the host's Docker daemon.
We will name it `my-docker-service` and we will use the `docker:dind` image for that purpose.
3. We will set the `DOCKER_HOST` environment variable to `tcp://my-docker-service:2375` in the CI container.
4. We will run the command in the nested container and hope for the best. ü§û

One caveat is that if we want to use the 2375 port, we need to disable TLS verification, which can be done
by setting the `DOCKER_TLS_CERTDIR` environment variable to an empty string. This is because the runner
is not configured to use TLS by default. We can also set the `FF_NETWORK_PER_BUILD` environment variable
to `true`, to enable the network sharing between the containers, which will be useful in a second when we will
configure our mock test environment in a sec. Any `docker` command within the CI container will also need to use
an additional `--network=host` flag to use the CI container network that is shared with the DinD service.

All of these steps can be performed by modifying the `.gitlab-ci.yml` file like so:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      tags:
        - "docker"
        - "dind"
      services:
        - name: docker:dind
          alias: my-docker-service
      variables:
        DOCKER_HOST: "tcp://my-docker-service:2375"
        DOCKER_TLS_CERTDIR: ""
        FF_NETWORK_PER_BUILD: "true"
      script: |
        echo "I am running!"
        docker \\
          run \\
            --network=host \\
            alpine:latest \\
            sh \\
              -c \\
                "echo 'I am running inside the container!'"
  `}
  language="yaml"
  filename=".gitlab-ci.yml (working)"
  annotations={`
    2:We use the docker image to be able to run docker commands inside the CI container
    3-5:We tell GitLab CI to use the runner with the predefined tags set up on it by the administrator
    6-8:We create a Docker service that will be used by the nested container to communicate with the host's Docker daemon
    10:We set the DOCKER_HOST environment variable to 'tcp://my-docker-service:2375' in the CI container
    11:We set the DOCKER_TLS_CERTDIR environment variable to an empty string, to disable TLS verification
    12:We set the FF_NETWORK_PER_BUILD environment variable to true, to enable the network sharing between the containers
    13:BTW, I've removed the ls command since it's not really needed here
    17:We use the --network=host flag to access the DinD container via the host's bridge network
  `}
/>

<CaptionedImage
  src="/assets/images/nested_container_dind.png"
  alt="Whale communication channel established üê≥"
  height="700"
/>

So what we have achieved here is that we have successfully created a workflow for creating environments
made up of multiple containers that can share a network and communicate with each other via the host's bridge network.
What is also useful is that services that are spawned via the `services` keyword are automatically resolvable
by their names, so we don't have to worry about the IP addresses of those containers. We can show it with
a quick `ping-pong` test for a Redis database hosted both as a service and as a nested container.
That nested container will then try to send a `PING` signal via `redis-cli` to the Redis service
named `some-random-cache`. This can be done with the following `.gitlab-ci.yml` file:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      tags:
        - "docker"
        - "dind"
      services:
        - name: docker:dind
          alias: my-docker-service
        - name: redis:latest
          alias: some-random-cache
      variables:
        DOCKER_HOST: "tcp://my-docker-service:2375"
        DOCKER_TLS_CERTDIR: ""
        FF_NETWORK_PER_BUILD: "true"
      script: |
        echo "I am running!"
        docker \\
          run \\
            --network=host \\
            redis:latest \\
            sh \\
              -c \\
                "redis-cli -h some-random-cache -p 6379 PING"
  `}
  language="yaml"
  filename=".gitlab-ci.yml (Redis test)"
  annotations={`
    9:We create the target Redis service
    23:We use the -h flag to specify the hostname and -p flag to specify the host and port of the Redis service
  `}
/>

<CaptionedImage
  src="/assets/images/nested_container_ping_pong.png"
  alt="The ping has been ponged üèì"
  height="700"
/>

Cool! So now we can spawn whatever we want inside our CI runner and make it communicate with each other,
but what about the actual integration tests? Well, we can use the same approach, but just changing the
type of image used by the nested container to the one containing our tests. Let's say we have a Python project
with some integration tests that require a Redis database to be present. The files can be structured like so:

<FileTree
  root="project/"
  tree={[
    "...",
    {
      "src/": [
          "connector.py",
          "..."
        ]
    },
    {
      "tests/": [
          "test_connector.py",
          "..."
        ]
    },
    "..."
  ]}
  annotations={`
    src/connector.py:Code with primitive adapter to Redis database.
    tests/test_connector.py:Integration tests for the connector.py module.
  `}
/>

[`services` keyword in the `.gitlab-ci.yml` file]: https://docs.gitlab.com/ee/ci/services/
[plethora]: https://medium.com/@oachuy/gitlab-runner-with-docker-dind-3e0e1862662f
[of]: https://docs.gitlab.com/ee/ci/docker/using_docker_build.html
[tutorials]: https://docs.gitlab.com/ee/ci/docker/using_docker_build.html