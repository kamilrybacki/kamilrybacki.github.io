---
title: "Containerized Integration"
date: "2023-11-30"
description: "How to easily spin up a mock environment for integration testing"
tags: ["gitlab", "ci", "tests", "docker", "containers", "integration"]
image:
  thumbnail: "components"
  hero: "components"
  alt: "Lab in a box"
---

## I'm afraid I can't let you do that, Dev ü§ñ

There is a certain kind of dread that comes with the acknowledgment that You can't always cross the cap
between Your local pythonic incantations and the desired environment, where those enchantments are to be
set to be roaming free, setting and getting those `os.environ`'s. "Being part of the Machine" is not only
a figurative thing that a societal contrarian might say, but - in terms of software development - it is
also a very literal thing. You design cogs of various sizes and shapes, that are to be put in the overarching
conglomerate of multi-lingual, multi-paradigm and multi-purpose mechanisms.

What is actually contrary to popular belief, the net is not about connecting people and/or machines at the upper echelons
of the enterprise-ish hierarchy. It's more often than not a convoluted web of rules and blockades,
that separate this connectivity from one environment to another - be it different teams of developers,
different services (often at their different stages) or in general the intra/internet gatekeeping conundrum.

This is, of course, done with good intent, so that the developers don't accidentally
set the production database to be the target of their unit tests, pushing around
valuable data about somebody's World of Warcraft character on the private server
run by a group of "real chill dudes", where You've recently managed to down
that one annoying boss with Your guild, **and** got the highest roll on [Crystalheart Pulse-Staff] üòå,
to the dismay of Your fellow restoration druids üå≤.

Those are the real casualties that would have been made without that one reasonable sysadmin
at Your software house, who's set up e.g. that VPN so all GitLab CI workers can't reach the K8s cluster,
where the actual services may be running. Additionally, he will also (rightfully) hunt You down
if You try to copy any credentials to those temporary nodes, to "cleverly" bypass those security measures.

I would like to stress here that I am not talking about any continuous deployment (CD) pipelines,
where the code is automatically deployed to the production environment and it mutates its state, in real time.
CD naturally assumes successful CI (continuous integration) and by integration, it means that the code
is already up to the security and coding standards of Your organization. In other words, it is **safe**.
The CI part is what I am really focusing on in this article, so - I am not GitOps hater or something like that üòÖ.

The thing is, some tasks require a bit more than just simple code linting tests and they must
touch upon another important aspect of integration with the existing environment - communication,
more often than not, in the form of authorized API requests
or, in general, calls made through specified protocols.

So we arrive at the "Schr√∂dinger's code problem" i.e. the code that must talk with our services, but really
can't talk with our services at the same time. But what is to be tested here? Language i.e. form.
In other words, we only care if our code can communicate **at all** with the service,
not really if it retrieves a specific portion of the data. The rest can be mocked.
So basically, if You can't play with the real tools - You can at least play with the toys,
and that is what mock test environments are for. An approximation.

In this article, I will try to show You how I spin up those sandboxes for my integration tests,
using GitLab CI Services and Docker, with additional tips for unit testing Python code via containerized
`pytest` nested runners.

**IMPORTANT NOTE**: This article assumes that there is a GitLab CI runner available in Your organization,
that is capable of Docker-in-Docker (DinD) builds. I've included some links to tutorials on how to set up
such a runner, but I will not go into details on how to do that, as it is not the main focus of this article.

## Alive in a box - dead to the outside world üêà‚Äç‚¨õüì¶

To figure out how to create our pseudo-environment, we must first understand what goes on under the hood
when our job is being executed on the GitLab CI runner. In very short terms, the runner
pulls the Docker image specified in the `.gitlab-ci.yml` for a given job and spins up a container,
which has access to the project's files and the GitLab CI variables. The container then executes
the commands specified in the `script` section of the job. The container is then destroyed,
along with all the changes made to the filesystem, unless we explicitly tell the runner to persist
some files between the jobs as either artifacts or cache.

Take a look at the following example:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest 
      script:
        - echo "I am running!"
        - |
          docker \\
            run \\
              alpine:latest \\
              sh \\
                -c \\
                  "echo 'I am running inside the container!'"
  `}
  language="yaml"
  filename=".gitlab-ci.yml"
  annotations={`
    2:We use the docker image to be able to run docker commands inside the container
    4:This command is executed at the uppermost container level
    5:Here we want to communicate to Docker daemon on the host that we want to nest a new container
    12:This command is executed inside the nested container
  `}
/>

A quick side note - if You see me using the `\` operator in the code snippets, it is just a way to
break the line in the `.gitlab-ci.yml` file (or other manifests/shell scripts), so that it is more readable. 
I like to separate the flags and arguments of the commands into separate lines, with indentations added to, for example,
flag values to see which keyword belongs to which flag. It is not necessary, but I find it
more readable than a single line with a bunch of flags and arguments, separated by spaces.

So, coming back to CI, we expect to be greeted by two messages, one from the host and one from the nested container.
However, when we run this job, we will only see the first message executed at the main container level,
but the second one will not be executed at all:

<CaptionedImage
  src="/assets/images/nested_container_no_socket.png"
  alt="Dude, where is my daemon? ü§î"
  height="500"
/>

The main reason is the way in which the parent container attempts to communicate with the Docker daemon
that is located on the host. The original Docker daemon is listening on a Unix socket, which is located at
`/var/run/docker.sock` on the host machine. 

The parent container, however, does not have access to this socket,
because it is not mounted to the container's filesystem. This is what line number 21 in the log printout above
is really about. The nested container tries to look for a way to communicate with its parent's daemon,
so it checks if the Docker socket is mounted into its filesystem. 

If not (which is the case here), it then tries to communicate with an external Docker **service**, 
specified under the `DOCKER_HOST` environment variable. **By default**, this variable (for GitLab runners)
is set to `tcp://docker:2375`, which translates to URL of: `http://docker:2375`.
And this is the reason why the nested container prints out the error message about the failed `TCP` lookup.

We can confirm this behavior by listing the sockets mounted to the nested container under `/var/run` path
and setting the `DOCKER_HOST` envvar to some bogus value like `tcp://docker-service:2375`:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      variables:
        DOCKER_HOST: "tcp://docker-service:2375"
      script: |
        - echo "I am running!"
        - |
          ls \\
            -la \\
            /var/run/
        - |
          docker \\
            run \\
              alpine:latest \\
              sh \\
                -c \\
                  "echo 'I am running inside the container!'"
  `}
  language="yaml"
  filename=".gitlab-ci.yml (checking behaviour)"
  annotations={`
    4:We point to some other Docker service
    7-9:Check for mounted Docker socket in the nested container
  `}
/>

Here we basically expect the overall effect to be the same as previously, but this time we will see
that there is basically nothing mounted under `/var/run` path and the `TCP` lookup will be performed for
the `docker-service` host, which will fail, as expected.

<CaptionedImage
  src="/assets/images/nested_container_behavior.png"
  alt="Bingo! üëç"
  height="500"
/>

And this is what we get, the `/var/run` directory is empty (red box in Pic. 2)
and the `TCP` lookup fails (yellow underline in Pic. 2). So now we have to decide what is really open to us
as a solution to open up the communication between the nested container and the host's Docker daemon.
The first thing that comes to mind is to simply delegate the creation of any new Docker containers 
to an autonomous **Docker service**, that will be set up by the runner and reachable from the nested container.
This can be achieved by using the [`services` keyword in the `.gitlab-ci.yml` file]. 
Those services are basically Docker containers spawned beside the CI container itself, which 
are accessible from the tests container due to the network being shared
between all of the sub-hosts present on the runner.

**BUT** there is a catch. Using Docker-in-Docker (DinD) approach requires an additional configuration of the runner itself,
that can be then used by the CI jobs as a shared runner by specifying the `tags` keyword in the `.gitlab-ci.yml` file.
In most cases, kind of runners are already present as a CI/CD tool for software development teams, 
because, for example, building Docker images is a very common task among them, so we can already capitalize on that.
However, if there is no such runner available, we can always create one ourselves. There is a [plethora] [of] [tutorials] 
on how to do that, so I will **assume that this kind of resource is already present in our organization**.

### To test and to serve üö®

So the game plan now is like so:

1. We will tell GitLab CI to use the runner with the predefined tags set up on it by the administrator.
2. We create a Docker service that will be used by the nested container to communicate with the host's Docker daemon.
We will name it `my-docker-service` and we will use the `docker:dind` image for that purpose.
3. We will set the `DOCKER_HOST` environment variable to `tcp://my-docker-service:2375` in the CI container.
4. We will run the command in the nested container and hope for the best. ü§û

One caveat is that if we want to use the 2375 port, we need to disable TLS verification, which can be done
by setting the `DOCKER_TLS_CERTDIR` environment variable to an empty string. This is because the runner
is not configured to use TLS by default. We can also set the `FF_NETWORK_PER_BUILD` environment variable
to `true`, to enable the network sharing between the containers, which will be useful in a second when we will
configure our mock test environment in a sec. Any `docker` command within the CI container will also need to use
an additional `--network=host` flag to use the CI container network that is shared with the DinD service.

All of these steps can be performed by modifying the `.gitlab-ci.yml` file like so:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      tags:
        - "docker"
        - "dind"
      services:
        - name: docker:dind
          alias: my-docker-service
      variables:
        DOCKER_HOST: "tcp://my-docker-service:2375"
        DOCKER_TLS_CERTDIR: ""
        FF_NETWORK_PER_BUILD: "true"
      script: |
        - echo "I am running!"
        - |
          docker \\
            run \\
              --network=host \\
              alpine:latest \\
              sh \\
                -c \\
                  "echo 'I am running inside the container!'"
  `}
  language="yaml"
  filename=".gitlab-ci.yml (working)"
  annotations={`
    2:We use the docker image to be able to run docker commands inside the CI container
    3-5:We tell GitLab CI to use the runner with the predefined tags set up on it by the administrator
    6-8:We create a Docker service that will be used by the nested container to communicate with the host's Docker daemon
    10:We set the DOCKER_HOST environment variable to 'tcp://my-docker-service:2375' in the CI container
    11:We set the DOCKER_TLS_CERTDIR environment variable to an empty string, to disable TLS verification
    12:We set the FF_NETWORK_PER_BUILD environment variable to true, to enable the network sharing between the containers
    13:BTW, I've removed the ls command since it's not really needed here
    17:We use the --network=host flag to access the DinD container via the host's bridge network
  `}
/>

<CaptionedImage
  src="/assets/images/nested_container_dind.png"
  alt="Whale communication channel established üê≥"
  height="700"
/>

So what we have achieved here is that we have successfully created a workflow for creating environments
made up of multiple containers that can share a network and communicate with each other via the host's bridge network.
What is also useful is that services that are spawned via the `services` keyword are automatically resolvable
by their names, so we don't have to worry about the IP addresses of those containers. We can show it with
a quick `ping-pong` test for a Redis database hosted both as a service and as a nested container.
That nested container will then try to send a `PING` signal via `redis-cli` to the Redis service
named `some-random-cache`. This can be done with the following `.gitlab-ci.yml` file:

<CodeSnippet
  code={`
    test-job:
      image: docker:latest
      tags:
        - "docker"
        - "dind"
      services:
        - name: docker:dind
          alias: my-docker-service
        - name: redis:latest
          alias: some-random-cache
      variables:
        DOCKER_HOST: "tcp://my-docker-service:2375"
        DOCKER_TLS_CERTDIR: ""
        FF_NETWORK_PER_BUILD: "true"
      script: |
        - echo "I am running!"
        - |
          docker \\
            run \\
              --network=host \\
              redis:latest \\
              sh \\
                -c \\
                  "redis-cli -h some-random-cache -p 6379 PING"
  `}
  language="yaml"
  filename=".gitlab-ci.yml (Redis test)"
  annotations={`
    9:We create the target Redis service
    23:We use the -h flag to specify the hostname and -p flag to specify the host and port of the Redis service
  `}
/>

<CaptionedImage
  src="/assets/images/nested_container_ping_pong.png"
  alt="The ping has been ponged üèì"
  height="700"
/>

### Snake-in-a-box üêçüì¶ (but not that hypercube one)

Cool! So now we can spawn whatever we want inside our CI runner and make it communicate with each other,
but what about the actual integration tests? Well, we can use the same approach, but just changing the
type of image used by the nested container to the one containing our tests. Let's say we have a Python project
with some integration tests that require a Redis database to be present. The files can be structured like so:

<FileTree
  root="project/"
  tree={[
    ".env",
    "connector.py",
    "test_connector.py",
    "requirements.txt"
  ]}
  annotations={`
    .env:A way to introduce env variables to the nested container (for now).
    connector.py:Code with primitive adapter to Redis database.
    test_connector.py:Integration tests for the connector.py module.
    requirements.txt:Python dependencies for the connector.py module.
  `}
/>

The `connector.py` module will use the `redis` library to connect to the database 
and the `test_connector.py` module will contain tests for our new class. 
The `requirements.txt` file will contain the `redis` library as a dependency
to ensure that the runner container will have all of the required modules installed. So what does
the `connector.py` module has to offer? A janky üçë wrapper around the basic `redis.StricRedis` methods
that will allow us to connect to the database and set/get some values from it:

<CodeSnippet
  code={`
    import dataclasses
    import logging
    import typing

    import redis

    @dataclasses.dataclass
    class RedisConnector:
        host: str = "localhost"
        port: int = 6379
        db: int = 0
        _session: redis.StrictRedis = dataclasses.field(init=False, default_factory=redis.StrictRedis)

        def __post_init__(
            self,
        ) -> redis.StrictRedis:
            try:
                self._session = redis.StrictRedis(
                    host=self.host,
                    port=self.port,
                    db=self.db
                )
                logging.info("Redis connection established")
                return self._session
            except Exception as connection_failed:
                logging.error("Redis connection failed")
                logging.error(connection_failed)
                raise connection_failed

        def __del__(self) -> None:
            self.disconnect()

        def disconnect(self) -> None:
            if self._session:
                self._session.close()
                logging.info("Redis connection closed")

        def __getitem__(self, key) -> typing.Any:
            return self._session.get(key)

        def __setitem__(self, key: str, value: typing.Any) -> None:
            self._session.set(key, value)
  `}
  language="python"
  filename="connector.py"
  annotations={`
    7:We will use dataclasses to simplify the initialization of the class
    12:This field will store the already established connection to the Redis database
    14-28:Here we try to connect to the Redis database and log the result
    30:Here we define the destructor for the class, which will close the connection
    33:Here we define the method for closing the connection
    38-42:Here we define the setter and getter for the Redis database
  `}
/>

This may not be the apex of the Pythonic design, but it will do for our purposes. The `test_connector.py` module
is just a basic implementation of the `ping-pong` test, that utilizes the `.ping()` method of the `redis.StrictRedis`
class:

<CodeSnippet
  code={`
    import os

    import connector


    def test_redis_connection():
        target_host = os.environ.get("REDIS_HOST")
        target_port = os.environ.get("REDIS_PORT")
        target_db = os.environ.get("REDIS_DB")
        redis_instance = connector.RedisConnector(
            host=target_host,
            port=target_port,
            db=target_db
        )
        assert redis_instance._session.ping()
  `}
  language="python"
  filename="test_connector.py"
  annotations={`
    7-9:We retrieve the environment variables from the nested container
    15:We reach to the underlying redis.StricRedis class and use its .ping() method
  `}
/>

Here we can see the purpose of `.env` file - itwill contain the two environment variables 
that are to be used by the connector class to, well, ... connect to the Redis database: 
`REDIS_HOST`, `REDIS_PORT` and `REDIS_DB`. So we want to do the following things:

1. Set up a temporary Redis database.
2. Mount the repository with out code to a `pytest` runner container.
3. Install the required dependencies to carry out the unit tests.
4. Run `pytest` and (as previously) - hope for the best. ü§û

The main goal of these tests will be to check if our interface is compatible with the one
offered by the `redis` library. So we will not be testing the actual data retrieval, but rather
the ability to retrieve the data at all. A simple case, that we will, of course, complicate in a bit.
So, the `.gitlab-ci.yml`:

<CodeSnippet
  code={`
      test-job:
      image: docker:latest
      tags:
        - "docker"
        - "dind"
      services:
        - name: docker:dind
          alias: my-docker-service
        - name: redis:latest
          alias: some-random-cache
      variables:
        DOCKER_HOST: "tcp://my-docker-service:2375"
        DOCKER_TLS_CERTDIR: ""
        FF_NETWORK_PER_BUILD: "true"
      script: |
        - echo "I am running!"
        - |
          docker \\
            run \\
              --network=host \\
              --volume $PWD:/app \\
              --workdir /app \\
              --env-file .env \\
              python:3.10 \\
              sh -c "pip install -r requirements.txt pytest && pytest"
  `}
  language="yaml"
  filename=".gitlab-ci.yml (testing connector)"
  annotations={`
    20:We mount the WHOLE repo to some path in the container
    21:We set the working directory to the mounted repo
    22:We set the environment variables from the mounted .env file
    24:We install the required dependencies and run the tests
  `}
/>

The value for the `REDIS_HOST` environment variable will be `some-random-cache` and the value for the `REDIS_PORT`
environment variable will be `6379`. The `REDIS_DB` environment variable will be set to `0` by default.
As it can be seen in the log snippet below, the tests have passed (I've omitted the initial setup logs,
since it they are almost identical to previous ones - only this time the `python:3.10` image is pulled ü§∑):

<CaptionedImage
  src="/assets/images/nested_container_pytest_pong.png"
  alt="The ping has been ponged again, programmatically üêçüèì"
  height="700"
/>

### Matryoshka of problems ü™Ü

So we have successfully created a mock test environment for our integration tests.
It is self-contained and somewhat "invisible" (or dead üíÄ) to the outside architecture of the organization,
but we can see it is alive and kicking from the inside üò∏. However, some things about it may (and in practice - often will) 
create some practical issues. üòø

The first one is the fact that we have stored our environment variables in a file that is mounted to the container. 
This is not a good practice, because we are basically storing our secrets in plain text and totally negating the existence of any
sensitive data protection measures that we may have in place e.g. Vault or some other secrets management tool.
Also, GitLab can use environment variables defined in CI/CD settings of the repo, so we can use that
to our advantage.

The second one is that if we want to spawn more than one container via `docker` command, we will have to
create long `script` sections in our `.gitlab-ci.yml` file, which will be hard to maintain and read.
Moreover, if we would like to pass the aforementioned environment variables to the nested containers,
we would have to do it manually for each of them, using the `--env` flag. This is not a good practice either,
because we are basically repeating ourselves and we are not really using the full potential of the containerization.
Also - container dependencies. If we want to spawn a container that depends on another container, we would have to
create a script that will wait for the first container to be ready and then spawn the second one, manually.
And how do we define that the first container is ready? Well, we would have to create a healthcheck script.
And mount it to the container, under a predefined path for Docker daemon to use.

The third issue that may pop up from time to time - network management. Sometimes, we want to be able to
use raw IP addresses of the containers, instead of their DNS-resolved names. In some cases, it may even speed up the
tests execution because the DNS lookups are not being performed constantly.

And the fourth one - observability. We want to be able to see what is going on inside the containers,
especially the ones defined as services, since we will be sending various requests to them and we want to be sure
that they are being processed correctly.

So, we have three issues connected to the configuration of **compositions** made up from **Docker** containers.
I think You know where I'm going with this. ü§î

## Compose Yourself, before You test Yourself üêãüõ†Ô∏è

So yeah, let's pack up our happy family of containers into a single YAML, that can be nicely stored in our repo
and reviewed by our fellow developers. We can do that by using the `docker-compose` tool, which is a part of the
Docker ecosystem. It allows us to define a set of containers that will be spawned together and may share a network.

I will try to include all of the aforementioned aspects into this new setup, but bear in mind that for this
simple case of a single container, it may be a bit of an overkill üôà. But it will easily scale to a more complex
and appropriate problem shortly. Without further ado, let's take a look at the `docker-compose.yml` file:

<CodeSnippet
  code={`
    version: '3'

    services:
      redis:
        image: redis:6.2.6-alpine
        ports:
          - 6379:6379
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        healthcheck:
          test: ["CMD", "redis-cli", "ping", '-h', 'localhost', '-p', '6379']
          interval: 5s
          timeout: 5s
          retries: 5

      pytest-runner:
        image: python:3.10
        depends_on:
          - redis
        networks:
          test-network:
            ipv4_address: 10.5.0.3
        volumes:
          - \$\{CI_PROJECT_DIR\}:/app
        working_dir: /app
        environment:
          - REDIS_HOST
          - REDIS_PORT
          - REDIS_DB
        command:
          - sh
          - -c
          - |
            pip \\
              install \\
                -r requirements.txt \\
                pytest
            pytest

    networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml"
  annotations={`
    8-10:Redis service networking configuration
    12-15:Redis healthcheck
    21-23:Pytest runner networking configuration
    24:Mounting the code
    26:Setting the working directory to cloned repo
    27-30:Environment variables pulled from parent CI container
    32-39:Multiline command syntax I tend to use
    46-48:Setting manually the subnet and gateway for the test network
  `}
/>

Since it is a lot to take in at once due to some caveats present, I will go through the most
juicy parts ü•ùüçãüçé of this file.

### Decomposing the composition ‚úÇÔ∏è

The general goal of this file is to create a network called `test-network` and spawn two containers
inside it - one with a Redis database and one with our `pytest` runner. So, I will divide this section
into two aspects: container general setup and container interconnectivity (i.e. networking).

#### Container general setup üë©‚Äçüè≠üì¶

The first thing that we can see is that we have two services defined under the `services` keyword:

* the Redis database, which is defined as a service with the `redis` name,
* the `pytest` runner, which is defined as a service with the `pytest-runner` name.

For the first one, since we're focused here on setting up **JUST** the Redis database,
without any additional services in its container, the `alpine` version of Redis Docker image is used.

For the `pytest` runner, we use the `python:3.10` image, which is the same as the previously used tag
in our last version of `.gitlab-ci.yml` file. The logical thing here is to sping up that container
**only** if the Redis service has not only started, but is also ready to accept connections i.e. healthy.
This can be achieved by using the [`depends_on` Docker Compose keyword], which will make the `pytest` runner container
wait for the Redis container to be ready.

To ensure that the `redis` container is ready, we will
manually define a [healthcheck] for it, which will be performed by the Docker daemon. This healthcheck
will be performed every 5 seconds, 5 times, with a timeout of 5 seconds. The gist of it is the
`redis-cli ping` command, that You have seen at the beginning of this article.

<CodeSnippet
  code={`
    healthcheck:
      test: ["CMD", "redis-cli", "ping", '-h', 'localhost', '-p', '6379']
      interval: 5s
      timeout: 5s
      retries: 5
  `}
  language="yaml"
  filename="docker-compose.yml (healthcheck)"
  annotations={`
    2:We define the command that will be executed by the Docker daemon
    3:We define the interval between the healthchecks
    4:We define the timeout for the healthcheck
    5:We define the number of retries for the healthcheck
  `}
/>

We also need to configure the `pytest` to contain necessary environment variables, that will be used
by the `connector.py` module to connect to the Redis database. This can be done by using the `environment`
keyword, which will set the environment variables in the container. Here, we can just list the names
of the variables, without their values, because they will be pulled from the parent CI container
(the one that is actually spawned by the GitLab runner). However, nothing stops us from setting
the values here, if we want to override the values from the parent container, using the
`[VARIABLE_NAME]: [VARIABLE_VALUE]` syntax.

And of course, since we are testing our Python module,
we need to mount the repository with our code to the container via `volumes` keyword and set the
working directory to the mounted repo via `working_dir` keyword. The last step is optional,
but I prefer to do it because it makes it more natural to issue commands in the container
as if we were in the repo root directory on our local machine.

When it comes to the `pytest-runner` commands, we can see that we are using the multiline syntax
to install the dependencies and run the tests. This is because we want to avoid the use of
the `&&` operator, which is used to chain target commands in a single line or some other line-breaking
maneuvers (due to the underlying `sh` command), which are not really readable. So as You can see,
I just pack the actual directives into a single string and use the `|` operator to tell the shell
to treat it as a multiline command:

<CodeSnippet
  code={`
    command:
      - sh
      - -c
      - |
        pip \\
          install \\
            -r requirements.txt \\
            pytest
        pytest
  `}
  language="yaml"
  filename="docker-compose.yml (multiline command)"
  annotations={`
    4:The | operator tells the shell to treat the following lines as a multiline command
  `}
/>

And that's it for the general setup of the containers. Now let's take a look at the networking.

#### Container interconnectivity üì¶üîóüì¶

First, we will start from the bottom of our `docker-compose.yml` file, where we define the `test-network`.
Here, just to be üíØ% sure, I explicitly set its name to `test-network`. It's not really necessary,
since it will be automatically named after the directory in which the `docker-compose.yml` file is located,
if it suits Your case.

<CodeSnippet
  code={`
      networks:
      test-network:
        name: test-network
        ipam:
          driver: default
          config:
            - subnet: 10.5.0.0/16
              gateway: 10.5.0.1
  `}
  language="yaml"
  filename="docker-compose.yml (network)"
  annotations={`
    3:We define the name of the network
    4:We do the manual IP address management (IPAM)
    7:Creating the subnet (group of IP addresses) for the network
    8:Setting the gateway for the network
  `}
/>

Then, we move onto the IP address management (IPAM) part. Here, we define the subnet for the network
where out containers will be located, by explicitly assigning them adresses from the range
from `10.5.0.0` to `10.5.255.254` (where 65534 possible hosts is ofc an overshot, but it's just an example).
To get this kind of IP address range, we must define the subnet as `10.5.0.0/16`, where the `/16` part
is the CIDR notation for the subnet mask. The `gateway` keyword is used to define the IP address
of the gateway for the network, which is the address of the host machine on which the containers
will be spawned i.e. the parent CI container. The rule of thumb here is that **the gateway address
must be the first address in the subnet range**, so in our case, it would be `10.5.0.1`.

As for the network configuration of the services, we must simply specify for each service the
network to which it belongs and the IP address that it will be assigned to. This can be done
by using the `networks` keyword, which will contain the name of the network and the `ipv4_address`
field with the desired IP address:

<CodeSnippet
  code={`
    services:
      redis:
        ...
        networks:
          test-network:
            ipv4_address: 10.5.0.2
        ...

      pytest-runner:
        ...
        networks:
          test-network:
            ipv4_address: 10.5.0.3
        ...
  `}
  language="yaml"
  filename="docker-compose.yml (services networking snippets)"
  annotations={`
    4-6:Redis service networking configuration
    11-13:Pytest runner networking configuration
  `}
/>

So now in our configuration of unit tests, we can be sure that whenever we make any calls like
API calls or database queries, we will be able to use the IP addresses of the containers instead
of their DNS-resolved names.

Finally, after running our modified mock stack setup, we should see that the Docker daemon
successfully spins up a new network with two containers inside it, performs the tests defined
in the `test_connector.py` module and then destroys the network along with the containers.
Let's see if that is the case üôà.

<CaptionedImage
  src="/assets/images/compose_first_stack.png"
  alt="The composed pinger has received its service-oriented pong üêãüèì"
  height="400"
/>

We see not only that the mock environment has been successfully created and executed its workload,
**but** we have now access to nicely separated logs of the services that are spawned by the `docker-compose` tool,
due to their lines being prepended with the container's name. These logs can be even scrapped by some other
tool (if we're talking about a setup oriented towards CD) for further analysis. The possibilities are there‚ùï

But, we have reached our goal of testing integration of our connector with the Redis database,
but the thing is that we have used the `redis` library that, by the design, already accomplishes
that. So now it is time to move to some raw and dirty HTTP requests and barebones JSON responses.
We're talking about REST API here. 4Ô∏è‚É£0Ô∏è‚É£4Ô∏è‚É£‚ò†Ô∏è

## Covert DevOps üïµÔ∏èüì¶

We will go wild on this one, but it actually references one of the cases I've encountered in my work.
The main goal here is to provide a middle-man application, **an agent** üïµÔ∏è, that requests metrics
from one database and then pushes it into some other remote endpoint. This is kind of similar to
[how Prometheus works in remote-write mode] - it scrapes the metrics from the targets
and passes them forward to some other place.

For demonstration purposes, the agent will be a simple Python application that will be
sending HTTP requests using `requests` library. To produce some metrics, we will use
the awesome [`fake-metrics-generator`] project by Grafana team, which already has a Docker image available
to quickly spin up containers spitting out random metrics. The target endpoint for this data
will be a simple [MongoDB] instance with a database named `metrics` and a collection named `fake`.

This setup may look out of touch with reality, but imagine substituting the `fake-metrics-generator`
with Prometheus or Grafana instance and MongoDB with some other database or application
that analyzes the metrics. Or just "don't-imagine" projects like [prometheus-kafka-adapter],
these things need implementation all over the place due to different data flows present in the organizations.

This also gives us a chance to do some HTTP calls to the `fake-metrics-generator` container
and communicate with MongoDB via `pymongo` library. So, two distinct ways of communication -
via REST API and a programmatic interface, which is cool for our tutorial here.

[Crystalheart Pulse-Staff]: https://www.wowhead.com/tbc/item=28782/crystalheart-pulse-staff
[`services` keyword in the `.gitlab-ci.yml` file]: https://docs.gitlab.com/ee/ci/services/
[plethora]: https://medium.com/@oachuy/gitlab-runner-with-docker-dind-3e0e1862662f
[of]: https://docs.gitlab.com/ee/ci/docker/using_docker_build.html
[tutorials]: https://docs.gitlab.com/ee/ci/docker/using_docker_build.html
[`depends_on` Docker Compose keyword]: https://docs.docker.com/compose/compose-file/05-services/#depends_on
[healthcheck]: https://docs.docker.com/engine/reference/builder/#healthcheck
[how Prometheus works in remote-write mode]: https://prometheus.io/docs/concepts/remote_write_spec/
[`fake-metrics-generator`]: https://github.com/grafana/fake-metrics-generator
[MongoDB]: https://www.mongodb.com/
[prometheus-kafka-adapter]: https://github.com/Telefonica/prometheus-kafka-adapter
